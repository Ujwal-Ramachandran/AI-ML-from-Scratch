{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53b4b679-c8d7-4e6f-b476-2fe92a7a8328",
   "metadata": {},
   "source": [
    "# Linear Algebra with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f68589-420d-459b-b01e-c2285b152596",
   "metadata": {},
   "source": [
    "#### Concept 1: Vectors, Matrices, and the $(N, D)$ Shape\n",
    "\n",
    "**The Why:**\n",
    "In Machine Learning, we rarely process one data point at a time. We process \"batches\" of data for efficiency.\n",
    "* A **Vector** (1D array) usually represents a single data point's features.\n",
    "* A **Matrix** (2D array) represents a **batch** of data points.\n",
    "\n",
    "We almost always denote this shape as $(N, D)$, where:\n",
    "* $N$ = Batch Size (number of samples).\n",
    "* $D$ = Input Dimensions (number of features per sample).\n",
    "\n",
    "**Micro-Task 1:**\n",
    "Using `numpy`:\n",
    "1.  Set a random seed (for reproducibility).\n",
    "2.  Create a \"batch\" of data $X$ representing **5 samples**, where each sample has **4 features** (e.g., height, weight, age, income).\n",
    "3.  Print the array and, crucially, print its `.shape` attribute to verify it matches $(N, D)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786024fa-8fed-43fa-b479-92e965e812ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Computers aren't actually random; they follow a complex list of instructions to look random. \n",
    "# If we set a \"seed\" (like a starting point for that list), we can make sure we get the exact same \"random\" numbers every time we run the code. \n",
    "# This is crucial for debugging!\n",
    "\n",
    "np.random.seed(67)\n",
    "\n",
    "# Create a random X\n",
    "X = np.random.rand(5,4)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914250b-5e31-474d-a1ef-04f829cc48d0",
   "metadata": {},
   "source": [
    "#### Concept 2: Matrix Multiplication & Dot Products\n",
    "\n",
    "**The Why:**\n",
    "This is the single most important operation in Deep Learning. A \"Dense Layer\" in a neural network is just a dot product between your data ($X$) and a matrix of learnable weights ($W$).\n",
    "\n",
    "If $X$ holds your data and $W$ holds the network's knowledge, multiplying them ($X \\cdot W$) gives you the network's prediction.\n",
    "\n",
    "**The Rule:**\n",
    "The **inner dimensions** must match.\n",
    "$$(N, \\underbrace{D) \\cdot (D}_{match}, M) \\rightarrow (N, M)$$\n",
    "\n",
    "**Micro-Task 2:**\n",
    "We have our input `X` with shape `(5, 4)`.\n",
    "1.  Create a random weight matrix `W`. We want this layer to have **3 neurons** (output features).\n",
    "2.  Compute the dot product `Y` using `np.dot(X, W)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263618a-5134-4cca-890a-de1e68a69e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a weight that can be multiplied with X\n",
    "W = np.random.rand(4,3)\n",
    "\n",
    "# Dot product\n",
    "Y = np.dot(X, W)\n",
    "np.shape(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442049a-1802-4532-a345-76347253a569",
   "metadata": {},
   "source": [
    "This `(N, M)` output is exactly what you'd feed into the next layer of a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Concept 3: Broadcasting\n",
    "\n",
    "**The Why:**\n",
    "In Python, if you want to add a single number (bias) to a whole matrix, you'd typically need a loop. `numpy` handles this automatically via **Broadcasting**. It \"stretches\" the smaller array to match the larger one without actually copying data, which is incredibly fast.\n",
    "\n",
    "In a Dense Layer, the formula is $Y = X \\cdot W + b$.\n",
    "* $X \\cdot W$ is `(N, M)` (as we just saw).\n",
    "* $b$ (bias) is usually `(M,)`—one bias value per neuron.\n",
    "\n",
    "Numpy will automatically add that `(M,)` vector to *every single row* of the `(N, M)` matrix.\n",
    "\n",
    "**Micro-Task 3:**\n",
    "1.  Create a bias vector `b` with shape `(3,)` (since we have 3 output neurons).\n",
    "2.  Add `b` to your previous result `Y`.\n",
    "3.  Print the shape of the result to prove it didn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258ac7dc-5629-4b4e-bf43-f20fe76a3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(Y) is (5,3) so b should be (1,3)\n",
    "# Possible ways of writing:\n",
    "#     b = np.random.rand(3)\n",
    "#     b = np.random.rand(1, 3) Creates a 2D matrix, incorrect format but broadcast is smart enough to understand\n",
    "#     b = np.random.rand(3, )\n",
    "b = np.random.rand(3)\n",
    "Y = np.dot(X,W) + b\n",
    "np.shape(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec631cf2-c97b-4e72-8b54-eec44ec467db",
   "metadata": {},
   "source": [
    "#### Concept 4: ReLU Activation (Vectorization)\n",
    "\n",
    "**The Why:**\n",
    "Right now, our calculation $XW + b$ is purely **linear**. If we stack 100 linear layers on top of each other, mathematically, they compress down to just one single linear layer. We gain no power.\n",
    "\n",
    "To learn complex patterns (like curves, faces, or grammar), we need to inject **non-linearity**. We do this with an \"Activation Function.\"\n",
    "The most common one is **ReLU (Rectified Linear Unit)**. It's deceptively simple:\n",
    "* If the number is positive, keep it.\n",
    "* If the number is negative, turn it to 0.\n",
    "\n",
    "**Micro-Task 4:**\n",
    "We need to apply ReLU to your matrix `Y`.\n",
    "1.  Create a new variable `A` (for Activation).\n",
    "2.  Set `A` equal to `Y`, but replace all negative values with `0`.\n",
    "3.  **Constraint:** Do **not** use a `for` loop. Use `np.maximum(..., ...)` to do it all at once (Vectorization).\n",
    "4.  Print `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f2a1b3-cd47-43e3-978c-1b27c9286a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.maximum(0, Y)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ffcc9-1858-4a95-8b94-0969de5884cb",
   "metadata": {},
   "source": [
    "### Step 3: The Project\n",
    "**Project Title: The \"Manual\" Dense Layer & Efficiency Benchmark**\n",
    "\n",
    "We are going to prove *why* we use NumPy by building a forward pass and then pitting it against standard Python loops.\n",
    "\n",
    "**Specifications:**\n",
    "1.  **Data Setup:**\n",
    "    * Create a random input batch $X$ of shape `(100, 500)` (Batch Size = 100, Features = 500).\n",
    "    * Create a random weight matrix $W$ of shape `(500, 64)` (500 Input features $\\to$ 64 Output neurons).\n",
    "2.  **Task A: The NumPy Approach**\n",
    "    * Compute $Y = X \\cdot W$ using `np.dot`.\n",
    "    * Time how long this takes (you can use the `time` module).\n",
    "3.  **Task B: The Naive Loop Approach**\n",
    "    * Write a function that computes the exact same matrix multiplication using standard Python `for` loops (iterating over rows and columns).\n",
    "    * Time how long this takes.\n",
    "4.  **Output:**\n",
    "    * Print the shape of the result (should be `(100, 64)`).\n",
    "    * Print the time taken for both approaches to show the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc385a-56e1-4638-b4f0-e069fb070c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "X1 = np.random.rand(100,500)\n",
    "W1 = np.random.rand(500, 64)\n",
    "\n",
    "start_time1 = time.time()\n",
    "Y1 = np.dot(X1, W1)\n",
    "end_time1 = time.time()\n",
    "total_time1 = end_time1 - start_time1\n",
    "\n",
    "print(np.shape(Y1), f\"\\nThe numpy process took {total_time1:.4f} sec\")\n",
    "\n",
    "X2 = np.random.rand(100,500)\n",
    "W2 = np.random.rand(500, 64)\n",
    "\n",
    "n = np.shape(X2)[0]\n",
    "l = np.shape(X2)[1]\n",
    "m = np.shape(W2)[1]\n",
    "\n",
    "Y2 = np.zeros((n, m))\n",
    "\n",
    "start_time2 = time.time()\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(m):\n",
    "        x_sum = 0\n",
    "        for k in range(l):\n",
    "            x_sum += X2[i][k] * W2[k][j]\n",
    "        Y2[i][j] = x_sum\n",
    "\n",
    "end_time2 = time.time()\n",
    "total_time2 = end_time2 - start_time2\n",
    "\n",
    "print(np.shape(Y2), f\"\\nThe non-numpy process took {total_time2:.4f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd02fa-4ba1-4881-b30e-395d78c554c8",
   "metadata": {},
   "source": [
    "This is a fantastic result. You have just empirically proven the \"Why\" of Deep Learning frameworks.\n",
    "\n",
    "* **NumPy Time:** ~0.0010 sec\n",
    "* **Loop Time:** ~2.7564 sec\n",
    "* **Speedup:** Your NumPy code is roughly **2,750x faster**.\n",
    "\n",
    "If training a real model took 1 week with NumPy, doing it with loops would take **52 years**. This is why we never write raw loops for math in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 4: The Code Review\n",
    "\n",
    "Here is my critique of your implementation:\n",
    "\n",
    "**1. The \"Pro\" Syntax (Pass)**\n",
    "You correctly identified the triple loop structure required for matrix multiplication:\n",
    "* Outer loops ($i, j$) iterate through every output cell.\n",
    "* Inner loop ($k$) computes the dot product for that cell.\n",
    "* This is mathematically perfect.\n",
    "\n",
    "**2. Optimizations & Best Practices**\n",
    "* **Indexing:** You used `X2[i][k]`. In NumPy, it is slightly faster and more standard to use comma-separated indexing: `X2[i, k]`. The `[][]` syntax creates a temporary intermediate view, whereas `[,]` accesses the memory directly.\n",
    "* **Data Reuse:** You created new variables `X2` and `W2`. For a strict benchmark, it's better to reuse `X1` and `W1`. This ensures we are timing the *algorithm*, not the memory allocation differences or random number generation quirks.\n",
    "\n",
    "**3. The \"Under the Hood\" Magic**\n",
    "Why was NumPy so fast?\n",
    "1.  **C-Backend:** NumPy delegates the heavy lifting to C and Fortran code, avoiding Python's slow interpreter overhead.\n",
    "2.  **SIMD (Single Instruction, Multiple Data):** Modern CPUs can multiply 4, 8, or 16 numbers in a single clock cycle. NumPy leverages this; Python loops do not.\n",
    "3.  **BLAS:** NumPy links to highly optimized linear algebra libraries (like Intel MKL or OpenBLAS) that manage memory cache much better than a naive loop.\n",
    "\n",
    "**Rating:** ⭐ **PASS** (Strong implementation).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5: Real-World Context\n",
    "\n",
    "**Where does this fit in the Big Picture?**\n",
    "\n",
    "You just built the engine of a **Fully Connected Network (FCN)** (also called a Multi-Layer Perceptron).\n",
    "\n",
    "In production (e.g., at Netflix or Google):\n",
    "1.  **The \"Batch\":** We don't predict what *one* user wants to watch. We process users in batches of 128, 256, or even 1024 at a time (just like your `N=100`).\n",
    "2.  **The \"Dense Layer\":** This $X \\cdot W$ operation happens billions of times a day.\n",
    "3.  **Hardware:** While your CPU is fast, in Week 2 and 3, we will move this exact logic to a **GPU**. GPUs are designed to do thousands of these $(N, D) \\times (D, M)$ operations in parallel.\n",
    "\n",
    "\n",
    "\n",
    "**Summary of Week 1:**\n",
    "* **Vectors/Matrices:** The language of data.\n",
    "* **Broadcasting:** The trick to avoid loops.\n",
    "* **Dot Product:** The engine of prediction.\n",
    "* **ReLU:** The spark of non-linearity.\n",
    "* **Vectorization:** The key to speed.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21f651e-3dbb-4f46-8d1c-0bab1d1fe356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
