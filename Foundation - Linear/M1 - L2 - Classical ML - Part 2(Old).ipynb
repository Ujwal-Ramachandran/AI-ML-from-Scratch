{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24babb6f-2a12-4c64-82ad-48e9275dfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb002e59-e9e1-482e-ad7f-e1a9fa65dc58",
   "metadata": {},
   "source": [
    "### **Concept 1: The Confusion Matrix**.\n",
    "\n",
    "Imagine you are building a cancer detection system.\n",
    "\n",
    "  * If you predict \"No Cancer\" but the patient *has* cancer, they die. (False Negative - **FN**)\n",
    "  * If you predict \"Cancer\" but they are healthy, they just get scared. (False Positive - **FP**)\n",
    "  * Standard \"Accuracy\" treats these errors as equal, which is dangerous. The Confusion Matrix breaks predictions down into four buckets: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negative (FN).\n",
    "\n",
    "**The Micro-Task:**\n",
    "I will provide you with two small lists: ground truth labels (`y_true`) and your model's predictions (`y_pred`).\n",
    "\n",
    "**Your Task:**\n",
    "Write a Python function `calculate_confusion_matrix(y_true, y_pred)` using **only raw Python lists or NumPy** (no Scikit-Learn).\n",
    "It should return a dictionary or tuple containing the counts for `{TP, TN, FP, FN}`.\n",
    "\n",
    "Assume:\n",
    "\n",
    "  * `1` = Positive Class\n",
    "  * `0` = Negative Class\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Input Data\n",
    "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 1]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f13ed90-91aa-440b-a7ac-09284b9e6523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 1])\n",
    "\n",
    "## Beginner level: To understand the logic\n",
    "def calculate_confusion_matrix_manual(y_true, y_pred):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == 0 and y_true[i] == y_pred[i]:\n",
    "            tn += 1\n",
    "        elif y_true[i] == 1 and y_true[i] == y_pred[i]:\n",
    "            tp += 1\n",
    "        elif y_true[i] == 0 and y_true[i] != y_pred[i]:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    return tp, tn, fp, fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de4e6a2-d475-42dc-8fa2-131d56620433",
   "metadata": {},
   "source": [
    "**Optimization (The \"Pro\" Way):**\n",
    "While the loop is readable, loops are slow in Python for large datasets (millions of rows). In Data Science, we avoid loops whenever possible and use **Vectorization** (NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "012eccc2-5ec7-4db8-b765-a39d5ce6a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0])\n",
    "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 1])\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix_numpy(y_true, y_pred):\n",
    "    # Boolean logic masks the arrays instantly. Use & and not \"and\"\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c1895-27d3-4c49-b0e9-8d3f75602fad",
   "metadata": {},
   "source": [
    "### **Concept 2: Precision and Recall**\n",
    "\n",
    "Now that we have the raw counts, we need to turn them into metrics. We rarely look at just one; we look at the trade-off between **Precision** and **Recall**.\n",
    "\n",
    "  * **Precision:** \"Of all the times the model screamed 'Cancer\\!', how often was it right?\" (High Precision = Low False Positives\n",
    "        **The Formula:**\n",
    "$$Precision = \\frac{\\text{True Positives}}{\\text{Total Predicted Positives}} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "  * **Recall (Sensitivity):** \"Of all the people who actually have cancer, how many did we find?\" (High Recall = Low False Negatives).\n",
    "        **The Formula:**\n",
    "$$Recall = \\frac{\\text{True Positives}}{\\text{Total Actual Positives}} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "\n",
    "**The Micro-Task:**\n",
    "Using the `TP, TN, FP, FN` values you calculated in the previous step, write a function `calculate_metrics(tp, tn, fp, fn)` that returns a dictionary with `\"precision\"` and `\"recall\"`.\n",
    "\n",
    "**Constraint:** Handle the \"division by zero\" edge case (e.g., if the model never predicts positive, `TP + FP` will be 0). If the denominator is 0, return 0.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "44b7807d-8e3b-4a70-ac9b-dbe1b816c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6666666666666666, 0.8)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, tn, fp, fn = calculate_confusion_matrix_numpy(y_true, y_pred)\n",
    "\n",
    "def calculate_metrics(tp, tn, fp, fn):\n",
    "    tfp = tp + fp\n",
    "    precision = tp / tfp if tfp > 0 else 0.0\n",
    "\n",
    "    tpfn = tp + fn\n",
    "    recall = tp / tpfn if tpfn > 0 else 0.0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "calculate_metrics(tp, tn, fp, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdd7f4f-70da-4a14-977f-405a4f93c989",
   "metadata": {},
   "source": [
    "### **Concept 3: F1 Score**\n",
    "\n",
    "You now have two numbers: Precision (0.67) and Recall (0.8).\n",
    "\n",
    "If you have Model A with (P=0.9, R=0.1) and Model B with (P=0.4, R=0.6), which one is better? It's hard to compare.\n",
    "\n",
    "Enter the **F1 Score**. It is the *Harmonic Mean* of Precision and Recall. Unlike a simple average, the Harmonic Mean punishes extreme values. If *either* Precision or Recall drops to zero, the F1 Score tanks to zero. It forces the model to balance both.\n",
    "\n",
    "**The Micro-Task:**\n",
    "Write a function `calculate_f1(precision, recall)` using the values from your previous step.\n",
    "\n",
    "  * **The Formula:**\n",
    "$$F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "  * **Constraint:** Handle the case where `Precision + Recall == 0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f6ffd843-cde6-453c-a01c-cec03562565d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(precision, recall):\n",
    "    f1 = (2 * precision * recall)/(precision + recall) if precision+recall > 0 else 0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f2a97-d428-479d-aeaf-8b289a58421a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10ed29c-3ecd-466a-a3e4-07b31aeef117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3c9413-dcdb-458a-b0ab-ac2615e136db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d632066a-6435-4aa2-856d-eb81706ae02f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58167f-2a43-46c7-be08-334d6eb93356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61f178-20a4-4464-99bc-a26c170fffda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257c9fa3-0ece-4a5d-b9b2-469df3149652",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84625358-14fc-41ef-b9ce-3927453d73e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b736a-859e-4e88-9b52-e508d855acb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e88b8b-cf2d-4506-978b-b5fc264e00a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeff986-c881-48c3-91d0-d96728541f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73587aec-22b2-4558-98a3-3b07df4f31e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3b22c4-0d99-492a-bda4-4d728efa64c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e36fce-9fb3-4da3-a7e0-b113831ef1d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e0d47-596d-4042-824e-0aa6ab132e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b6eff-e66b-4b28-977e-1c622a5b701c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eca536-6ec0-4e8c-85d4-71264b83b65e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76783d4b-8516-46f5-955d-6ffce5a754ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c370262-2e53-4157-aae6-fb91f721f829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
