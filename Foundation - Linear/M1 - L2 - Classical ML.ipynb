{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cefc40dd-9258-4e6d-b40f-ed7b0428a9ad",
   "metadata": {},
   "source": [
    "# **Classical ML**\n",
    "\n",
    "Welcome to **Module L2: Classical ML**.\n",
    "\n",
    "In L1, we built the *engine* (Linear Algebra). In L2, we teach the engine how to *drive itself* (Learning).\n",
    "\n",
    "We are shifting focus from \"calculating outputs\" to **optimizing weights**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b727d1e4-5c82-4ee8-b15a-f145b0792910",
   "metadata": {},
   "source": [
    "### **Concept 1:**\n",
    "Imagine you are trying to predict house prices based on their size. Linear Regression simply tries to draw the \"best-fitting straight line\" through that data. Mathematically, it tries to find the optimal weights ($w$) and bias ($b$) so that the difference between your predicted value ($\\hat{y} = wx + b$) and the actual value ($y$) is minimized.\n",
    "\n",
    "### **The Intuition: Drawing the Best Line**\n",
    "\n",
    "Imagine you have a piece of graph paper.\n",
    "* **X-axis:** Number of hours studied.\n",
    "* **Y-axis:** Exam score.\n",
    "\n",
    "You plot 5 dots representing 5 students. The dots go generally up (more study = higher score), but they aren't in a perfect straight line.\n",
    "\n",
    "**The Goal:**\n",
    "We want to draw **one straight line** through those dots that allows us to predict the score for *any* number of hours studied.\n",
    "\n",
    "**The Problem:**\n",
    "You can draw infinite lines. Some are too steep, some are too flat, some are too high. How do you mathematically prove which line is the \"best\"?\n",
    "\n",
    "### **The \"Why\": Measuring the Mistake**\n",
    "\n",
    "To find the best line, we need a way to score how \"bad\" a line is. We call this the **Cost Function** or **Loss Function**.\n",
    "\n",
    "1.  **Calculate the Error:** We look at a specific dot. We measure the vertical distance between the **actual dot** (Real Score) and the **line** (Predicted Score). That distance is the \"Error.\"\n",
    "2.  **Square It:** Why square it?\n",
    "    * *Reason 1:* Sometimes the dot is below the line (negative error). If we just added up the errors, $-5$ and $+5$ would cancel out to $0$, making us think the line is perfect when it's not. Squaring makes everything positive.\n",
    "    * *Reason 2:* It punishes big mistakes. Being off by 10 points ($10^2 = 100$) is much worse than being off by 1 point ($1^2 = 1$).\n",
    "3.  **Take the Average:** We add up all those squared errors and divide by the number of students.\n",
    "\n",
    "This gives us the **Mean (Average) Squared Error (MSE)**.\n",
    "\n",
    "**The \"best fit line\" is simply the line that results in the lowest possible MSE.**\n",
    "\n",
    "\n",
    "**The Micro-Task:**\n",
    "Before we use Scikit-Learn, I want you to understand the cost function.\n",
    "1.  Create two NumPy arrays: `y_true` (actual values) and `y_pred` (predicted values).\n",
    "2.  Write a Python function `calculate_mse(y_true, y_pred)` from scratch (using NumPy, no sklearn yet) that calculates the **Mean Squared Error (MSE)**.\n",
    "\n",
    "*Equation hint: $MSE = \\frac{1}{n} \\sum (y_{true} - y_{pred})^2$*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "146e7afc-e687-460f-9164-94e6486a463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Loop MSE = 136.4 took 0.0015187263488769531 sec\n",
      "NP Mean MSE = 136.4 took 0.0 sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "## Create some random y_true and y_pred\n",
    "y_true = np.array([55, 74, 49, 82, 94])\n",
    "y_pred = np.array([50, 60, 70, 80, 90])\n",
    "\n",
    "start_time1 = time.time()\n",
    "mse1 = 0\n",
    "n = len(y_true)\n",
    "for y in range(n):\n",
    "    mse1 += ((y_true[y]-y_pred[y])**2)/n\n",
    "\n",
    "end_time1 = time.time()\n",
    "total_time1 = end_time1 - start_time1\n",
    "start_time2 = time.time()\n",
    "mse2 = np.mean((y_true - y_pred)**2)\n",
    "end_time2 = time.time()\n",
    "total_time2 = end_time2 - start_time2\n",
    "print(f\"Traditional Loop MSE = {mse1} took {total_time1} sec\\nNP Mean MSE = {mse2} took {total_time2} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a422c9-8deb-41ce-9991-60a126bce938",
   "metadata": {},
   "source": [
    "**Perfect.** You have just built the engine that measures \"how wrong\" a model is.\n",
    "\n",
    "Now, we need the engine that **fixes** the mistake.\n",
    "\n",
    "---\n",
    "\n",
    "### **Concept 2: Gradient Descent**\n",
    "\n",
    "**The \"Why\":**\n",
    "We calculated the MSE (the error). Our goal is to make that MSE as close to 0 as possible. We do this by changing the weights (the slope of our line).\n",
    "\n",
    "**The Intuition (The Mountain Analogy):**\n",
    "Imagine you are standing on top of a mountain blindfolded.\n",
    "* **Height:** The Error (MSE). You want to get to the bottom (Zero Error).\n",
    "* **Position:** Your current Weights.\n",
    "* **Strategy:** You feel the ground with your foot. If it slopes down to the right, you take a step right. If it slopes down to the left, you take a step left.\n",
    "\n",
    "**The Math:**\n",
    "We calculate the **Gradient** (the slope of the mountain at your feet). Then we take a step in the *opposite* direction of the slope to go downhill.\n",
    "\n",
    "The formula to update our weight ($w$) is:\n",
    "\n",
    "$$w_{new} = w_{old} - (\\text{learning\\_rate} \\times \\text{gradient})$$\n",
    "\n",
    "* **Learning Rate:** How big of a step you take. (Too big = you jump over the valley; Too small = it takes forever).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Micro-Task: The Single Step**\n",
    "\n",
    "I want you to write a function that performs **one single step** of Gradient Descent.\n",
    "\n",
    "**Task:**\n",
    "1.  Define a function `update_weight(weight, gradient, learning_rate)`.\n",
    "2.  It should return the new weight using the formula above.\n",
    "3.  Test it with:\n",
    "    * `weight = 10`\n",
    "    * `gradient = 2` (Slope is positive, so we should go down/left)\n",
    "    * `learning_rate = 0.1`\n",
    "\n",
    "**Write the code and tell me the new weight.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3bb35c-d6cf-4f90-b353-06b50d46cdfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d1467c-7537-46e6-9284-3939424ecfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e48d2ec-e142-4b7d-bfef-13b2c699ad67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc661be-7dcb-4a7a-8dc8-3c2bf2453f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070f114-6765-499b-aebb-a6088340dfb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
