{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f70e1d8-157c-4fc9-9946-28656ccb7da1",
   "metadata": {},
   "source": [
    "### 1. Why is OOP \"Good\"?\n",
    "\n",
    "Right now, your code is likely a list of instructions: \"Do A, then B, then C.\" This works for small scripts, but as projects grow, it becomes \"Spaghetti Code\", messy and hard to untangle.\n",
    "\n",
    "OOP changes the mental model. Instead of a list of instructions, you build **smart objects** that know how to handle themselves.\n",
    "\n",
    "* **Organization (Encapsulation):** You group related data (variables) and behavior (functions) into a single container. The `DataGenerator` keeps its secrets (like the random seed) to itself. The rest of your code doesn't need to worry about it.\n",
    "* **Reusability (Blueprints):** A Class is like a **Blueprint**. Once you write the code for a `Car`, you can create 100 distinct cars without rewriting code.\n",
    "* **Safety:** If a variable is hidden inside an object, other parts of your code can't accidentally change it and break things.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Anatomy of a Class\n",
    "\n",
    "Think of a Class as a **template**.\n",
    "\n",
    "* **Attributes (Variables):** What the object *knows* (e.g., color, size, name).\n",
    "* **Methods (Functions):** What the object *does* (e.g., drive, bark, calculate).\n",
    "\n",
    "#### The \"Self\" Concept (The most confusing part!)\n",
    "\n",
    "You will see `self` everywhere.\n",
    "\n",
    "* Imagine you have a blueprint for a Human.\n",
    "* `self.name` means: \"The name of **this specific** human I am building right now.\"\n",
    "* Without `self`, the code wouldn't know if you meant \"Humanity's name\" or \"Ujwal's name.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 3. A Sample Class (The \"Coffee Machine\")\n",
    "\n",
    "Let's look at a simple example unrelated to your housing data so you can see the structure clearly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e3ff2a-59c9-46ac-8394-163332b4aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Machine is OFF. Please turn it on.\n",
      "Machine is now ON.\n",
      "Brewing a hot cup of Arabica coffee...\n",
      "Water remaining: 300ml\n",
      "Brewing a hot cup of Arabica coffee...\n",
      "Water remaining: 100ml\n",
      "Error: Not enough water!\n"
     ]
    }
   ],
   "source": [
    "class CoffeeMachine:\n",
    "    \"\"\"\n",
    "    A blueprint for a smart coffee machine.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. The Setup (__init__)\n",
    "    # This runs automatically effectively when you create a new object.\n",
    "    # It sets the initial \"State\".\n",
    "    def __init__(self, water_level, bean_type):\n",
    "        self.water = water_level   # Attribute: The machine remembers its water level\n",
    "        self.beans = bean_type     # Attribute: The machine remembers the bean type\n",
    "        self.is_on = False         # Attribute: Default state is OFF\n",
    "\n",
    "    # 2. A Method (Behavior)\n",
    "    # This modifies the object's state\n",
    "    def turn_on(self):\n",
    "        self.is_on = True\n",
    "        print(\"Machine is now ON.\")\n",
    "\n",
    "    # 3. Another Method (Logic)\n",
    "    # This uses the object's state to do something\n",
    "    def make_coffee(self):\n",
    "        if not self.is_on:\n",
    "            print(\"Error: Machine is OFF. Please turn it on.\")\n",
    "            return\n",
    "        \n",
    "        if self.water < 200:\n",
    "            print(\"Error: Not enough water!\")\n",
    "        else:\n",
    "            self.water = self.water - 200 # Update the state (consume water)\n",
    "            print(f\"Brewing a hot cup of {self.beans} coffee...\")\n",
    "            print(f\"Water remaining: {self.water}ml\")\n",
    "\n",
    "# --- HOW TO USE IT ---\n",
    "\n",
    "# 1. Create an Instance (Build the object from the blueprint)\n",
    "# \"my_machine\" is now a real object.\n",
    "my_machine = CoffeeMachine(water_level=500, bean_type=\"Arabica\")\n",
    "\n",
    "# 2. Interact with it\n",
    "my_machine.make_coffee() # Fails because it's off\n",
    "my_machine.turn_on()     # Changes state to ON\n",
    "my_machine.make_coffee() # Success! Consumes water.\n",
    "my_machine.make_coffee() # Success! Consumes water.\n",
    "my_machine.make_coffee() # Fails! Not enough water left."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce83232-ca07-4c28-9baf-2bfa3af8aad5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Coming back to Ensemble Methods\n",
    "\n",
    "### 1. The Mental Model: \"The Factory Line\"\n",
    "\n",
    "Think of your code as a factory floor. You need three distinct machines.\n",
    "\n",
    "* **Machine A (Generator):** You give it raw settings; it spits out raw materials (Data).\n",
    "* **Machine B (Preprocessor):** It takes raw materials, cleans them, and prepares them for assembly. It needs to remember how it cleaned the last batch so it can do the same for the next.\n",
    "* **Machine C (Trainer):** It takes prepared materials and learns how to build the final product (Model).\n",
    "\n",
    "### 2. How to Design Your Classes\n",
    "\n",
    "In Python, a class generally needs two things:\n",
    "\n",
    "1. **State (Attributes/`self`):** What does the machine need to *know* or *remember*? (e.g., the random seed, the saved model, the scaler settings).\n",
    "2. **Behavior (Methods):** What does the machine *do*? (e.g., `generate()`, `clean()`, `train()`).\n",
    "\n",
    "Here is how you should structure your three classes:\n",
    "\n",
    "#### Class 1: The Data Generator\n",
    "\n",
    "* **Goal:** Encapsulate all the fake data logic so it doesn't clutter your main code.\n",
    "* **What it needs to know (`__init__`):**\n",
    "* How many samples do you want? (`n=1000`)\n",
    "* What is the random seed? (So it's reproducible).\n",
    "\n",
    "\n",
    "* **What it does (Methods):**\n",
    "* `create_data()`: This method should contain that updated math/logic we discussed (linear relationship). It returns a DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "#### Class 2: The Preprocessor (CRITICAL)\n",
    "\n",
    "* **Goal:** This is the trickiest one. It must **remember** the math it used on the training set (e.g., the mean value for filling missing data) so it can apply the *exact same math* to the test set.\n",
    "* **What it needs to know (`__init__`):**\n",
    "* It needs a place to store the \"pipeline\" or \"transformer\" object once it's created. Initially, this is `None`.\n",
    "\n",
    "\n",
    "* **What it does (Methods):**\n",
    "* `fit_transform(X_train)`: Learns the mean/mode from training data and transforms it. **Save the learner to `self**`.\n",
    "* `transform(X_test)`: Uses the *saved* learner to transform new data. **Do not learn new means here!**\n",
    "\n",
    "\n",
    "\n",
    "#### Class 3: The Model Trainer\n",
    "\n",
    "* **Goal:** Handle the messy parts of machine learning (GridSearch, fitting, predicting).\n",
    "* **What it needs to know (`__init__`):**\n",
    "* Which algorithm are we using? (XGBoost).\n",
    "* The model object itself.\n",
    "\n",
    "\n",
    "* **What it does (Methods):**\n",
    "* `train(X, y)`: Runs the GridSearch you learned earlier. It should find the best parameters and update `self.model` with the winner.\n",
    "* `evaluate(X, y)`: Uses `self.model` to predict and return the error score (RMSE).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What to Look Out For (The \"Gotchas\")\n",
    "\n",
    "**1. The \"State\" Trap (Data Leakage)**\n",
    "\n",
    "* *The Mistake:* Creating a preprocessor inside a function, using it, and letting it die.\n",
    "* *The Fix:* Your `Preprocessor` class must survive between the training step and the testing step. If you re-initialize the class before testing, it \"forgets\" the training mean and calculates a new mean from the test data. **This is cheating (Data Leakage).**\n",
    "\n",
    "**2. Hardcoding Values**\n",
    "\n",
    "* *The Mistake:* Writing `price = size * 150` inside the generate method and leaving it there.\n",
    "* *The Fix:* Try to pass these \"magic numbers\" as arguments or constants if you can, but for now, just keeping them inside the class is better than global variables.\n",
    "\n",
    "**3. The \"God Object\"**\n",
    "\n",
    "* *The Mistake:* Making one giant class called `HousingSystem` that does everything.\n",
    "* *The Fix:* Keep them separate. The `Generator` shouldn't know that XGBoost exists. The `Trainer` shouldn't care how the data was cleaned, only that it *is* cleaned.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Don't worry about the whole thing yet. **Start by writing just the first class: `HousingDataGenerator`.**\n",
    "\n",
    "Draft it out:\n",
    "\n",
    "1. Define the class.\n",
    "2. Write the `__init__` to accept `n_samples`.\n",
    "3. Write a method called `get_data()` that includes your *fixed* logic (with the price formula)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "497251fe-8004-4727-a365-7a92764f564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class HousingDataGenerator():\n",
    "    \"\"\"\n",
    "    Encapsulate all the fake data logic so it doesn't clutter your main code.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples = 1000, seed_val = 42):\n",
    "        self.n_samples = n_samples\n",
    "        self.seed_val = seed_val\n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "            np.random.seed(self.seed_val)\n",
    "            n = self.n_samples\n",
    "            \n",
    "            # 1. Generate Features\n",
    "            size = np.random.normal(1500, 500, n)\n",
    "            year = np.random.randint(1950, 2024, n)\n",
    "            neighborhood = np.random.choice(['Downtown', 'Suburb', 'Rural'], n)\n",
    "            style = np.random.choice(['Modern', 'Victorian', 'Ranch', np.nan], n)\n",
    "            \n",
    "            # 2. Define the \"Rules\" (The Relationship)\n",
    "            # Start with a Base Price\n",
    "            price = 50000 + (size * 150) + ((year - 1950) * 1000)\n",
    "            \n",
    "            # Add value for Neighborhoods\n",
    "            # (We add boolean logic: if Downtown, add 50k, etc.)\n",
    "            price += np.where(neighborhood == 'Downtown', 50000, 0)\n",
    "            price += np.where(neighborhood == 'Suburb', 20000, 0)\n",
    "            \n",
    "            # 3. Add some \"Noise\" (Randomness)\n",
    "            # Real life isn't perfect math, so we add a little random fluctuation\n",
    "            noise = np.random.normal(0, 20000, n)\n",
    "            final_price = price + noise\n",
    "            \n",
    "            df = pd.DataFrame({\n",
    "                'Size_sqft': size,\n",
    "                'Year_Built': year,\n",
    "                'Neighborhood': neighborhood,\n",
    "                'Style': style,\n",
    "                'Price': final_price \n",
    "            })\n",
    "            # Add NaNs as before\n",
    "            df.loc[np.random.choice(n, 50), 'Size_sqft'] = np.nan\n",
    "            return df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3988e0-2611-4219-ab55-c1d543e6a240",
   "metadata": {},
   "source": [
    "This is **perfect**. You have successfully created your first \"Machine\" in the factory.\n",
    "\n",
    "* **Structure:** Your `__init__` correctly stores the settings.\n",
    "* **Logic:** Your `get_data` correctly uses `self.n_samples` and `self.seed_val`.\n",
    "* **Result:** This class is now a reusable tool. You can create `gen1 = HousingDataGenerator(n_samples=500)` and `gen2 = HousingDataGenerator(n_samples=5000)` without rewriting any logic.\n",
    "\n",
    "---\n",
    "\n",
    "### Class 2: The \"Cleaner\" (DataPreprocessor)\n",
    "\n",
    "This is the hardest class, so we will take it slowly.\n",
    "\n",
    "**The Goal:**\n",
    "In your script version, you probably did `fit_transform` on the whole dataset or re-fitted on the test set. **That is dangerous.**\n",
    "The `DataPreprocessor` class must act like a smart robot that:\n",
    "\n",
    "1. **Learns** from the Training Data (calculates means, modes).\n",
    "2. **Remembers** those values (stores them in `self`).\n",
    "3. **Applies** exactly those values to the Test Data (no peeking!).\n",
    "\n",
    "\n",
    "### Class: `DataPreprocessor`\n",
    "\n",
    "**Goal:** Create a machine that learns transformation rules (imputing, scaling) from training data and applies them strictly to test data.\n",
    "\n",
    "#### Step 1: The `__init__` Method\n",
    "\n",
    "* **Action:** Initialize a variable (let's call it `self.preprocessor`) to `None`.\n",
    "* **Why:** This variable will eventually hold your \"Pipeline\" object (the machine that does the actual cleaning). We start with `None` because we haven't built the machine yet.\n",
    "\n",
    "#### Step 2: The `create_pipeline` Method\n",
    "\n",
    "* **Action:** This is where you define your cleaning rules. You need to create a `ColumnTransformer`.\n",
    "* **Instruction A (Numeric):** Create a pipeline for numeric columns (`Size_sqft`, `Year_Built`).\n",
    "    * It should first Impute missing values (using 'mean').\n",
    "    * It should then Scale values (StandardScaler).\n",
    "\n",
    "\n",
    "* **Instruction B (Categorical):** Create a pipeline for categorical columns (`Neighborhood`, `Style`).\n",
    "    * It should first Impute missing values (using 'most_frequent').\n",
    "    * It should then One-Hot Encode (handle_unknown='ignore').\n",
    "\n",
    "\n",
    "* **Instruction C (Combine):** Use `ColumnTransformer` to bundle these two pipelines together.\n",
    "\n",
    "* **Critical Final Step:** Assign this `ColumnTransformer` object to `self.preprocessor`.\n",
    "\n",
    "#### Step 3: The `process_and_split` Method\n",
    "\n",
    "* **Input:** Accepts a raw DataFrame (`df`) and the name of the target column (`target_col`).\n",
    "* **Logic:**\n",
    "   1. **Check:** If `self.preprocessor` is still `None`, run `self.create_pipeline()` to build it.\n",
    "   2. **Separate:** Split your DataFrame into Features (`X`) and Target (`y`).\n",
    "   3. **Split:** Use `train_test_split` to create your 4 arrays: `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "   4. **The Golden Rule:**\n",
    "* On `X_train`: Run **`fit_transform`** using `self.preprocessor`. (This learns the means/standard deviations AND changes the data).\n",
    "* On `X_test`: Run **`transform`** using `self.preprocessor`. (This only changes the data using the *learned* means. It does NOT learn new ones).\n",
    "\n",
    "* **Return:** The four processed arrays.\n",
    "\n",
    "---\n",
    "\n",
    "**Your Turn:**\n",
    "Write the `DataPreprocessor` class based on these instructions. Focus on getting the `fit_transform` vs `transform` logic correct in step 3—that is the most common interview question in this field!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7db1ba15-8db3-466b-9eb8-af3a782e3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    Create a machine that learns transformation rules (imputing, scaling) from training data and applies them strictly to test data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.preprocessor = None\n",
    "        self.random_seed = 42\n",
    "        self.test_size = 0.2\n",
    "\n",
    "    \n",
    "    def create_pipeline(self):\n",
    "        \"\"\"\n",
    "        Here is where you define the RULES for cleaning.\n",
    "        (Imputers, Scalers, OneHotEncoders)\n",
    "        \"\"\"\n",
    "        num_cols = [\"Size_sqft\", \"Year_Built\"]\n",
    "        cat_cols = [\"Neighborhood\", \"Style\"]\n",
    "        \n",
    "        num_pipeline = Pipeline([\n",
    "            ## Name, Function\n",
    "            (\"impute\", SimpleImputer(strategy = \"mean\")),\n",
    "            (\"scale\", StandardScaler())\n",
    "        ])\n",
    "\n",
    "        cat_pipeline = Pipeline([\n",
    "            ## Name, Function\n",
    "            (\"impute\", SimpleImputer(strategy = \"most_frequent\")),\n",
    "            (\"encoder\", OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        transform = ColumnTransformer(\n",
    "            transformers = [\n",
    "                ## Name, Pipeline, Columns\n",
    "                (\"num\", num_pipeline, num_cols),\n",
    "                (\"cat\", cat_pipeline, cat_cols)\n",
    "            ],\n",
    "            remainder = \"passthrough\"\n",
    "        )\n",
    "        self.preprocessor = transform\n",
    "        \n",
    "\n",
    "    def test_train_fit_data(self, df, target_col = \"Price\"):\n",
    "        \"\"\"\n",
    "        This splits and fits data from the df to test and training set\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.preprocessor is None:\n",
    "            self.create_pipeline()\n",
    "            \n",
    "        X = df.drop(columns = target_col, axis = 1)\n",
    "        y = df[target_col]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = self.test_size, random_state = self.random_seed)\n",
    "        \n",
    "        X_train_processed = self.preprocessor.fit_transform(X_train) ## This learns the means/standard deviations AND changes the data\n",
    "        X_test_processed = self.preprocessor.transform(X_test) ## This only changes the data using the learned means. It does NOT learn new ones\n",
    "        \n",
    "        return X_train_processed, X_test_processed, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dc2f0-081a-41aa-a654-c11f3671d2b3",
   "metadata": {},
   "source": [
    "### Class 3: `ModelTrainer`\n",
    "\n",
    "**Goal:** Initialize three different models, train them all, compare their scores, and save the winner.\n",
    "\n",
    "#### Step 1: The `__init__` Method\n",
    "\n",
    "* **Action:**\n",
    "* Create a dictionary called `self.models`.\n",
    "* **Keys:** Strings like \"Linear\", \"RandomForest\", \"XGBoost\".\n",
    "* **Values:** The actual model objects (e.g., `LinearRegression()`, `RandomForestRegressor(random_state=42)`, etc.).\n",
    "* Initialize `self.best_model` to `None`.\n",
    "\n",
    "\n",
    "#### Step 2: The `train_and_evaluate` Method\n",
    "\n",
    "* **Input:** Accepts all your data: `X_train`, `y_train`, `X_test`, `y_test`.\n",
    "* **Logic (The Loop):**\n",
    "* Create a variable `best_rmse` and set it to `float('inf')` (infinity) so the first model always beats it.\n",
    "* **Loop** through your `self.models` dictionary.\n",
    "* For each model:\n",
    "   1. **Fit** it on `X_train` and `y_train`.\n",
    "   2. **Predict** on `X_test`.\n",
    "   3. **Calculate RMSE** (Root Mean Squared Error).\n",
    "   4. **Print** the result (e.g., `\"Linear Regression RMSE: 10200\"`).\n",
    "   5. **Compare:** If this model's RMSE is lower than `best_rmse`:\n",
    "* Update `best_rmse`.\n",
    "* Save this model to `self.best_model`.\n",
    "\n",
    "\n",
    "* **Final Output:** Print which model won and the final score.\n",
    "\n",
    "#### Optional (Bonus Step): Hyperparameters\n",
    "\n",
    "* If you want to keep it simple, just use standard `.fit()`.\n",
    "* If you want to be fancy, you can add a `GridSearchCV` inside the loop, but I recommend getting the simple loop working first before adding Grid Search back in.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c4fdd3b-493e-4ca3-a682-8190dcfd958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from xgboost import XGBRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class ModelTrainer():\n",
    "    \"\"\"\n",
    "     Creates three different models, train them all, compare their scores, and save the winner.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_estimators = 100\n",
    "        self.learning_rate = 0.1\n",
    "        self.random_seed = 42\n",
    "        self.best_rmse = float(\"inf\")\n",
    "        \n",
    "        self.models = {\n",
    "            \"Linear\": LinearRegression(),\n",
    "            \"RandomForest\": RandomForestRegressor(random_state = self.random_seed),\n",
    "            \"XGBoost\": XGBRegressor(n_estimators = self.n_estimators, \n",
    "                                    learning_rate = self.learning_rate, \n",
    "                                    random_state = self.random_seed)\n",
    "        }\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        \n",
    "\n",
    "    def train_and_evaluate(self, X_train, y_train, X_test, y_test):\n",
    "        for k, v in self.models.items():\n",
    "            print(f\"Starting evalutation for {k}\")\n",
    "            model_fit = v.fit(X_train, y_train)\n",
    "            y_pred = model_fit.predict(X_test)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            if rmse < self.best_rmse :\n",
    "                self.best_rmse = rmse\n",
    "                self.best_model = v\n",
    "                self.best_model_name = k\n",
    "            print(f\"Root Mean Squared Error = {rmse}\\n==========================\")\n",
    "\n",
    "        print(f\"The best model is {self.best_model_name} with a RMSE = {self.best_rmse}\")\n",
    "\n",
    "        return self.best_model_name, self.best_model\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2dbc30-9368-4f5f-9a3e-a82b522306fe",
   "metadata": {},
   "source": [
    "\n",
    "You have built:\n",
    "\n",
    "1. **`HousingDataGenerator`**: Creates data.\n",
    "2. **`DataPreprocessor`**: Cleans data.\n",
    "3. **`ModelTrainer`**: Trains and compares models.\n",
    "\n",
    "Now, we need the **Main Execution Block** to tie them all together. This is where you push the \"Start\" button on your factory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6895076e-eb18-4973-a80e-3c817ac13b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evalutation for Linear\n",
      "Root Mean Squared Error = 31316.214460850868\n",
      "==========================\n",
      "Starting evalutation for RandomForest\n",
      "Root Mean Squared Error = 37729.77183190089\n",
      "==========================\n",
      "Starting evalutation for XGBoost\n",
      "Root Mean Squared Error = 37646.25156837973\n",
      "==========================\n",
      "The best model is Linear with a RMSE = 31316.214460850868\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = HousingDataGenerator().get_data()\n",
    "    \n",
    "    DP = DataPreprocessor()\n",
    "    X_train, X_test, y_train, y_test = DP.test_train_fit_data(df)\n",
    "    \n",
    "    mt = ModelTrainer()\n",
    "    \n",
    "    best_model_name, best_model = mt.train_and_evaluate(X_train, y_train, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30060e22-9c18-4080-b32a-45af634f40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b28a7c-9857-4959-b601-e452818a5947",
   "metadata": {},
   "source": [
    "\n",
    "### Class 4: The \"Shipper\" (ModelSaver)\n",
    "\n",
    "A model is useless if it stays stuck in your Python script. In the real world, we need to **save** it to a file so we can send it to a website, an app, or a cloud server.\n",
    "\n",
    "We call this \"Serialization\" (or \"Pickling\").\n",
    "\n",
    "#### \"Serialization\" (Saving the Game)\n",
    "\n",
    "Imagine you are playing a video game. If you turn off the console without saving, you lose all your progress.\n",
    "\n",
    "   * **Running your Python script** is like playing the game. The model learns and gets smarter.\n",
    "   * **Closing Python** is like turning off the console. The model \"dies\" and forgets everything.\n",
    "   * **\"Pickling\" (Serialization)** is simply hitting the **\"Save Game\"** button. It freezes your smart model into a file (like `savefile.pkl`) so you can open it tomorrow, or send it to a friend, and it remembers everything it learned.\n",
    "\n",
    "\n",
    "**Your Final Challenge:** Write the `ModelSaver` class.\n",
    "\n",
    "#### The \"Gotcha\" (Critical Concept) ⚠️\n",
    "\n",
    "Most beginners just save the model. **That is a mistake.**\n",
    "If you load the model later and try to predict a house with `Size=2000`, the model will crash because it expects **scaled data** (e.g., `Size=0.5`), not raw numbers.\n",
    "\n",
    "**You must save BOTH:**\n",
    "\n",
    "1. The Trained Model (`best_model`)\n",
    "2. The Fitted Preprocessor (`DP.preprocessor`)\n",
    "\n",
    "**The Problem:** Your model DOES NOT understand real-world numbers anymore.\n",
    "   * You taught it that a \"big house\" is `Size = 1.5` (scaled) and a \"small house\" is `Size = -0.5`.\n",
    "   * If a user on your website types `Size = 2000`, the model will panic. It thinks `2000` is a gigantic, impossible number because it expects small, scaled numbers.\n",
    "\n",
    "   * **The Preprocessor:** This is your **Translator**. It knows exactly how to turn \"2000\" into \"1.5\".\n",
    "   * **The Lesson:** If you save *only* the Model, you are saving a genius who speaks a secret language, but you forgot to pack their dictionary. You must save **both** the Model (the brain) and the Preprocessor (the translator) together.\n",
    "\n",
    "\n",
    "#### Instructions for Class 4: `ModelSaver`\n",
    "\n",
    "1. **Library:** You will need `import joblib` (standard for saving AI models).\n",
    "2. **`__init__`**: It doesn't strictly *need* anything, but you could pass a default folder name if you wanted. For now, keep it empty.\n",
    "3. **`save(self, model, preprocessor, filename)`**:\n",
    "* Create a dictionary to bundle them together:\n",
    "```python\n",
    "artifact = {\n",
    "    \"model\": model,\n",
    "    \"preprocessor\": preprocessor\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "* Use `joblib.dump(artifact, filename)` to save that dictionary to a file (usually ending in `.pkl` or `.joblib`).\n",
    "* Print a confirmation message (\"Model saved to...\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ba9f82a-b9d7-4ddd-aa29-a4b60e5598d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "class ModelSaver:\n",
    "    \"\"\"\n",
    "    Saves the Model and the Preprocessor to a single file.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename = r\"Data\\m1-l3-model.pkl\"):\n",
    "        self.filename = filename\n",
    "\n",
    "    def save_model(self, model, preprocessor, model_name):\n",
    "        # Bundle them together!\n",
    "        # This is the crucial step: We save a DICTIONARY containing both.\n",
    "        artifact = {\n",
    "            \"model\": model,\n",
    "            \"model_name\": model_name,\n",
    "            \"preprocessor\": preprocessor\n",
    "        }\n",
    "        \n",
    "        # 'dump' means 'save to file'\n",
    "        joblib.dump(artifact, self.filename)\n",
    "        print(f\"Success! Model {model_name} and Preprocessor saved to {self.filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2095f7e-5521-4af6-9ae2-b2d7338042cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Model Linear and Preprocessor saved to Data\\m1-l3-model.pkl\n"
     ]
    }
   ],
   "source": [
    "saver = ModelSaver()\n",
    "saver.save_model(best_model, DP.preprocessor, best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c247ec-5ba8-4b16-bf4e-c796cce997a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52817031-1430-4888-9950-cb38f9963437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e339e40-c7f2-4354-8da3-79226eec78bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0007fef-2777-4aa4-ae75-963a5631226f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f651d6-5445-4b25-b621-e52a6a4f783f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6580c8bc-bd39-4356-b6cd-0dc811ddcc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddc55cb-4229-4b69-90a9-994dbef85496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c455cb-4694-49fd-bc28-abc43f1e859b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ae04b9-2fb6-4f98-9335-cde4354dc138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735ca19-1b30-439f-b463-cca0a249233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e6bed-e827-43c9-a4f3-05049567c7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71651790-e8cd-4fa6-a5a9-fdfc2edb06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f70e5a5-f324-407f-b061-369e0deeab39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
