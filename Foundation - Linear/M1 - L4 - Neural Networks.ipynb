{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087e1ade-e346-484e-ab93-8b43672ee291",
   "metadata": {},
   "source": [
    "# **Neural Networks**\n",
    "\n",
    "At its core, a Neural Network is a mathematical function that maps an input to an output. However, unlike a simple linear regression equation (y = mx + c), a neural network is composed of layers of interconnected \"neurons\" that can approximate incredibly complex, non-linear functions.\n",
    "\n",
    "### The Biological Inspiration\n",
    "\n",
    "The concept is loosely inspired by the human brain.\n",
    "\n",
    "   * **Biological Neuron:** Receives electrical signals through dendrites. If the signal is strong enough, it \"fires\" (activates) and sends the signal down the axon to other neurons.\n",
    "   * **Artificial Neuron:** Receives numerical inputs. It multiplies them by specific \"weights\" (importance), adds a \"bias\" (threshold), and passes the result through an \"activation function\" (decision rule).\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "A standard Neural Network consists of three types of layers:\n",
    "\n",
    "   1. **Input Layer:** The raw data enters here (e.g., pixels of an image, words in a sentence). No computation happens here; it's just a doorway.\n",
    "   2. **Hidden Layers:** The \"magic\" happens here. These layers transform the input data into abstract features. Deep Learning simply refers to networks with many hidden layers.\n",
    "   3. **Output Layer:** The final decision (e.g., \"Cat\" vs. \"Dog\", or a stock price prediction).\n",
    "\n",
    "### How It \"Learns\"\n",
    "\n",
    "A neural network is initially dumb. It starts with random weights.\n",
    "\n",
    "   1. **Forward Pass (Guessing):** Data flows through the network. It makes a guess.\n",
    "   2. **Loss Calculation (Grading):** We compare the guess to the actual answer. The difference is the \"Loss\" or error.\n",
    "   3. **Backward Pass (Learning):** We use calculus (Chain Rule) to calculate which weights contributed most to the error.\n",
    "   4. **Optimization (Correction):** We adjust the weights slightly in the opposite direction of the error to reduce it next time.\n",
    "\n",
    "**Analogy:** Imagine trying to tune a radio to a specific clear station (the optimal function). You turn the knobs (weights) randomly at first. You hear static (high loss). You turn one knob slightly; the static gets worse. You turn it the other way; the static gets better. You keep doing this for all knobs until the signal is clear.\n",
    "\n",
    "---\n",
    "\n",
    "## **Topic Tree**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cf9986f-5120-4dd1-91d7-7768c8b3b4bc",
   "metadata": {},
   "source": [
    "L4: Neural Networks from Scratch\n",
    "â”œâ”€â”€ Concept 1: The Linear Layer (Forward Propagation)\n",
    "â”‚   â”œâ”€â”€ Y = WX + b\n",
    "â”‚   â”œâ”€â”€ Matrix dimensions and broadcasting\n",
    "â”‚   â”œâ”€â”€ Explanation: The \"skeleton\" of the network that transforms input space.\n",
    "â”‚   â””â”€â”€ Task: Implement a Linear class with weight initialization and forward pass.\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 2: Activation Functions (ReLU & Sigmoid)\n",
    "â”‚   â”œâ”€â”€ Introducing non-linearity\n",
    "â”‚   â”œâ”€â”€ Explanation: The \"brain\" that allows learning complex patterns (not just lines).\n",
    "â”‚   â””â”€â”€ Task: Implement forward methods for ReLU and Sigmoid.\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 3: Loss Function (Binary Cross-Entropy)\n",
    "â”‚   â”œâ”€â”€ Measuring error for binary classification\n",
    "â”‚   â”œâ”€â”€ MSE vs. Log Loss\n",
    "â”‚   â”œâ”€â”€ Explanation: A score telling the network how \"wrong\" it is.\n",
    "â”‚   â””â”€â”€ Task: Implement a function to calculate the cost.\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 4: The Chain Rule (Backpropagation Theory)\n",
    "â”‚   â”œâ”€â”€ Partial derivatives\n",
    "â”‚   â”œâ”€â”€ Local gradients vs. Global gradients\n",
    "â”‚   â”œâ”€â”€ Explanation: How blame is assigned to weights for the error.\n",
    "â”‚   â””â”€â”€ Task: Theoretical derivation (mental check).\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 5: Implementing Gradients (Backward Propagation)\n",
    "â”‚   â”œâ”€â”€ Calculating dW, db, and dZ manually\n",
    "â”‚   â”œâ”€â”€ Derivative of ReLU and Sigmoid\n",
    "â”‚   â”œâ”€â”€ Explanation: converting the math from Concept 4 into NumPy code.\n",
    "â”‚   â””â”€â”€ Task: Implement the backward pass methods for layers and activations.\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 6: Optimization (Stochastic Gradient Descent)\n",
    "â”‚   â”œâ”€â”€ W = W - learning_rate * gradients\n",
    "â”‚   â”œâ”€â”€ Explanation: Taking a step down the error mountain.\n",
    "â”‚   â””â”€â”€ Task: Implement a function to update parameters.\n",
    "â”‚\n",
    "â”œâ”€â”€ Concept 7: Data Preparation (Moons Dataset)\n",
    "â”‚   â”œâ”€â”€ sklearn.datasets.make_moons\n",
    "â”‚   â”œâ”€â”€ Reshaping for matrix ops\n",
    "â”‚   â”œâ”€â”€ Explanation: Creating the problem space.\n",
    "â”‚   â””â”€â”€ Task: Generate and visualize the dataset.\n",
    "â”‚\n",
    "â””â”€â”€ Mini-Project: Training the 2-Layer Neural Network\n",
    "    â”œâ”€â”€ Putting it all together\n",
    "    â””â”€â”€ Training loop (Forward -> Loss -> Backward -> Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97856f05-85ea-47d1-8b14-e4975483195b",
   "metadata": {},
   "source": [
    "## **Concept 1: The Linear Layer (Forward Propagation)**\n",
    "\n",
    "The fundamental building block of a neural network is the Linear Layer (also called the Dense or Fully Connected layer). It performs a linear transformation on the input.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "For a single layer, the mathematical operation is:\n",
    "\n",
    "$$Z = W \\cdot A + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $A$: The input to this layer (or the output of the previous layer).\n",
    "   * $W$: The Weight matrix (the \"knobs\" we tune).\n",
    "   * $b$: The Bias vector (shifts the activation up or down).\n",
    "   * $Z$: The linear output (pre-activation).\n",
    "\n",
    "**Dimensions (The Most Important Part)**\n",
    "To make the matrix multiplication work, the dimensions must align. We will use the convention where each **column** is a training example.\n",
    "\n",
    "   * $A$: Shape $(n_{in}, m)$ - where $n_{in}$ is input features,  is number of examples.\n",
    "   * $W$: Shape $(n_{out}, n_{in})$ - where $n_{out}$ is the number of neurons in this layer.\n",
    "   * $b$: Shape $(n_{out}, 1)$ -  Python broadcasting will apply this to all  examples.\n",
    "   * $Z$: Shape $(n_{out}, m)$.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Think of $W$ as a filter. If the input $A$ matches the pattern in row $i$ of $W$, the resulting dot product will be high. The bias $b$ acts as a threshold; if the weighted sum isn't high enough to overcome a negative bias, the neuron stays \"quiet\" (after activation).\n",
    "\n",
    "### Trade-offs & Pitfalls\n",
    "\n",
    "* **Initialization:** You cannot initialize $W$ to zeros. If you do, every neuron learns the exact same thing (symmetry problem). We usually initialize them with small random numbers. $b$ can be initialized to zeros.\n",
    "* **Broadcasting:** In Python/NumPy, if you aren't careful with the shape of $b$(e.g., `(n,)` vs `(n, 1)`), you might introduce subtle bugs where dimensions mismatch silently.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement two functions using **NumPy**:\n",
    "\n",
    "1. `initialize_parameters(n_in, n_out)`:\n",
    "* Initialize `W` with random values from a standard normal distribution, scaled by `0.01` (to keep values small).\n",
    "* Initialize `b` with zeros.\n",
    "* **Return:** `W`, `b`\n",
    "\n",
    "\n",
    "2. `linear_forward(A, W, b)`:\n",
    "   * Perform the dot product $W \\cdot A$.\n",
    "   * Add the bias $b$.\n",
    "   * **Return:** `Z` (the result), and a `cache` tuple containing `(A, W, b)` (we will need these later for backpropagation).\n",
    "\n",
    "\n",
    "\n",
    "**Input Constraints:**\n",
    "\n",
    "   * Use `numpy.random.randn` for weights.\n",
    "   * Ensure  has shape $(n_{out}, 1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b611d4a-8f6a-479a-b9b7-802f0b9f12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN1():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self.sd = 0.01\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Initialize W with random values from a standard normal distribution, scaled by 0.01 (to keep values small).\n",
    "        Initialize b with zeros.\n",
    "        Return: W, b\n",
    "        \"\"\"\n",
    "        W = self.sd * np.random.randn(n_out, n_in)\n",
    "        b = np.zeros((n_out, 1))\n",
    "        return W, b\n",
    "\n",
    "\n",
    "    def linear_forward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Perform the dot product: W.A \n",
    "        Add the bias b.\n",
    "        Return: Z (the result), and a cache tuple containing (A, W, b) (we will need these later for backpropagation).\n",
    "        \"\"\"\n",
    "\n",
    "        Z = np.dot(W, A) + b\n",
    "        return Z, (A, W, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4991fe-bb6e-4ba8-8e0c-053c83c71ec1",
   "metadata": {},
   "source": [
    "## **Concept 2: Activation Functions (ReLU & Sigmoid)**\n",
    "\n",
    "If we only used linear layers (like the one you just built), it wouldn't matter how many layers we stackedâ€”the final output would still be just a linear combination of the inputs. The network could only learn to draw straight lines (linear boundaries).\n",
    "\n",
    "**Activation Functions** introduce non-linearity, allowing the network to learn complex curves and shapes.\n",
    "\n",
    "### 1. ReLU (Rectified Linear Unit)\n",
    "    \n",
    "$$A = \\max(0, Z)$$\n",
    "\n",
    "   * **Intuition:** It's a gate. If the input is positive, let it through unchanged. If it's negative, shut it off (make it zero).\n",
    "   * **Context:** This is the default choice for **hidden layers**.\n",
    "   * **Why?** It is computationally cheap (just a `max` check) and helps prevent the \"vanishing gradient\" problem that older functions (like Tanh) suffered from.\n",
    "\n",
    "### 2. Sigmoid\n",
    "\n",
    "$$A = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "   * **Intuition:** It squashes any number (from  to ) into a range between **0 and 1**.\n",
    "   * **Context:** This is the standard choice for the **output layer** in binary classification.\n",
    "   * **Why?** It lets us interpret the output as a probability (e.g., \"70% chance this is a Moon\").\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Add these static methods or standalone functions to your class (or outside it, your choice).\n",
    "\n",
    "1. `sigmoid(Z)`:\n",
    "   * Implement the formula: $1 / (1 + np.exp(-Z))$\n",
    "   * **Return:** `A`, `cache` (where `cache` is just `Z`).\n",
    "\n",
    "\n",
    "2. `relu(Z)`:\n",
    "   * Implement: Return `Z` if `Z > 0`, else `0`. (Hint: `np.maximum` is faster than `max`).\n",
    "   * **Return:** `A`, `cache` (where `cache` is just `Z`).\n",
    "\n",
    "\n",
    "\n",
    "**Why return `Z` as cache?**\n",
    "Just like in the linear layer, we need the input `Z` to calculate the slope (derivative) of these functions later during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723c7f4-5272-4ce7-8b88-e1c3d1704e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class N12():\n",
    "    \"\"\"\n",
    "    Class with ReLU and Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "    def get_sigmoid(self):\n",
    "        A = 1/(1 + np.exp(-self.Z))\n",
    "        return A, self.Z\n",
    "\n",
    "    def get_ReLU(self):\n",
    "        A = np.maximum(0, self.Z)\n",
    "        return A, self.Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee650352-e475-4fb3-b53c-a956d14a980e",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 3: Loss Function (Binary Cross-Entropy)**\n",
    "\n",
    "Now that our network can make a prediction ($A$), we need to measure how \"wrong\" that prediction is. This measurement is called the **Loss** or **Cost**.\n",
    "\n",
    "Since we are doing **Binary Classification** (0 vs 1), we use **Binary Cross-Entropy Loss** (also called Log Loss).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "The cost $J$ over $m$ examples is:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(a^{(i)}) + (1 - y^{(i)}) \\log(1 - a^{(i)}) \\Big]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $y^{(i)}$: The true label (0 or 1).\n",
    "   * $a^{(i)}$: The network's prediction (probability between 0 and 1).\n",
    "\n",
    "### Intuition\n",
    "\n",
    "   * **If correct answer is 1 ($y=1$):** The second term $(1-y)$ becomes 0. We only care about $\\log(a)$.\n",
    "   * If prediction $a$ is close to 1 (e.g., 0.99), $\\log(0.99) \\approx 0$. **Cost is low.**\n",
    "   * If prediction $a$ is close to 0 (e.g., 0.01), $\\log(0.01)$ is a large negative number. **Cost is high.**\n",
    "   \n",
    "   \n",
    "   * **If correct answer is 0 ($y=0$):** The first term becomes 0. We care about $\\log(1-a)$.\n",
    "   * It works symmetrically.\n",
    "\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **MSE (Mean Squared Error):** Common for regression (predicting house prices), but bad for classification because it learns slowly when the error is large (due to the shape of the gradient).\n",
    "* **Cross-Entropy:** Heavily penalizes confident wrong predictions, speeding up learning.\n",
    "\n",
    "\n",
    "Certainly. Let's strip away the math symbols and look at the intuition.\n",
    "\n",
    "### Simpler Explanation: The \"Strict Teacher\" Analogy\n",
    "\n",
    "Imagine you are taking a test where you have to answer \"True\" (1) or \"False\" (0). However, instead of just answering Yes/No, you have to write down your **confidence percentage**.\n",
    "\n",
    "**Scenario 1: The correct answer is TRUE (1).**\n",
    "\n",
    "**Student A** says: \"99% sure it's True.\"\n",
    "Teacher: \"Great job. You are right and confident.\" -> **Loss: 0.01 (Tiny penalty)**\n",
    "\n",
    "**Student B** says: \"50% sure it's True.\"\n",
    "Teacher: \"Okay, you're wishy-washy.\" -> **Loss: 0.69 (Medium penalty)**\n",
    "\n",
    "**Student C** says: \"0.1% sure it's True\" (They are 99.9% sure it's False).\n",
    "Teacher: \"You are confidently wrong! That is the worst mistake.\" -> **Loss: Huge Number (Severe penalty)**\n",
    "\n",
    "\n",
    "\n",
    "**Why do we use Logs?**\n",
    "The logarithm function is just a mathematical tool to make the penalty \"explode\" the closer you get to being 100% wrong.\n",
    "\n",
    "   * If you are slightly wrong, the Log Loss is small.\n",
    "   * If you are completely, arrogantly wrong (predicting 100% \"No\" when it is \"Yes\"), the Log Loss goes to **infinity**.\n",
    "\n",
    "**Why not just use simple difference (Mean Squared Error)?**\n",
    "If we used simple subtraction (like in linear regression), the penalty for being \"confidently wrong\" isn't harsh enough. The Cross-Entropy loss forces the neural network to fix its worst mistakes (the ones where it is confidently wrong) *very* quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `compute_cost(AL, Y)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "   1. Implement the formula: $J = -\\frac{1}{m} \\sum \\Big[ y \\log(a) + (1 - y) \\log(1 - a) \\Big]$ using `np.multiply`, `np.log`, and `np.sum`.\n",
    "   2. `AL` is the activation output of the final layer (shape: $1, m$).\n",
    "   3. `Y` is the true label vector (shape: $1, m$).\n",
    "   4. **Important:** The result `cost` must be a scalar (a single number). Use `np.squeeze()` at the end to turn `[[0.5]]` into `0.5`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3393e0-fdab-4d66-8cc4-f4de354cbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    J = -np.multiply((1/m), np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1- Y), np.log(1 - AL))))\n",
    "    return np.squeeze(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb7f43-1c4b-4c27-af64-777675cfae28",
   "metadata": {},
   "source": [
    "**Pro Tip (Optimization):** In production code, we often add a tiny number (epsilon, e.g., `1e-8`) inside the `np.log()` to prevent `log(0)` errors if the model predicts exactly 0 or 1. For now, your strict implementation is mathematically perfect.\n",
    "\n",
    "#### 1. The Problem: The \"Black Hole\" of Log(0)\n",
    "\n",
    "In math, the function $\\log(x)$ asks: *\"To what power must I raise  (2.718) to get $x$?\"*\n",
    "\n",
    "If you try to calculate $\\log(0)$, you are asking: *\"To what power do I raise 2.718 to get 0?\"*\n",
    "There is no answer. You can get very close to zero by using a huge negative power (like $e^{-1000}$), but you never actually touch zero. Therefore, mathematically:\n",
    "$$\\log(0) = -\\infty \\text{ (Negative Infinity)}$$\n",
    "\n",
    "**In Python:**\n",
    "If your neural network predicts a perfect `0` (which can happen due to rounding errors) for a label that is actually `1`:\n",
    "\n",
    "```python\n",
    "np.log(0)  # Output: -inf\n",
    "\n",
    "```\n",
    "\n",
    "If you try to do math with `-inf` (like adding it to other numbers), your entire calculation crashes or turns into `NaN` (Not a Number). The training stops instantly.\n",
    "\n",
    "#### 2. The Solution: The \"Safety Cushion\"\n",
    "\n",
    "To prevent the code from crashing, we add a tiny, insignificant number called **epsilon** ($\\epsilon$), usually $10^{-8}$ (0.00000001).\n",
    "\n",
    "Instead of calculating $\\log(A)$, we calculate $\\log(A + \\epsilon)$.\n",
    "\n",
    "   * If $A = 0.5$: $\\log(0.50000001) \\approx -0.69$. (No noticeable change).\n",
    "   * If $A = 0$: $\\log(0.00000001) \\approx -18.4$.\n",
    "      * Now, instead of \"Negative Infinity,\" we just get a \"Very Large Penalty.\" The computer is happy, and training continues.\n",
    "\n",
    "\n",
    "This concept was **Cross-Entropy Math**. Adding \"Numerical Stability Hacks\" (like epsilon) creates noise when you are trying to just memorize the formula.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddf2b3-8170-45dd-8a7c-79bb122ef80f",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 4: The Chain Rule (Backpropagation Theory)**\n",
    "\n",
    "We have the Loss ($J$). Now we need to minimize it. To do that, we need to know: **\"If I nudge this specific weight  slightly up, does the Loss go up or down?\"**\n",
    "\n",
    "This is the **Gradient** ($\\frac{\\partial J}{\\partial W}$). Since our network is just a chain of functions nested inside each other ($Loss(Sigmoid(Linear(X)))$), we use the **Chain Rule** from calculus to find this gradient.\n",
    "\n",
    "### Intuition: The \"Blame Game\"\n",
    "\n",
    "Imagine a relay race where the team loses. Who is to blame?\n",
    "\n",
    "1. **The Coach (Loss Function):** Yells at the last runner (Activation). \"You were too slow!\"\n",
    "2. **The Last Runner (Activation):** Blames their shoes (Linear Output Z). \"The fit was tight!\"\n",
    "3. **The Shoes (Linear Output):** Blame the manufacturer (Weights W).\n",
    "\n",
    "Mathematically, we calculate the blame (gradient) in reverse order.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "We want to find $\\frac{\\partial J}{\\partial W}$. We break it down into steps:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W} = \\underbrace{\\frac{\\partial J}{\\partial A}}_{\\text{Change in Loss vs Pred}} \\cdot \\underbrace{\\frac{\\partial A}{\\partial Z}}_{\\text{Change in Pred vs Linear}} \\cdot \\underbrace{\\frac{\\partial Z}{\\partial W}}_{\\text{Change in Linear vs Weights}}$$\n",
    "\n",
    "This chain allows us to pass the error signal backward through any number of layers.\n",
    "\n",
    "### The \"Magic\" Shortcut\n",
    "\n",
    "For Logistic Regression (and the output layer of our binary classifier), the calculus simplifies beautifully.\n",
    "\n",
    "If you combine the derivative of the **Cross-Entropy Loss** with the derivative of the **Sigmoid Activation**, the first step of backpropagation ($dZ$) simplifies to just:\n",
    "$$dZ = A - Y$$\n",
    "\n",
    " * If prediction $A$ is 0.8 and Truth $Y$ is 1.0, the \"error signal\" is .\n",
    " * This term ($A-Y$) is the \"upstream gradient\" that kicks off the entire backpropagation process.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task (Mental Check)\n",
    "\n",
    "I will not ask for Python code yet. I want to verify you understand the data flow.\n",
    "\n",
    "**Scenario:**\n",
    "You have a network: $X \\xrightarrow{W_1} Z_1 \\xrightarrow{ReLU} A_1 \\xrightarrow{W_2} Z_2 \\xrightarrow{Sigmoid} A_2 \\xrightarrow{Loss} J$.\n",
    "\n",
    "**Question:** To calculate the gradient for the first layer's weights ($W_1$), we need the error signal from the future.\n",
    "Complete this dependency chain:\n",
    "To get $\\frac{\\partial J}{\\partial W_1}$, we need $\\frac{\\partial J}{\\partial Z_1}$.\n",
    "To get $\\frac{\\partial J}{\\partial Z_1}$, we need the gradient from the layer ahead of it. Which variable represents the \"error signal\" coming back from layer 2? (Choose one: $W_2$, $Z_2$, or $dZ_2$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3dd57-2b3e-4f0b-b452-5f3c87c090c6",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:** $dZ_2$ is the error signal at the linear part of Layer 2, which allows us to calculate everything before it.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5: Backward Propagation (The Linear Step)**\n",
    "\n",
    "Now we translate the math into code. We will tackle this in two parts. First, the **Linear Backward** step.\n",
    "\n",
    "Assume we already know the error $dZ$ at the end of a linear layer. We need to calculate three things:\n",
    "\n",
    "1. **$dW$ (Gradient of Weights):** How much to change $W$.\n",
    "2. **$db$ (Gradient of Bias):** How much to change $b$.\n",
    "3. **$dA_{prev}$ (Gradient of Input):** The error to send back to the previous layer.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "Recall that $Z = W \\cdot A_{prev} + b$.\n",
    "Using calculus (matrix calculus), the gradients are:\n",
    "\n",
    "   $$dW = \\frac{1}{m} dZ \\cdot A_{prev}^T$$\n",
    "   $$db = \\frac{1}{m} \\sum_{axis=1} dZ$$\n",
    "   $$dA_{prev} = W^T \\cdot dZ$$\n",
    "\n",
    "**Key Details:**\n",
    "\n",
    "   * **Transposing ($T$):** We transpose matrices to make the dimensions align for multiplication.\n",
    "      * $dZ$ : $(n_{out}, m)$\n",
    "      * $A_{prev}$ : $(n_{in}, m)$ -> $A_{prev}^T$ : $(m, n_{in})$\n",
    "      * Result $dW$ : $(n_{out}, n_{in})$ (Matches shape of $W$).\n",
    "\n",
    "   * **Summing:** For $db$, we sum the errors across all $m$ examples to get one average adjustment per neuron.\n",
    "\n",
    "\n",
    "### Simpler Explanation: The \"Project Post-Mortem\"\n",
    "\n",
    "Imagine a project failed (High Loss), and now we are doing a \"post-mortem\" meeting to figure out what went wrong. We are working backwards from the failure.\n",
    "\n",
    "We are currently analyzing the **Linear Layer** ($Z = W \\cdot A + b$).\n",
    "The layer ahead of us (the Activation) has just handed us a report called **$dZ$**. This report says: *\"The output  you gave me was too high/low by this amount.\"*\n",
    "\n",
    "Now, the Linear Layer has to decide who is responsible for this error. It splits the blame into three parts:\n",
    "\n",
    "**1. Blame the Weights ($dW$)**\n",
    "   * **Question:** \"Did we set the importance (weight) of the input incorrectly?\"\n",
    "   * **Intuition:** If the input ($A_{prev}$) was very loud (a large number), even a tiny mistake in the Weight ($W$) would cause a huge error in the result.\n",
    "   * **Action:** We calculate $dW$ so we can adjust the knobs ($W$) later to fix this.\n",
    "\n",
    "**2. Blame the Bias (db)**\n",
    "   * **Question:** \"Was our baseline threshold (bias) too high or low?\"\n",
    "   * **Intuition:** This is the \"average\" error that remains after checking the weights. If we were consistently too high across all examples, the bias takes the blame.\n",
    "   * **Action:** We calculate $db$ so we can shift the baseline ($b$) later.\n",
    "\n",
    "**3. Pass the Blame Backward ($dA_{prev}$) -- CRITICAL STEP**\n",
    "   * **Question:** \"Maybe I (the Linear Layer) did my job correctly, but the *data given to me* ($dA_{prev}$) by the previous guy was garbage?\"\n",
    "   * **Intuition:** We need to calculate how much the *input* contributed to the error.\n",
    "   * **Action:** We calculate $dA_{prev}$ and hand it backward to the previous layer. They will use this as their \"incoming error\" to fix their own weights.\n",
    "\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `linear_backward(dZ, cache)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "* **Inputs:**\n",
    "   * `dZ`: The gradient of the cost with respect to the linear output ($Z$).\n",
    "   * `cache`: The tuple `(A_prev, W, b)` you stored during the forward pass.\n",
    "\n",
    "\n",
    "* **Operations:**\n",
    "   * Retrieve `A_prev`, `W`, `b` from `cache`.\n",
    "   * Get `m` from the shape of `A_prev` (number of columns).\n",
    "   * Calculate `dW`, `db`, `dA_prev` using the formulas above.\n",
    "   * **Important:** `db` requires `np.sum` with `axis=1` and `keepdims=True` (to maintain shape `(n, 1)`).\n",
    "\n",
    "\n",
    "* **Returns:** `dA_prev`, `dW`, `db`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdbbf2-57bc-43c8-8089-8ce4c4c35500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    At = np.transpose(A_prev)\n",
    "    dW = np.multiply((1/m), np.dot(dZ, At))\n",
    "    db = np.multiply((1/m),np.sum(dZ, axis = 1, keepdims = True))\n",
    "    dA_prev = np.dot(np.transpose(W), dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77df8e9-22f9-4435-b0f3-693914382421",
   "metadata": {},
   "source": [
    "\n",
    "You have successfully implemented the \"Linear\" part of the backward pass. Now we need to implement the \"Activation\" part to complete the chain.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5 (Part 2): Activation Backward**\n",
    "\n",
    "We just calculated how to move from $dZ$ to $dW$. But how do we get $dZ$ in the first place?\n",
    "We get it from $dA$ (the gradient coming from the layer ahead).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "The chain rule tells us:\n",
    "$$dZ = \\frac{\\partial L}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} = dA \\cdot g'(Z)$$\n",
    "\n",
    "This means calculating $dZ$ is just an **element-wise multiplication** between the incoming error $dA$ and the **slope (derivative)** of the activation function at that point $Z$.\n",
    "\n",
    "#### 1. Derivative of ReLU\n",
    "\n",
    "If $Z > 0$, the slope is 1 (linear). If $Z \\le 0$, the slope is 0 (flat).\n",
    "$$g'(Z) = \\begin{cases} 1 & \\text{if } Z > 0 \\\\ 0 & \\text{if } Z \\le 0 \\end{cases}$$\n",
    "\n",
    "   * **Logic:** Copy $dA$ exactly where $Z$ was positive. Zero out $dA$ where $Z$ was negative.\n",
    "\n",
    "#### 2. Derivative of Sigmoid\n",
    "\n",
    "The derivative of the sigmoid function $s(Z)$ has a beautiful property:\n",
    "$$g'(Z) = s(Z) \\cdot (1 - s(Z))$$\n",
    "\n",
    "   * **Logic:** We first calculate the sigmoid $s$ of the cached $Z$, then use it to scale $dA$.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement two standalone functions:\n",
    "\n",
    "1. `relu_backward(dA, cache)`\n",
    "   * **Input:** `dA` (gradient of activation), `cache` (this is just `Z` stored from forward pass).\n",
    "   * **Logic:** Create `dZ` which is a copy of `dA`. Set values in `dZ` to `0` wherever the corresponding `Z` (from cache) is $\\le 0$.\n",
    "   * **Return:** `dZ`.\n",
    "\n",
    "\n",
    "2. `sigmoid_backward(dA, cache)`\n",
    "   * **Input:** `dA`, `cache` (stored `Z`).\n",
    "   * **Logic:**\n",
    "      * Calculate $s = 1 / (1 + e^{-Z})$.\n",
    "      * Calculate $dZ = dA * s * (1 - s)$. (Element-wise multiplication).\n",
    "\n",
    "   * **Return:** `dZ`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6aecab-6b31-40ab-985e-c5bcae276053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0 \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1 + np.exp(-Z))\n",
    "    dZ = np.multiply(dA, (s*(1-s)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91374a19-462d-480d-9f77-c87469eaba24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "We have successfully built the engine (Linear), the transmission (Activation), and the diagnostic tools (Loss & Gradients). Now we need the driver.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 6: Optimization (Gradient Descent)**\n",
    "\n",
    "Calculating the gradients ($dW$, $db$) tells us the direction of the steepest slope uphill. Since we want to minimize the loss (go downhill), we simply take a step in the **opposite direction**.\n",
    "\n",
    "This process is called **Gradient Descent**.\n",
    "\n",
    "### Intuition: The Mountain in the Fog\n",
    "\n",
    "Imagine you are lost on a mountain at night (foggy). You want to get to the village at the bottom (Minimum Loss).\n",
    "\n",
    "   1. You feel the slope of the ground under your feet (Calculate Gradients).\n",
    "   2. You take a small step downhill (Update Parameters).\n",
    "   3. You repeat this until the ground flattens out (Convergence).\n",
    "\n",
    "**The Learning Rate ($\\alpha$):**\n",
    "This controls the size of your step.\n",
    "\n",
    "   * **Too small:** You will take forever to reach the bottom.\n",
    "   * **Too big:** You might overshoot the village and end up on the other side of the valley (divergence).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "For every parameter $\\theta$ (which represents all our $W$ and $b$ matrices):\n",
    "$$\\theta = \\theta - \\alpha \\cdot d\\theta$$\n",
    "\n",
    "In our specific case with a dictionary of parameters:\n",
    "\n",
    "   * $W = W - \\alpha \\cdot dW$\n",
    "   * $b = b - \\alpha \\cdot db$\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement `update_parameters(parameters, grads, learning_rate)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "   * **Inputs:**\n",
    "      * `parameters`: A dictionary containing `{\"W1\": ..., \"b1\": ..., \"W2\": ..., \"b2\": ...}`.\n",
    "      * `grads`: A dictionary containing `{\"dW1\": ..., \"db1\": ..., \"dW2\": ..., \"db2\": ...}`. (Output from backprop).\n",
    "      * `learning_rate`: A scalar (e.g., 0.01).\n",
    "   \n",
    "   \n",
    "   * **Logic:**\n",
    "      * Update every parameter in the dictionary using the formula above.\n",
    "      * **Note:** You must update them \"in place\" or return a new dictionary with the updated values. Returning a new dictionary is safer and cleaner.\n",
    "   * **Return:** `parameters` (the updated dictionary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da549cbd-d2c9-46b7-8c58-12c051db5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    new_params = {}\n",
    "    new_params.update(parameters)\n",
    "\n",
    "    for key in new_params.keys():\n",
    "        grad_key = \"d\" + key\n",
    "        val = new_params[key]\n",
    "        new_params[key] = val - np.multiply(learning_rate, grads[grad_key])\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87447956-bbe4-4e3c-838c-d77370f3d8ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "## **Concept 7: Data Preparation (The \"Moons\" Dataset)**\n",
    "\n",
    "We have a functioning neural network engine. Now we need fuel.\n",
    "We will use the **\"Make Moons\"** dataset from Scikit-Learn. It generates two interleaving half-circles.\n",
    "\n",
    "**Why this dataset?**\n",
    "A linear classifier (like Logistic Regression) cannot solve this. You cannot draw a single straight line to separate the two moons. This proves our Neural Network's ability to learn **non-linear boundaries**.\n",
    "\n",
    "\n",
    "We are building a Neural Network, which is complex and computationally expensive. If we just wanted to classify data that can be separated by a straight line (like separating \"Tall people\" from \"Short people\"), we could just use simple Logistic Regression.\n",
    "\n",
    "We need a problem that **fails** on simple models to prove our Neural Network is actually working.\n",
    "\n",
    "   * **The Shape:** The \"Moons\" dataset consists of two crescent shapes that interlock like two bananas.\n",
    "   * **The Challenge:** You cannot draw a single straight line to separate these two shapes. You need a curvy, wiggle line.\n",
    "   * **The Proof:** If our network successfully classifies this, it proves the **Hidden Layer (ReLU)** is doing its job of bending the decision boundary. If the network fails (gets 50% accuracy), we know our non-linearity isn't working.\n",
    "\n",
    "### Mechanics & Dimensions (Critical)\n",
    "\n",
    "Scikit-Learn returns data in the standard format: **Rows = Examples**.\n",
    "\n",
    "* `X`: Shape `(m, 2)` (where m is 1000 examples, 2 features).\n",
    "* `y`: Shape `(m,)` (Rank-1 array).\n",
    "\n",
    "**Our Network's Requirement:**\n",
    "Our math assumes **Columns = Examples**.\n",
    "\n",
    "* We need `X` to be `(2, m)`.\n",
    "* We need `y` to be `(1, m)` (a proper row vector, not a Rank-1 array).\n",
    "\n",
    "### Why Transpose and Reshape? (The \"Dimension Mismatch\" Trap)\n",
    "\n",
    "This is the most common source of bugs in Deep Learning.\n",
    "\n",
    "**The World Standard (Scikit-Learn/Pandas):**\n",
    "Most data libraries store data like a spreadsheet:\n",
    "   * **Rows:** Examples (Users, Images, etc.)\n",
    "   * **Columns:** Features (Age, Height, Pixel values)\n",
    "   * Shape: `(1000 examples, 2 features)`\n",
    "\n",
    "**Our Math Standard (Linear Algebra/Vectorization):** \n",
    "Our formula is $Z = W \\cdot X + b$..\n",
    "\n",
    "   * $W$ (Weights) has shape `(Neurons, Features)`.\n",
    "   * To multiply $W$ by $X$, the rows of $X$ must match the columns of $W$.\n",
    "   * Therefore, we need $X$ to be `(Features, Examples)`.\n",
    "\n",
    "If we feed the default Scikit-Learn data into our code:\n",
    "\n",
    "$$W_{(4, 2)} \\cdot X_{(1000, 2)}$$\n",
    "\n",
    "**CRASH.** Inner dimensions (2 vs 1000) don't match.\n",
    "We must Transpose $X$ to be `(2, 1000)` so that:\n",
    "$$W_{(4, 2)} \\cdot X_{(2, 1000)} = Z_{(4, 1000)}$$\n",
    "\n",
    "**The Rank-1 Array Issue (`y`):**\n",
    "Scikit-Learn returns labels `y` as a flat array: `[0, 1, 1, 0...]` with shape `(1000,)`.\n",
    "\n",
    "   * NumPy treats this as \"shapeless\" in certain operations.\n",
    "   * If you subtract this from a matrix `(1, 1000)`, NumPy might create a giant `(1000, 1000)` matrix instead of what you want.\n",
    "   * **The Fix:** We force it to be a strict row vector: `(1, 1000)`.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement a function `load_data()`:\n",
    "\n",
    "1. Import `sklearn.datasets`.\n",
    "2. Use `sklearn.datasets.make_moons` to generate `N=1000` samples with `noise=0.2`.\n",
    "3. **Transpose X:** Convert shape from `(1000, 2)` to `(2, 1000)`.\n",
    "4. **Reshape Y:** Convert shape from `(1000,)` to `(1, 1000)`.\n",
    "5. **Return:** `X`, `Y`.\n",
    "\n",
    "*(Note: No visualization code required yet, just data processing)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca67c00-8376-4e97-a489-98cb78e28308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    X, Y = make_moons(n_samples = 1000, noise = 0.2)\n",
    "    Xt = np.transpose(X)\n",
    "    Yt = Y.reshape(1, -1)\n",
    "    return Xt, Yt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcb9fb-2e76-4f77-b1df-fb4ebe019bd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "We have our data loaded and reshaped:\n",
    "\n",
    "   * `X`: Shape `(2, 1000)` $\\rightarrow$ Row 0 is \"x-coordinate\", Row 1 is \"y-coordinate\".\n",
    "   * `Y`: Shape `(1, 1000)` $\\rightarrow$ Row 0 is the color (0 or 1).\n",
    "\n",
    "We need to see the \"Moons\" to confirm that a straight line indeed cannot separate them.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `visualize_data(X, Y)`.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Import `matplotlib.pyplot` as `plt`.\n",
    "   2. Create a scatter plot:\n",
    "      * **x-axis:** The first row of `X` (`X[0, :]`).\n",
    "      * **y-axis:** The second row of `X` (`X[1, :]`).\n",
    "      * **Colors:** Use `c=Y.ravel()` (flattens `Y` to a 1D array for the color argument) and `cmap=plt.cm.Spectral`.\n",
    "   \n",
    "   3. Add `plt.show()` at the end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd21d1c-f191-436c-9b62-640b1d6f7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(X, Y):\n",
    "    plt.scatter(X[0, :], X[1, :], c=Y.ravel(), cmap = plt.cm.Spectral)\n",
    "    plt.show()\n",
    "\n",
    "X, Y = load_data()\n",
    "visualize_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63ccd6-50b7-4e02-82f8-4b03e7ef0fc0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "You can clearly see why a simple straight line (Linear Regression) would fail hereâ€”it needs to curve around the blue moon to separate it from the red one.\n",
    "\n",
    "Now that we have verified our tools (functions) and our materials (data), it is time to build the machine.\n",
    "\n",
    "## Mini-Project: The Training Loop (`nn_model`)\n",
    "\n",
    "**Objective:** Combine all the isolated components you have built into a single function that learns the parameters `W` and `b`.\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "   * **Input Layer:** 2 features (X-coordinate, Y-coordinate)\n",
    "   * **Hidden Layer:** `n_h` neurons with **ReLU** activation.\n",
    "   * **Output Layer:** 1 neuron with **Sigmoid** activation (prediction 0 or 1).\n",
    "\n",
    "### The Data Flow\n",
    "\n",
    "1. **Forward:** $X \\to Linear \\to ReLU \\to Linear \\to Sigmoid \\to AL$ (Prediction)\n",
    "2. **Loss:** Compare $AL$ vs $Y$.\n",
    "3. **Backward:** Calculate gradients ($dZ2 \\to dW2 \\to db2 \\to dA1 \\to dZ1 \\to dW1 \\to db1$).\n",
    "4. **Update:** Adjust weights using gradients.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `nn_model(X, Y, n_h, num_iterations=10000, print_cost=False)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "1. **Initialize:** Call `initialize_parameters`.\n",
    "   * `n_in` comes from `X.shape[0]`.\n",
    "   * `n_out` for the final layer is `1`.\n",
    "\n",
    "\n",
    "2. **Loop** for `num_iterations`:\n",
    "   * **Forward Propagation:**\n",
    "      * Layer 1: Calculate `Z1` using `linear_forward`, then `A1` using `relu` (custom function: `np.maximum(0, Z1)`).\n",
    "      * Layer 2: Calculate `Z2` using `linear_forward`, then `A2` using `sigmoid`.\n",
    "\n",
    "\n",
    "   * **Cost:** Call `compute_cost`.\n",
    "   * **Backward Propagation:**\n",
    "      * **Layer 2 (Output):**\n",
    "         * *Pro Tip:* For Sigmoid + Cross-Entropy, the derivative simplifies to `dZ2 = A2 - Y`. Use this directly; it's numerically more stable than calculating `dA2` then using `sigmoid_backward`.\n",
    "         * Calculate `dA1`, `dW2`, `db2` using `linear_backward` with `dZ2`.\n",
    "\n",
    "      * **Layer 1 (Hidden):**\n",
    "         * Calculate `dZ1` using `relu_backward(dA1, Z1)`.\n",
    "         * Calculate `dW1`, `db1`, `dA0` using `linear_backward`.\n",
    "\n",
    "   * **Update:** Call `update_parameters`.\n",
    "\n",
    "3. **Logging:** If `print_cost` is True, print the cost every 1000 iterations.\n",
    "4. **Return:** The final `parameters` dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0840d0-ed49-439e-b0bc-20e7c47edc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(n_in, n_h, n_out):\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_in) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_out, n_h) * 0.01\n",
    "    b2 = np.zeros((n_out, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    return Z\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1 / m) * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
    "    return cost\n",
    "\n",
    "def linear_backward(dZ, A_prev, W):\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    new_params = {}\n",
    "    new_params.update(parameters)\n",
    "\n",
    "    for key in new_params:\n",
    "        grad_key = \"d\" + key\n",
    "        new_params[key] -= learning_rate * grads[grad_key]\n",
    "\n",
    "    return new_params\n",
    "\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost = False):\n",
    "    n_in = X.shape[0]\n",
    "    n_out = 1\n",
    "\n",
    "    parameters = initialize_parameters(n_in, n_h, n_out)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        Z1 = linear_forward(X, parameters[\"W1\"], parameters[\"b1\"])\n",
    "        A1 = np.maximum(0, Z1)\n",
    "\n",
    "        Z2 = linear_forward(A1, parameters[\"W2\"], parameters[\"b2\"])\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        cost = compute_cost(A2, Y)\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dA1, dW2, db2 = linear_backward(dZ2, A1, parameters[\"W2\"])\n",
    "\n",
    "        dZ1 = relu_backward(dA1, Z1)\n",
    "        dA0, dW1, db1 = linear_backward(dZ1, X, parameters[\"W1\"])\n",
    "\n",
    "        grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, 0.01)\n",
    "\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd12d476-cf84-43ca-9f77-9f816ab2f96f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "### Phase 3: Verification (The Decision Boundary)\n",
    "\n",
    "We have a trained model, but numbers like `Cost: 0.2` are abstract. We need visual proof. To draw the \"curvy line\" that separates the moons, we need a function that takes grid points and outputs a hard Class (0 or 1).\n",
    "\n",
    "## **Concept 8: Prediction**\n",
    "\n",
    "The `forward_propagation` gives us a probability (e.g., 0.75).\n",
    "The `predict` function converts that to a decision (e.g., Class 1).\n",
    "\n",
    "**Logic:**\n",
    "$$\\text{Prediction} = \\begin{cases} 1 & \\text{if } A2 > 0.5 \\\\ 0 & \\text{if } A2 \\le 0.5 \\end{cases}$$\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement `predict(parameters, X)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "1. **Forward Pass:** Calculate `A2` using the weights in `parameters` and input `X`. (You can copy the forward logic from your model or helper functions).\n",
    "2. **Threshold:** Convert `A2` probabilities to 0 or 1.\n",
    "3. **Return:** The predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdf4f-331b-4138-9b33-435b2b0d2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict(parameters, X):\n",
    "    Z1 = linear_forward(X, parameters[\"W1\"], parameters[\"b1\"])\n",
    "    A1 = np.maximum(0, Z1)\n",
    "    Z2 = linear_forward(A1, parameters[\"W2\"], parameters[\"b2\"])\n",
    "    A2 = sigmoid(Z2)\n",
    "    predictions = (A2 > 0.5).astype(int)\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81380652-a8cf-48f2-994f-51a90404cfbb",
   "metadata": {},
   "source": [
    "It is time to ignite the engine and see if it actually learns.\n",
    "\n",
    "### Final Task: Train and Evaluate\n",
    "\n",
    "We will now run your `nn_model` on the \"Moons\" dataset.\n",
    "\n",
    "**Your Task:**\n",
    "Write a script (or cell) to perform the following:\n",
    "\n",
    "1. **Load Data:** Call your `load_data()` function to get `X` and `Y`.\n",
    "2. **Train:** Call `nn_model` with:\n",
    "   * `X, Y`\n",
    "   * `n_h = 4` (4 hidden neurons)\n",
    "   * `num_iterations = 10000`\n",
    "   * `print_cost = True`\n",
    "\n",
    "\n",
    "3. **Predict:** Use your `predict` function on the training set `X`.\n",
    "4. **Evaluate:** Calculate and print the **Accuracy** percentage.\n",
    "   * *Formula:* `np.mean(predictions == Y) * 100`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f9dc1-46d3-4c57-b819-4ec26de46efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_data()\n",
    "\n",
    "parameters = nn_model(X, Y, 4, 10000, True)\n",
    "\n",
    "predictions = predict(parameters, X)\n",
    "\n",
    "accuracy = np.mean(predictions == Y) * 100\n",
    "print(f\"Training Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9c797-7d3b-4375-a50d-feccbab0f76b",
   "metadata": {},
   "source": [
    "### **Project Review**\n",
    "\n",
    "**Status: PASS (With Distinction)** ðŸŒŸ\n",
    "\n",
    "You have successfully built a fully functioning Neural Network from scratch in NumPy. This is a significant milestone. You demonstrated:\n",
    "\n",
    "1. **Matrix Calculus:** Correctly deriving and implementing backpropagation.\n",
    "2. **Data Handling:** Correctly reshaping rank-1 arrays (the most common NumPy bug).\n",
    "3. **Architecture:** successfully wiring the forward and backward passes.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638ab4d0-2333-4c1b-9a82-e282cc8ac5ed",
   "metadata": {},
   "source": [
    "## Mini-Project 2: Heart Disease Predictor\n",
    "\n",
    "**Objective:** Build a robust, object-oriented Neural Network framework to predict the presence of heart disease.\n",
    "\n",
    "### 1. The Dataset\n",
    "\n",
    "We will use the **Cleveland Heart Disease Dataset** from the UCI Machine Learning Repository.\n",
    "   * **Source:** You will load this directly using `pandas` from a URL (e.g., raw GitHub content or UCI).\n",
    "   * **Problem Type:** Binary Classification (Target: `target` column where 1=Disease, 0=No Disease).\n",
    "   * **Features:** 13 features including Age, Sex, Cholesterol, Resting BP, etc.\n",
    "\n",
    "**Critical Challenge:** The features have vastly different ranges (e.g., Age $\\approx$ 60 vs. Cholesterol $\\approx$ 250). If you fail to **Standardize** (Normalize) the data, your gradient descent will oscillate and fail.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Architecture Requirements\n",
    "\n",
    "You must implement the following three classes with strict encapsulation.\n",
    "\n",
    "#### **Class 1: `DataHandler`**\n",
    "\n",
    "**Responsibility:** ETL (Extract, Transform, Load).\n",
    "\n",
    "   * **`__init__(self, file_path)`**: Initialize with the data source.\n",
    "   * **`load_data(self)`**:\n",
    "      * Load CSV using Pandas.\n",
    "      * Handle missing values (if any) or drop them.\n",
    "      * Separate Features ($X$) and Target ($y$)\n",
    "   \n",
    "   \n",
    "   * **`normalize(self)`**:\n",
    "      * Implement **Standard Scaling**: $X_{new} = \\frac{X - \\mu}{\\sigma}$ (Subtract Mean, Divide by Std Dev).\n",
    "      * **Crucial:** You must store $\\mu$ and $\\sigma$ so you can normalize future/test data using the training stats.\n",
    "   \n",
    "   \n",
    "   * **`split_data(self, test_size=0.2)`**:\n",
    "      * Split into Train and Test sets.\n",
    "      * Ensure shapes are transposed correctly for your Network (Features as Rows, Examples as Columns).\n",
    "\n",
    "\n",
    "\n",
    "#### **Class 2: `NeuralNetwork`**\n",
    "\n",
    "**Responsibility:** The Brain (Math & State).\n",
    "\n",
    "  * **`__init__(self, layer_dims)`**:\n",
    "     * Accept a list of dimensions, e.g., `[13, 16, 16, 1]`.\n",
    "     * Initialize parameters ($W, b$) for an arbitrary number of layers (Make it dynamic, not just hardcoded for 2 layers).\n",
    "  \n",
    "  \n",
    "  * **`forward(self, X)`**:\n",
    "     * Iterate through layers.\n",
    "     * Apply ReLU for hidden layers, Sigmoid for output.\n",
    "     * Cache values for backprop.\n",
    "  \n",
    "  \n",
    "  * **`backward(self, AL, Y)`**:\n",
    "     * Calculate gradients for all layers.\n",
    "  \n",
    "  \n",
    "  * **`update_params(self, learning_rate)`**:\n",
    "     * Apply Gradient Descent.\n",
    "\n",
    "#### **Class 3: `Trainer`**\n",
    "\n",
    "**Responsibility:** The Coach (Loop & Logging).\n",
    "\n",
    "   * **`__init__(self, model, data_handler)`**: Composition. Takes instances of the other two classes.\n",
    "      * **`train(self, epochs, learning_rate, print_cost)`**:\n",
    "      * Run the optimization loop.\n",
    "      * Store costs for plotting.\n",
    "   \n",
    "   \n",
    "   * **`evaluate(self, X, Y)`**:\n",
    "      * Run predictions.\n",
    "      * Return **Accuracy** and a **Confusion Matrix** (True Pos, False Pos, False Neg, True Neg).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Implementation Steps\n",
    "   1. **Setup:** Import `pandas`, `numpy`, and `matplotlib`.\n",
    "   2. **Data Loading:** Get data from Data\n",
    "   *(Note: This is a standard mirror of the Cleveland dataset. Target 1 = Disease).*\n",
    "   3. **Build:** Implement the three classes.\n",
    "   4. **Execution:**\n",
    "      * Instantiate `DataHandler`.\n",
    "      * Normalize and Split.\n",
    "      * Instantiate `NeuralNetwork` with architecture `[13, 16, 8, 1]` (Input=13 features).\n",
    "      * Instantiate `Trainer`.\n",
    "      * Train for 2000-5000 epochs.\n",
    "      * Report Train Accuracy and Test Accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19eebf16-b08a-4d72-907b-b60375873ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded. X Shape: (299, 13), Y Shape: (1, 299)\n",
      "Starting Training...\n",
      "Epoch 0, Cost: 1.8916\n",
      "Epoch 1000, Cost: 0.2973\n",
      "Epoch 2000, Cost: 0.2414\n",
      "\n",
      "Final Training Accuracy: 92.89%\n",
      "Final Test Accuracy:     90.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Handler\n",
    "# ==========================================\n",
    "class DataHandler:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.mu = None\n",
    "        self.sigma = None\n",
    "        self.random_seed = 42\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "\n",
    "    def load_and_clean_data(self):\n",
    "        # 1. Load Data (Use read_csv for CSV files)\n",
    "        try:\n",
    "            df = pd.read_csv(self.file_path)\n",
    "        except:\n",
    "            # Fallback if user actually provides an excel file\n",
    "            df = pd.read_excel(self.file_path)\n",
    "            \n",
    "        df = df.dropna()\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # 2. Handle Target (Binary)\n",
    "        # 'num' > 0 means disease (1), else 0\n",
    "        df[\"num\"] = df[\"num\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "        \n",
    "        # 3. Handle Categorical Data\n",
    "        cat_cols = ['sex', 'dataset', 'cp', 'restecg', 'thal', 'slope']\n",
    "        # Only process columns that actually exist in the dataframe\n",
    "        existing_cats = [c for c in cat_cols if c in df.columns]\n",
    "        \n",
    "        for col in existing_cats:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "    \n",
    "        # 4. Create X and Y\n",
    "        # Drop columns safely\n",
    "        cols_to_drop = [\"id\", \"dataset\", \"num\"]\n",
    "        existing_drops = [c for c in cols_to_drop if c in df.columns]\n",
    "        \n",
    "        self.X = df.drop(columns=existing_drops).values\n",
    "        self.Y = df[\"num\"].values.reshape(1, -1)\n",
    "        \n",
    "        print(f\"Data Loaded. X Shape: {self.X.shape}, Y Shape: {self.Y.shape}\")\n",
    "\n",
    "    def normalize(self, X):\n",
    "        # Standardize using Mean and Std Dev\n",
    "        if self.mu is None:\n",
    "            self.mu = np.mean(X, axis=1, keepdims=True)\n",
    "            self.sigma = np.std(X, axis=1, keepdims=True)\n",
    "            self.sigma[self.sigma == 0] = 1e-8\n",
    "            \n",
    "        return (X - self.mu) / self.sigma\n",
    "\n",
    "    def split_data(self, test_size=0.2):\n",
    "        # We split the data (Rows=Examples) FIRST\n",
    "        # Then we Transpose (Cols=Examples) for the Neural Net\n",
    "        X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(\n",
    "            self.X, self.Y.T, test_size=test_size, random_state=self.random_seed\n",
    "        )\n",
    "        \n",
    "        # Transpose to (Features, Examples)\n",
    "        X_train = X_train_orig.T\n",
    "        X_test = X_test_orig.T\n",
    "        y_train = y_train_orig.T\n",
    "        y_test = y_test_orig.T\n",
    "        \n",
    "        # Normalize Data (Compute stats on Train, apply to Test)\n",
    "        X_train = self.normalize(X_train)\n",
    "        X_test = self.normalize(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "# ==========================================\n",
    "# 2. Neural Network\n",
    "# ==========================================\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layer_dims):\n",
    "        self.layer_dims = layer_dims\n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dims) - 1\n",
    "        self.caches = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "        np.random.seed(1)\n",
    "        for l in range(1, len(layer_dims)):\n",
    "            # He Initialization (Better for ReLU)\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.caches = {}\n",
    "        A = X\n",
    "        self.caches['A0'] = X \n",
    "        \n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            W = self.parameters['W' + str(l)]\n",
    "            b = self.parameters['b' + str(l)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            \n",
    "            self.caches['Z' + str(l)] = Z\n",
    "            \n",
    "            if l < self.num_layers:\n",
    "                A = np.maximum(0, Z) # ReLU\n",
    "            else:\n",
    "                A = 1 / (1 + np.exp(-Z)) # Sigmoid\n",
    "            \n",
    "            self.caches['A' + str(l)] = A\n",
    "            \n",
    "        return A\n",
    "\n",
    "    def backward(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        Y = Y.reshape(AL.shape)\n",
    "        \n",
    "        dZ = AL - Y # Derivative of Sigmoid + Cross Entropy\n",
    "        \n",
    "        for l in reversed(range(1, self.num_layers + 1)):\n",
    "            A_prev = self.caches['A' + str(l-1)]\n",
    "            W = self.parameters['W' + str(l)]\n",
    "            \n",
    "            # Gradients\n",
    "            self.gradients['dW' + str(l)] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "            self.gradients['db' + str(l)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "            \n",
    "            # Backprop to next layer\n",
    "            if l > 1:\n",
    "                dA_prev = np.dot(W.T, dZ)\n",
    "                Z_prev = self.caches['Z' + str(l-1)]\n",
    "                dZ = np.array(dA_prev, copy=True)\n",
    "                dZ[Z_prev <= 0] = 0 # ReLU Derivative\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            self.parameters['W' + str(l)] -= learning_rate * self.gradients['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= learning_rate * self.gradients['db' + str(l)]\n",
    "\n",
    "# ==========================================\n",
    "# 3. Trainer\n",
    "# ==========================================\n",
    "class Trainer:\n",
    "    def __init__(self, model, data_handler):\n",
    "        self.model = model\n",
    "        self.data_handler = data_handler\n",
    "        \n",
    "        # Load and Prepare Data\n",
    "        self.data_handler.load_and_clean_data()\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = self.data_handler.split_data()\n",
    "\n",
    "    def cost_function(self, AL, Y):\n",
    "        m = Y.shape[1]\n",
    "        # Add epsilon to prevent log(0)\n",
    "        cost = - (1 / m) * np.sum(Y * np.log(AL + 1e-8) + (1 - Y) * np.log(1 - AL + 1e-8))\n",
    "        return np.squeeze(cost)\n",
    "    \n",
    "    def train(self, epochs, learning_rate, print_cost=True):\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            # Forward\n",
    "            AL = self.model.forward(self.X_train)\n",
    "            \n",
    "            # Cost\n",
    "            cost = self.cost_function(AL, self.Y_train)\n",
    "            \n",
    "            # Backward\n",
    "            self.model.backward(AL, self.Y_train)\n",
    "            \n",
    "            # Update\n",
    "            self.model.update_params(learning_rate)\n",
    "            \n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(f\"Epoch {i}, Cost: {cost:.4f}\")\n",
    "            costs.append(cost)\n",
    "        return costs\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        AL = self.model.forward(X)\n",
    "        predictions = (AL > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == Y) * 100\n",
    "        return accuracy\n",
    "\n",
    "# ==========================================\n",
    "# 4. Execution\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    dh = DataHandler(r\"Data/HDD.xlsx\")\n",
    "    \n",
    "    # Initialize Model with Dynamic Architecture\n",
    "    # 13 features -> 16 hidden -> 8 hidden -> 1 output\n",
    "    model = NeuralNetwork([13, 16, 8, 1])\n",
    "    \n",
    "    # Trainer handles the rest\n",
    "    trainer = Trainer(model, dh)\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    trainer.train(epochs=3000, learning_rate=0.01)\n",
    "    \n",
    "    # Results\n",
    "    train_acc = trainer.evaluate(trainer.X_train, trainer.Y_train)\n",
    "    test_acc = trainer.evaluate(trainer.X_test, trainer.Y_test)\n",
    "    \n",
    "    print(f\"\\nFinal Training Accuracy: {train_acc:.2f}%\")\n",
    "    print(f\"Final Test Accuracy:     {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9825630a-94b0-4110-9772-3ddf9c511e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138ec2db-cc65-430d-9234-e13ed63184bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e58ae8-a1c7-467e-8208-64e9cf5275eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63d657-32bf-433c-be60-4ed060b84f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379beeb3-bf7f-4309-9272-ac11d7691226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
