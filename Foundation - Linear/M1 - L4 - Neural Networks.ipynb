{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "087e1ade-e346-484e-ab93-8b43672ee291",
   "metadata": {},
   "source": [
    "# **Neural Networks**\n",
    "\n",
    "At its core, a Neural Network is a mathematical function that maps an input to an output. However, unlike a simple linear regression equation (y = mx + c), a neural network is composed of layers of interconnected \"neurons\" that can approximate incredibly complex, non-linear functions.\n",
    "\n",
    "### The Biological Inspiration\n",
    "\n",
    "The concept is loosely inspired by the human brain.\n",
    "\n",
    "   * **Biological Neuron:** Receives electrical signals through dendrites. If the signal is strong enough, it \"fires\" (activates) and sends the signal down the axon to other neurons.\n",
    "   * **Artificial Neuron:** Receives numerical inputs. It multiplies them by specific \"weights\" (importance), adds a \"bias\" (threshold), and passes the result through an \"activation function\" (decision rule).\n",
    "\n",
    "### The Architecture\n",
    "\n",
    "A standard Neural Network consists of three types of layers:\n",
    "\n",
    "   1. **Input Layer:** The raw data enters here (e.g., pixels of an image, words in a sentence). No computation happens here; it's just a doorway.\n",
    "   2. **Hidden Layers:** The \"magic\" happens here. These layers transform the input data into abstract features. Deep Learning simply refers to networks with many hidden layers.\n",
    "   3. **Output Layer:** The final decision (e.g., \"Cat\" vs. \"Dog\", or a stock price prediction).\n",
    "\n",
    "### How It \"Learns\"\n",
    "\n",
    "A neural network is initially dumb. It starts with random weights.\n",
    "\n",
    "   1. **Forward Pass (Guessing):** Data flows through the network. It makes a guess.\n",
    "   2. **Loss Calculation (Grading):** We compare the guess to the actual answer. The difference is the \"Loss\" or error.\n",
    "   3. **Backward Pass (Learning):** We use calculus (Chain Rule) to calculate which weights contributed most to the error.\n",
    "   4. **Optimization (Correction):** We adjust the weights slightly in the opposite direction of the error to reduce it next time.\n",
    "\n",
    "**Analogy:** Imagine trying to tune a radio to a specific clear station (the optimal function). You turn the knobs (weights) randomly at first. You hear static (high loss). You turn one knob slightly; the static gets worse. You turn it the other way; the static gets better. You keep doing this for all knobs until the signal is clear.\n",
    "\n",
    "---\n",
    "\n",
    "## **Topic Tree**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cf9986f-5120-4dd1-91d7-7768c8b3b4bc",
   "metadata": {},
   "source": [
    "L4: Neural Networks from Scratch\n",
    "├── Concept 1: The Linear Layer (Forward Propagation)\n",
    "│   ├── Y = WX + b\n",
    "│   ├── Matrix dimensions and broadcasting\n",
    "│   ├── Explanation: The \"skeleton\" of the network that transforms input space.\n",
    "│   └── Task: Implement a Linear class with weight initialization and forward pass.\n",
    "│\n",
    "├── Concept 2: Activation Functions (ReLU & Sigmoid)\n",
    "│   ├── Introducing non-linearity\n",
    "│   ├── Explanation: The \"brain\" that allows learning complex patterns (not just lines).\n",
    "│   └── Task: Implement forward methods for ReLU and Sigmoid.\n",
    "│\n",
    "├── Concept 3: Loss Function (Binary Cross-Entropy)\n",
    "│   ├── Measuring error for binary classification\n",
    "│   ├── MSE vs. Log Loss\n",
    "│   ├── Explanation: A score telling the network how \"wrong\" it is.\n",
    "│   └── Task: Implement a function to calculate the cost.\n",
    "│\n",
    "├── Concept 4: The Chain Rule (Backpropagation Theory)\n",
    "│   ├── Partial derivatives\n",
    "│   ├── Local gradients vs. Global gradients\n",
    "│   ├── Explanation: How blame is assigned to weights for the error.\n",
    "│   └── Task: Theoretical derivation (mental check).\n",
    "│\n",
    "├── Concept 5: Implementing Gradients (Backward Propagation)\n",
    "│   ├── Calculating dW, db, and dZ manually\n",
    "│   ├── Derivative of ReLU and Sigmoid\n",
    "│   ├── Explanation: converting the math from Concept 4 into NumPy code.\n",
    "│   └── Task: Implement the backward pass methods for layers and activations.\n",
    "│\n",
    "├── Concept 6: Optimization (Stochastic Gradient Descent)\n",
    "│   ├── W = W - learning_rate * gradients\n",
    "│   ├── Explanation: Taking a step down the error mountain.\n",
    "│   └── Task: Implement a function to update parameters.\n",
    "│\n",
    "├── Concept 7: Data Preparation (Moons Dataset)\n",
    "│   ├── sklearn.datasets.make_moons\n",
    "│   ├── Reshaping for matrix ops\n",
    "│   ├── Explanation: Creating the problem space.\n",
    "│   └── Task: Generate and visualize the dataset.\n",
    "│\n",
    "└── Mini-Project: Training the 2-Layer Neural Network\n",
    "    ├── Putting it all together\n",
    "    └── Training loop (Forward -> Loss -> Backward -> Optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97856f05-85ea-47d1-8b14-e4975483195b",
   "metadata": {},
   "source": [
    "## **Concept 1: The Linear Layer (Forward Propagation)**\n",
    "\n",
    "The fundamental building block of a neural network is the Linear Layer (also called the Dense or Fully Connected layer). It performs a linear transformation on the input.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "For a single layer, the mathematical operation is:\n",
    "\n",
    "$$Z = W \\cdot A + b$$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $A$: The input to this layer (or the output of the previous layer).\n",
    "   * $W$: The Weight matrix (the \"knobs\" we tune).\n",
    "   * $b$: The Bias vector (shifts the activation up or down).\n",
    "   * $Z$: The linear output (pre-activation).\n",
    "\n",
    "**Dimensions (The Most Important Part)**\n",
    "To make the matrix multiplication work, the dimensions must align. We will use the convention where each **column** is a training example.\n",
    "\n",
    "   * $A$: Shape $(n_{in}, m)$ - where $n_{in}$ is input features,  is number of examples.\n",
    "   * $W$: Shape $(n_{out}, n_{in})$ - where $n_{out}$ is the number of neurons in this layer.\n",
    "   * $b$: Shape $(n_{out}, 1)$ -  Python broadcasting will apply this to all  examples.\n",
    "   * $Z$: Shape $(n_{out}, m)$.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Think of $W$ as a filter. If the input $A$ matches the pattern in row $i$ of $W$, the resulting dot product will be high. The bias $b$ acts as a threshold; if the weighted sum isn't high enough to overcome a negative bias, the neuron stays \"quiet\" (after activation).\n",
    "\n",
    "### Trade-offs & Pitfalls\n",
    "\n",
    "* **Initialization:** You cannot initialize $W$ to zeros. If you do, every neuron learns the exact same thing (symmetry problem). We usually initialize them with small random numbers. $b$ can be initialized to zeros.\n",
    "* **Broadcasting:** In Python/NumPy, if you aren't careful with the shape of $b$(e.g., `(n,)` vs `(n, 1)`), you might introduce subtle bugs where dimensions mismatch silently.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement two functions using **NumPy**:\n",
    "\n",
    "1. `initialize_parameters(n_in, n_out)`:\n",
    "* Initialize `W` with random values from a standard normal distribution, scaled by `0.01` (to keep values small).\n",
    "* Initialize `b` with zeros.\n",
    "* **Return:** `W`, `b`\n",
    "\n",
    "\n",
    "2. `linear_forward(A, W, b)`:\n",
    "   * Perform the dot product $W \\cdot A$.\n",
    "   * Add the bias $b$.\n",
    "   * **Return:** `Z` (the result), and a `cache` tuple containing `(A, W, b)` (we will need these later for backpropagation).\n",
    "\n",
    "\n",
    "\n",
    "**Input Constraints:**\n",
    "\n",
    "   * Use `numpy.random.randn` for weights.\n",
    "   * Ensure  has shape $(n_{out}, 1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b611d4a-8f6a-479a-b9b7-802f0b9f12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NN1():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self.sd = 0.01\n",
    "\n",
    "\n",
    "    def initialize_parameters(self, n_in, n_out):\n",
    "        \"\"\"\n",
    "        Initialize W with random values from a standard normal distribution, scaled by 0.01 (to keep values small).\n",
    "        Initialize b with zeros.\n",
    "        Return: W, b\n",
    "        \"\"\"\n",
    "        W = self.sd * np.random.randn(n_out, n_in)\n",
    "        b = np.zeros((n_out, 1))\n",
    "        return W, b\n",
    "\n",
    "\n",
    "    def linear_forward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Perform the dot product: W.A \n",
    "        Add the bias b.\n",
    "        Return: Z (the result), and a cache tuple containing (A, W, b) (we will need these later for backpropagation).\n",
    "        \"\"\"\n",
    "\n",
    "        Z = np.dot(W, A) + b\n",
    "        return Z, (A, W, b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4991fe-bb6e-4ba8-8e0c-053c83c71ec1",
   "metadata": {},
   "source": [
    "## **Concept 2: Activation Functions (ReLU & Sigmoid)**\n",
    "\n",
    "If we only used linear layers (like the one you just built), it wouldn't matter how many layers we stacked—the final output would still be just a linear combination of the inputs. The network could only learn to draw straight lines (linear boundaries).\n",
    "\n",
    "**Activation Functions** introduce non-linearity, allowing the network to learn complex curves and shapes.\n",
    "\n",
    "### 1. ReLU (Rectified Linear Unit)\n",
    "    \n",
    "$$A = \\max(0, Z)$$\n",
    "\n",
    "   * **Intuition:** It's a gate. If the input is positive, let it through unchanged. If it's negative, shut it off (make it zero).\n",
    "   * **Context:** This is the default choice for **hidden layers**.\n",
    "   * **Why?** It is computationally cheap (just a `max` check) and helps prevent the \"vanishing gradient\" problem that older functions (like Tanh) suffered from.\n",
    "\n",
    "### 2. Sigmoid\n",
    "\n",
    "$$A = \\frac{1}{1 + e^{-Z}}$$\n",
    "\n",
    "   * **Intuition:** It squashes any number (from  to ) into a range between **0 and 1**.\n",
    "   * **Context:** This is the standard choice for the **output layer** in binary classification.\n",
    "   * **Why?** It lets us interpret the output as a probability (e.g., \"70% chance this is a Moon\").\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Add these static methods or standalone functions to your class (or outside it, your choice).\n",
    "\n",
    "1. `sigmoid(Z)`:\n",
    "   * Implement the formula: $1 / (1 + np.exp(-Z))$\n",
    "   * **Return:** `A`, `cache` (where `cache` is just `Z`).\n",
    "\n",
    "\n",
    "2. `relu(Z)`:\n",
    "   * Implement: Return `Z` if `Z > 0`, else `0`. (Hint: `np.maximum` is faster than `max`).\n",
    "   * **Return:** `A`, `cache` (where `cache` is just `Z`).\n",
    "\n",
    "\n",
    "\n",
    "**Why return `Z` as cache?**\n",
    "Just like in the linear layer, we need the input `Z` to calculate the slope (derivative) of these functions later during backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f723c7f4-5272-4ce7-8b88-e1c3d1704e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class N12():\n",
    "    \"\"\"\n",
    "    Class with ReLU and Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "\n",
    "    def get_sigmoid(self):\n",
    "        A = 1/(1 + np.exp(-self.Z))\n",
    "        return A, self.Z\n",
    "\n",
    "    def get_ReLU(self):\n",
    "        A = np.maximum(0, self.Z)\n",
    "        return A, self.Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee650352-e475-4fb3-b53c-a956d14a980e",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 3: Loss Function (Binary Cross-Entropy)**\n",
    "\n",
    "Now that our network can make a prediction ($A$), we need to measure how \"wrong\" that prediction is. This measurement is called the **Loss** or **Cost**.\n",
    "\n",
    "Since we are doing **Binary Classification** (0 vs 1), we use **Binary Cross-Entropy Loss** (also called Log Loss).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "The cost $J$ over $m$ examples is:\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} \\Big[ y^{(i)} \\log(a^{(i)}) + (1 - y^{(i)}) \\log(1 - a^{(i)}) \\Big]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "   * $y^{(i)}$: The true label (0 or 1).\n",
    "   * $a^{(i)}$: The network's prediction (probability between 0 and 1).\n",
    "\n",
    "### Intuition\n",
    "\n",
    "   * **If correct answer is 1 ($y=1$):** The second term $(1-y)$ becomes 0. We only care about $\\log(a)$.\n",
    "   * If prediction $a$ is close to 1 (e.g., 0.99), $\\log(0.99) \\approx 0$. **Cost is low.**\n",
    "   * If prediction $a$ is close to 0 (e.g., 0.01), $\\log(0.01)$ is a large negative number. **Cost is high.**\n",
    "   \n",
    "   \n",
    "   * **If correct answer is 0 ($y=0$):** The first term becomes 0. We care about $\\log(1-a)$.\n",
    "   * It works symmetrically.\n",
    "\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **MSE (Mean Squared Error):** Common for regression (predicting house prices), but bad for classification because it learns slowly when the error is large (due to the shape of the gradient).\n",
    "* **Cross-Entropy:** Heavily penalizes confident wrong predictions, speeding up learning.\n",
    "\n",
    "\n",
    "Certainly. Let's strip away the math symbols and look at the intuition.\n",
    "\n",
    "### Simpler Explanation: The \"Strict Teacher\" Analogy\n",
    "\n",
    "Imagine you are taking a test where you have to answer \"True\" (1) or \"False\" (0). However, instead of just answering Yes/No, you have to write down your **confidence percentage**.\n",
    "\n",
    "**Scenario 1: The correct answer is TRUE (1).**\n",
    "\n",
    "**Student A** says: \"99% sure it's True.\"\n",
    "Teacher: \"Great job. You are right and confident.\" -> **Loss: 0.01 (Tiny penalty)**\n",
    "\n",
    "**Student B** says: \"50% sure it's True.\"\n",
    "Teacher: \"Okay, you're wishy-washy.\" -> **Loss: 0.69 (Medium penalty)**\n",
    "\n",
    "**Student C** says: \"0.1% sure it's True\" (They are 99.9% sure it's False).\n",
    "Teacher: \"You are confidently wrong! That is the worst mistake.\" -> **Loss: Huge Number (Severe penalty)**\n",
    "\n",
    "\n",
    "\n",
    "**Why do we use Logs?**\n",
    "The logarithm function is just a mathematical tool to make the penalty \"explode\" the closer you get to being 100% wrong.\n",
    "\n",
    "   * If you are slightly wrong, the Log Loss is small.\n",
    "   * If you are completely, arrogantly wrong (predicting 100% \"No\" when it is \"Yes\"), the Log Loss goes to **infinity**.\n",
    "\n",
    "**Why not just use simple difference (Mean Squared Error)?**\n",
    "If we used simple subtraction (like in linear regression), the penalty for being \"confidently wrong\" isn't harsh enough. The Cross-Entropy loss forces the neural network to fix its worst mistakes (the ones where it is confidently wrong) *very* quickly.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `compute_cost(AL, Y)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "   1. Implement the formula: $J = -\\frac{1}{m} \\sum \\Big[ y \\log(a) + (1 - y) \\log(1 - a) \\Big]$ using `np.multiply`, `np.log`, and `np.sum`.\n",
    "   2. `AL` is the activation output of the final layer (shape: $1, m$).\n",
    "   3. `Y` is the true label vector (shape: $1, m$).\n",
    "   4. **Important:** The result `cost` must be a scalar (a single number). Use `np.squeeze()` at the end to turn `[[0.5]]` into `0.5`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf3393e0-fdab-4d66-8cc4-f4de354cbf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    n, m = Y.shape\n",
    "    J = -np.multiply((1/m), np.sum(np.multiply(Y, np.log(AL)) + np.multiply((1- Y), np.log(1 - AL))))\n",
    "    return np.squeeze(J)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb7f43-1c4b-4c27-af64-777675cfae28",
   "metadata": {},
   "source": [
    "**Pro Tip (Optimization):** In production code, we often add a tiny number (epsilon, e.g., `1e-8`) inside the `np.log()` to prevent `log(0)` errors if the model predicts exactly 0 or 1. For now, your strict implementation is mathematically perfect.\n",
    "\n",
    "#### 1. The Problem: The \"Black Hole\" of Log(0)\n",
    "\n",
    "In math, the function $\\log(x)$ asks: *\"To what power must I raise  (2.718) to get $x$?\"*\n",
    "\n",
    "If you try to calculate $\\log(0)$, you are asking: *\"To what power do I raise 2.718 to get 0?\"*\n",
    "There is no answer. You can get very close to zero by using a huge negative power (like $e^{-1000}$), but you never actually touch zero. Therefore, mathematically:\n",
    "$$\\log(0) = -\\infty \\text{ (Negative Infinity)}$$\n",
    "\n",
    "**In Python:**\n",
    "If your neural network predicts a perfect `0` (which can happen due to rounding errors) for a label that is actually `1`:\n",
    "\n",
    "```python\n",
    "np.log(0)  # Output: -inf\n",
    "\n",
    "```\n",
    "\n",
    "If you try to do math with `-inf` (like adding it to other numbers), your entire calculation crashes or turns into `NaN` (Not a Number). The training stops instantly.\n",
    "\n",
    "#### 2. The Solution: The \"Safety Cushion\"\n",
    "\n",
    "To prevent the code from crashing, we add a tiny, insignificant number called **epsilon** ($\\epsilon$), usually $10^{-8}$ (0.00000001).\n",
    "\n",
    "Instead of calculating $\\log(A)$, we calculate $\\log(A + \\epsilon)$.\n",
    "\n",
    "   * If $A = 0.5$: $\\log(0.50000001) \\approx -0.69$. (No noticeable change).\n",
    "   * If $A = 0$: $\\log(0.00000001) \\approx -18.4$.\n",
    "      * Now, instead of \"Negative Infinity,\" we just get a \"Very Large Penalty.\" The computer is happy, and training continues.\n",
    "\n",
    "\n",
    "This concept was **Cross-Entropy Math**. Adding \"Numerical Stability Hacks\" (like epsilon) creates noise when you are trying to just memorize the formula.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddf2b3-8170-45dd-8a7c-79bb122ef80f",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 4: The Chain Rule (Backpropagation Theory)**\n",
    "\n",
    "We have the Loss ($J$). Now we need to minimize it. To do that, we need to know: **\"If I nudge this specific weight  slightly up, does the Loss go up or down?\"**\n",
    "\n",
    "This is the **Gradient** ($\\frac{\\partial J}{\\partial W}$). Since our network is just a chain of functions nested inside each other ($Loss(Sigmoid(Linear(X)))$), we use the **Chain Rule** from calculus to find this gradient.\n",
    "\n",
    "### Intuition: The \"Blame Game\"\n",
    "\n",
    "Imagine a relay race where the team loses. Who is to blame?\n",
    "\n",
    "1. **The Coach (Loss Function):** Yells at the last runner (Activation). \"You were too slow!\"\n",
    "2. **The Last Runner (Activation):** Blames their shoes (Linear Output Z). \"The fit was tight!\"\n",
    "3. **The Shoes (Linear Output):** Blame the manufacturer (Weights W).\n",
    "\n",
    "Mathematically, we calculate the blame (gradient) in reverse order.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "We want to find $\\frac{\\partial J}{\\partial W}$. We break it down into steps:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial W} = \\underbrace{\\frac{\\partial J}{\\partial A}}_{\\text{Change in Loss vs Pred}} \\cdot \\underbrace{\\frac{\\partial A}{\\partial Z}}_{\\text{Change in Pred vs Linear}} \\cdot \\underbrace{\\frac{\\partial Z}{\\partial W}}_{\\text{Change in Linear vs Weights}}$$\n",
    "\n",
    "This chain allows us to pass the error signal backward through any number of layers.\n",
    "\n",
    "### The \"Magic\" Shortcut\n",
    "\n",
    "For Logistic Regression (and the output layer of our binary classifier), the calculus simplifies beautifully.\n",
    "\n",
    "If you combine the derivative of the **Cross-Entropy Loss** with the derivative of the **Sigmoid Activation**, the first step of backpropagation ($dZ$) simplifies to just:\n",
    "$$dZ = A - Y$$\n",
    "\n",
    " * If prediction $A$ is 0.8 and Truth $Y$ is 1.0, the \"error signal\" is .\n",
    " * This term ($A-Y$) is the \"upstream gradient\" that kicks off the entire backpropagation process.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task (Mental Check)\n",
    "\n",
    "I will not ask for Python code yet. I want to verify you understand the data flow.\n",
    "\n",
    "**Scenario:**\n",
    "You have a network: $X \\xrightarrow{W_1} Z_1 \\xrightarrow{ReLU} A_1 \\xrightarrow{W_2} Z_2 \\xrightarrow{Sigmoid} A_2 \\xrightarrow{Loss} J$.\n",
    "\n",
    "**Question:** To calculate the gradient for the first layer's weights ($W_1$), we need the error signal from the future.\n",
    "Complete this dependency chain:\n",
    "To get $\\frac{\\partial J}{\\partial W_1}$, we need $\\frac{\\partial J}{\\partial Z_1}$.\n",
    "To get $\\frac{\\partial J}{\\partial Z_1}$, we need the gradient from the layer ahead of it. Which variable represents the \"error signal\" coming back from layer 2? (Choose one: $W_2$, $Z_2$, or $dZ_2$?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3dd57-2b3e-4f0b-b452-5f3c87c090c6",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:** $dZ_2$ is the error signal at the linear part of Layer 2, which allows us to calculate everything before it.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5: Backward Propagation (The Linear Step)**\n",
    "\n",
    "Now we translate the math into code. We will tackle this in two parts. First, the **Linear Backward** step.\n",
    "\n",
    "Assume we already know the error $dZ$ at the end of a linear layer. We need to calculate three things:\n",
    "\n",
    "1. **$dW$ (Gradient of Weights):** How much to change $W$.\n",
    "2. **$db$ (Gradient of Bias):** How much to change $b$.\n",
    "3. **$dA_{prev}$ (Gradient of Input):** The error to send back to the previous layer.\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "Recall that $Z = W \\cdot A_{prev} + b$.\n",
    "Using calculus (matrix calculus), the gradients are:\n",
    "\n",
    "   $$dW = \\frac{1}{m} dZ \\cdot A_{prev}^T$$\n",
    "   $$db = \\frac{1}{m} \\sum_{axis=1} dZ$$\n",
    "   $$dA_{prev} = W^T \\cdot dZ$$\n",
    "\n",
    "**Key Details:**\n",
    "\n",
    "   * **Transposing ($T$):** We transpose matrices to make the dimensions align for multiplication.\n",
    "      * $dZ$ : $(n_{out}, m)$\n",
    "      * $A_{prev}$ : $(n_{in}, m)$ -> $A_{prev}^T$ : $(m, n_{in})$\n",
    "      * Result $dW$ : $(n_{out}, n_{in})$ (Matches shape of $W$).\n",
    "\n",
    "   * **Summing:** For $db$, we sum the errors across all $m$ examples to get one average adjustment per neuron.\n",
    "\n",
    "\n",
    "### Simpler Explanation: The \"Project Post-Mortem\"\n",
    "\n",
    "Imagine a project failed (High Loss), and now we are doing a \"post-mortem\" meeting to figure out what went wrong. We are working backwards from the failure.\n",
    "\n",
    "We are currently analyzing the **Linear Layer** ($Z = W \\cdot A + b$).\n",
    "The layer ahead of us (the Activation) has just handed us a report called **$dZ$**. This report says: *\"The output  you gave me was too high/low by this amount.\"*\n",
    "\n",
    "Now, the Linear Layer has to decide who is responsible for this error. It splits the blame into three parts:\n",
    "\n",
    "**1. Blame the Weights ($dW$)**\n",
    "   * **Question:** \"Did we set the importance (weight) of the input incorrectly?\"\n",
    "   * **Intuition:** If the input ($A_{prev}$) was very loud (a large number), even a tiny mistake in the Weight ($W$) would cause a huge error in the result.\n",
    "   * **Action:** We calculate $dW$ so we can adjust the knobs ($W$) later to fix this.\n",
    "\n",
    "**2. Blame the Bias (db)**\n",
    "   * **Question:** \"Was our baseline threshold (bias) too high or low?\"\n",
    "   * **Intuition:** This is the \"average\" error that remains after checking the weights. If we were consistently too high across all examples, the bias takes the blame.\n",
    "   * **Action:** We calculate $db$ so we can shift the baseline ($b$) later.\n",
    "\n",
    "**3. Pass the Blame Backward ($dA_{prev}$) -- CRITICAL STEP**\n",
    "   * **Question:** \"Maybe I (the Linear Layer) did my job correctly, but the *data given to me* ($dA_{prev}$) by the previous guy was garbage?\"\n",
    "   * **Intuition:** We need to calculate how much the *input* contributed to the error.\n",
    "   * **Action:** We calculate $dA_{prev}$ and hand it backward to the previous layer. They will use this as their \"incoming error\" to fix their own weights.\n",
    "\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement the function `linear_backward(dZ, cache)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "* **Inputs:**\n",
    "   * `dZ`: The gradient of the cost with respect to the linear output ($Z$).\n",
    "   * `cache`: The tuple `(A_prev, W, b)` you stored during the forward pass.\n",
    "\n",
    "\n",
    "* **Operations:**\n",
    "   * Retrieve `A_prev`, `W`, `b` from `cache`.\n",
    "   * Get `m` from the shape of `A_prev` (number of columns).\n",
    "   * Calculate `dW`, `db`, `dA_prev` using the formulas above.\n",
    "   * **Important:** `db` requires `np.sum` with `axis=1` and `keepdims=True` (to maintain shape `(n, 1)`).\n",
    "\n",
    "\n",
    "* **Returns:** `dA_prev`, `dW`, `db`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bdbbf2-57bc-43c8-8089-8ce4c4c35500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    At = np.transpose(A_prev)\n",
    "    dW = np.multiply((1/m), np.dot(dZ, At))\n",
    "    db = np.multiply((1/m),np.sum(dZ, axis = 1, keepdims = True))\n",
    "    dA_prev = np.dot(np.transpose(W), dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77df8e9-22f9-4435-b0f3-693914382421",
   "metadata": {},
   "source": [
    "\n",
    "You have successfully implemented the \"Linear\" part of the backward pass. Now we need to implement the \"Activation\" part to complete the chain.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5 (Part 2): Activation Backward**\n",
    "\n",
    "We just calculated how to move from $dZ$ to $dW$. But how do we get $dZ$ in the first place?\n",
    "We get it from $dA$ (the gradient coming from the layer ahead).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "The chain rule tells us:\n",
    "$$dZ = \\frac{\\partial L}{\\partial A} \\cdot \\frac{\\partial A}{\\partial Z} = dA \\cdot g'(Z)$$\n",
    "\n",
    "This means calculating $dZ$ is just an **element-wise multiplication** between the incoming error $dA$ and the **slope (derivative)** of the activation function at that point $Z$.\n",
    "\n",
    "#### 1. Derivative of ReLU\n",
    "\n",
    "If $Z > 0$, the slope is 1 (linear). If $Z \\le 0$, the slope is 0 (flat).\n",
    "$$g'(Z) = \\begin{cases} 1 & \\text{if } Z > 0 \\\\ 0 & \\text{if } Z \\le 0 \\end{cases}$$\n",
    "\n",
    "   * **Logic:** Copy $dA$ exactly where $Z$ was positive. Zero out $dA$ where $Z$ was negative.\n",
    "\n",
    "#### 2. Derivative of Sigmoid\n",
    "\n",
    "The derivative of the sigmoid function $s(Z)$ has a beautiful property:\n",
    "$$g'(Z) = s(Z) \\cdot (1 - s(Z))$$\n",
    "\n",
    "   * **Logic:** We first calculate the sigmoid $s$ of the cached $Z$, then use it to scale $dA$.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement two standalone functions:\n",
    "\n",
    "1. `relu_backward(dA, cache)`\n",
    "   * **Input:** `dA` (gradient of activation), `cache` (this is just `Z` stored from forward pass).\n",
    "   * **Logic:** Create `dZ` which is a copy of `dA`. Set values in `dZ` to `0` wherever the corresponding `Z` (from cache) is $\\le 0$.\n",
    "   * **Return:** `dZ`.\n",
    "\n",
    "\n",
    "2. `sigmoid_backward(dA, cache)`\n",
    "   * **Input:** `dA`, `cache` (stored `Z`).\n",
    "   * **Logic:**\n",
    "      * Calculate $s = 1 / (1 + e^{-Z})$.\n",
    "      * Calculate $dZ = dA * s * (1 - s)$. (Element-wise multiplication).\n",
    "\n",
    "   * **Return:** `dZ`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe6aecab-6b31-40ab-985e-c5bcae276053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0 \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1 + np.exp(-Z))\n",
    "    dZ = np.multiply(dA, (s*(1-s)))\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91374a19-462d-480d-9f77-c87469eaba24",
   "metadata": {},
   "source": [
    "\n",
    "We have successfully built the engine (Linear), the transmission (Activation), and the diagnostic tools (Loss & Gradients). Now we need the driver.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 6: Optimization (Gradient Descent)**\n",
    "\n",
    "Calculating the gradients ($dW$, $db$) tells us the direction of the steepest slope uphill. Since we want to minimize the loss (go downhill), we simply take a step in the **opposite direction**.\n",
    "\n",
    "This process is called **Gradient Descent**.\n",
    "\n",
    "### Intuition: The Mountain in the Fog\n",
    "\n",
    "Imagine you are lost on a mountain at night (foggy). You want to get to the village at the bottom (Minimum Loss).\n",
    "\n",
    "   1. You feel the slope of the ground under your feet (Calculate Gradients).\n",
    "   2. You take a small step downhill (Update Parameters).\n",
    "   3. You repeat this until the ground flattens out (Convergence).\n",
    "\n",
    "**The Learning Rate ($\\alpha$):**\n",
    "This controls the size of your step.\n",
    "\n",
    "   * **Too small:** You will take forever to reach the bottom.\n",
    "   * **Too big:** You might overshoot the village and end up on the other side of the valley (divergence).\n",
    "\n",
    "### Mechanics & Math\n",
    "\n",
    "For every parameter $\\theta$ (which represents all our $W$ and $b$ matrices):\n",
    "$$\\theta = \\theta - \\alpha \\cdot d\\theta$$\n",
    "\n",
    "In our specific case with a dictionary of parameters:\n",
    "\n",
    "   * $W = W - \\alpha \\cdot dW$\n",
    "   * $b = b - \\alpha \\cdot db$\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement `update_parameters(parameters, grads, learning_rate)`.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "   * **Inputs:**\n",
    "      * `parameters`: A dictionary containing `{\"W1\": ..., \"b1\": ..., \"W2\": ..., \"b2\": ...}`.\n",
    "      * `grads`: A dictionary containing `{\"dW1\": ..., \"db1\": ..., \"dW2\": ..., \"db2\": ...}`. (Output from backprop).\n",
    "      * `learning_rate`: A scalar (e.g., 0.01).\n",
    "   \n",
    "   \n",
    "   * **Logic:**\n",
    "      * Update every parameter in the dictionary using the formula above.\n",
    "      * **Note:** You must update them \"in place\" or return a new dictionary with the updated values. Returning a new dictionary is safer and cleaner.\n",
    "   * **Return:** `parameters` (the updated dictionary).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da549cbd-d2c9-46b7-8c58-12c051db5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    new_params = {}\n",
    "    new_params.update(parameters)\n",
    "\n",
    "    for key in new_params.keys():\n",
    "        grad_key = \"d\" + key\n",
    "        val = new_params[key]\n",
    "        new_params[key] = val - np.multiply(learning_rate, grads[grad_key])\n",
    "    return new_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87447956-bbe4-4e3c-838c-d77370f3d8ad",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 7: Data Preparation (The \"Moons\" Dataset)**\n",
    "\n",
    "We have a functioning neural network engine. Now we need fuel.\n",
    "We will use the **\"Make Moons\"** dataset from Scikit-Learn. It generates two interleaving half-circles.\n",
    "\n",
    "**Why this dataset?**\n",
    "A linear classifier (like Logistic Regression) cannot solve this. You cannot draw a single straight line to separate the two moons. This proves our Neural Network's ability to learn **non-linear boundaries**.\n",
    "\n",
    "\n",
    "We are building a Neural Network, which is complex and computationally expensive. If we just wanted to classify data that can be separated by a straight line (like separating \"Tall people\" from \"Short people\"), we could just use simple Logistic Regression.\n",
    "\n",
    "We need a problem that **fails** on simple models to prove our Neural Network is actually working.\n",
    "\n",
    "   * **The Shape:** The \"Moons\" dataset consists of two crescent shapes that interlock like two bananas.\n",
    "   * **The Challenge:** You cannot draw a single straight line to separate these two shapes. You need a curvy, wiggle line.\n",
    "   * **The Proof:** If our network successfully classifies this, it proves the **Hidden Layer (ReLU)** is doing its job of bending the decision boundary. If the network fails (gets 50% accuracy), we know our non-linearity isn't working.\n",
    "\n",
    "### Mechanics & Dimensions (Critical)\n",
    "\n",
    "Scikit-Learn returns data in the standard format: **Rows = Examples**.\n",
    "\n",
    "* `X`: Shape `(m, 2)` (where m is 1000 examples, 2 features).\n",
    "* `y`: Shape `(m,)` (Rank-1 array).\n",
    "\n",
    "**Our Network's Requirement:**\n",
    "Our math assumes **Columns = Examples**.\n",
    "\n",
    "* We need `X` to be `(2, m)`.\n",
    "* We need `y` to be `(1, m)` (a proper row vector, not a Rank-1 array).\n",
    "\n",
    "### Why Transpose and Reshape? (The \"Dimension Mismatch\" Trap)\n",
    "\n",
    "This is the most common source of bugs in Deep Learning.\n",
    "\n",
    "**The World Standard (Scikit-Learn/Pandas):**\n",
    "Most data libraries store data like a spreadsheet:\n",
    "   * **Rows:** Examples (Users, Images, etc.)\n",
    "   * **Columns:** Features (Age, Height, Pixel values)\n",
    "   * Shape: `(1000 examples, 2 features)`\n",
    "\n",
    "**Our Math Standard (Linear Algebra/Vectorization):** \n",
    "Our formula is $Z = W \\cdot X + b$..\n",
    "\n",
    "   * $W$ (Weights) has shape `(Neurons, Features)`.\n",
    "   * To multiply $W$ by $X$, the rows of $X$ must match the columns of $W$.\n",
    "   * Therefore, we need $X$ to be `(Features, Examples)`.\n",
    "\n",
    "If we feed the default Scikit-Learn data into our code:\n",
    "\n",
    "$$W_{(4, 2)} \\cdot X_{(1000, 2)}$$\n",
    "\n",
    "**CRASH.** Inner dimensions (2 vs 1000) don't match.\n",
    "We must Transpose $X$ to be `(2, 1000)` so that:\n",
    "$$W_{(4, 2)} \\cdot X_{(2, 1000)} = Z_{(4, 1000)}$$\n",
    "\n",
    "**The Rank-1 Array Issue (`y`):**\n",
    "Scikit-Learn returns labels `y` as a flat array: `[0, 1, 1, 0...]` with shape `(1000,)`.\n",
    "\n",
    "   * NumPy treats this as \"shapeless\" in certain operations.\n",
    "   * If you subtract this from a matrix `(1, 1000)`, NumPy might create a giant `(1000, 1000)` matrix instead of what you want.\n",
    "   * **The Fix:** We force it to be a strict row vector: `(1, 1000)`.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement a function `load_data()`:\n",
    "\n",
    "1. Import `sklearn.datasets`.\n",
    "2. Use `sklearn.datasets.make_moons` to generate `N=1000` samples with `noise=0.2`.\n",
    "3. **Transpose X:** Convert shape from `(1000, 2)` to `(2, 1000)`.\n",
    "4. **Reshape Y:** Convert shape from `(1000,)` to `(1, 1000)`.\n",
    "5. **Return:** `X`, `Y`.\n",
    "\n",
    "*(Note: No visualization code required yet, just data processing)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca67c00-8376-4e97-a489-98cb78e28308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fcb9fb-2e76-4f77-b1df-fb4ebe019bd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd21d1c-f191-436c-9b62-640b1d6f7cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63ccd6-50b7-4e02-82f8-4b03e7ef0fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0840d0-ed49-439e-b0bc-20e7c47edc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12d476-cf84-43ca-9f77-9f816ab2f96f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cdf4f-331b-4138-9b33-435b2b0d2562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
