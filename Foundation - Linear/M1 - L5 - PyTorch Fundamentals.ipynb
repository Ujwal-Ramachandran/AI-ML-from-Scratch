{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a833bf-e511-4230-b619-c8e286091b6a",
   "metadata": {},
   "source": [
    "# **PyTorch Fundamentals**\n",
    "\n",
    "\n",
    "You are transitioning from manual gradient calculations to using a framework that handles the heavy lifting, allowing you to focus on architecture and data.\n",
    "\n",
    "Here is the breakdown for **L5: PyTorch Fundamentals**.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "I have structured this to ensure you understand the \"PyTorch Way\" of doing things before we assemble the full MNIST classifier.\n",
    "\n",
    "```text\n",
    "L5: PyTorch Fundamentals\n",
    "├── Concept 1: Tensors & Device Management\n",
    "│   ├── Tensor creation and operations (vs NumPy)\n",
    "│   ├── GPU/CUDA context (Hardware Check)\n",
    "│   ├── Purpose: The fundamental data structure of deep learning\n",
    "│   ├── Simple terms: N-dimensional arrays that live on video cards\n",
    "│   └── Task: Create tensors, perform math, and move them to your RTX 4060\n",
    "│\n",
    "├── Concept 2: Autograd (Automatic Differentiation)\n",
    "│   ├── Computational Graphs (DAGs)\n",
    "│   ├── .requires_grad and .backward()\n",
    "│   ├── Purpose: Automating the chain rule\n",
    "│   ├── Simple terms: The engine that remembers your math to calculate gradients later\n",
    "│   └── Task: Manually compute gradients for a simple equation using Autograd\n",
    "│\n",
    "├── Concept 3: Data Handling (Dataset & DataLoader)\n",
    "│   ├── torch.utils.data.Dataset (Custom Class structure)\n",
    "│   ├── torch.utils.data.DataLoader (Batching, Shuffling)\n",
    "│   ├── Purpose: Decoupling data loading from training logic\n",
    "│   ├── Simple terms: An organized conveyor belt feeding data to your model\n",
    "│   └── Task: Implement a dummy Custom Dataset and iterate through it\n",
    "│\n",
    "├── Concept 4: Model Architecture (nn.Module)\n",
    "│   ├── nn.Module class structure (__init__, forward)\n",
    "│   ├── nn.Sequential (Container)\n",
    "│   ├── Purpose: Encapsulating state (weights) and behavior (forward pass)\n",
    "│   ├── Simple terms: Blueprints for your neural network layers\n",
    "│   └── Task: Define a simple Multilayer Perceptron (MLP) for MNIST input\n",
    "│\n",
    "├── Concept 5: Loss & Optimization (Implicit Prerequisite)\n",
    "│   ├── torch.nn Loss functions (CrossEntropyLoss)\n",
    "│   ├── torch.optim (SGD/Adam)\n",
    "│   ├── Purpose: Measuring error and updating weights\n",
    "│   ├── Simple terms: The scoreboard (loss) and the coach (optimizer) correcting the players\n",
    "│   └── Task: Initialize loss and optimizer for the model\n",
    "│\n",
    "└── Concept 6: The Training Loop (The \"Build\")\n",
    "    ├── The Standard Cycle: Forward → Loss → Backward → Step → Zero Grad\n",
    "    ├── Purpose: Orchestrating the learning process\n",
    "    ├── Simple terms: The actual practice session where learning happens\n",
    "    └── Mini-Project: Complete MNIST Digit Classifier Training\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a77c0-9fe9-4e50-8234-467db521b79f",
   "metadata": {},
   "source": [
    "## **Concept 1: Tensors & Device Management**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the world of PyTorch, the **Tensor** is the primary citizen. While it looks and behaves almost exactly like a NumPy array (n-dimensional grid of numbers), it has two superpowers that NumPy lacks:\n",
    "   1. **GPU Acceleration:** Tensors can live on the GPU (VRAM) rather than just the CPU (RAM). This allows for massive parallel processing, which is critical for deep learning.\n",
    "   2. **Autograd Compatibility:** Tensors can track the history of operations performed on them to automatically calculate gradients later (we will cover this in Concept 2).\n",
    "\n",
    "Think of a CPU as a Professor: extremely smart, capable of complex logic, but can only do one or two things at a time. Think of a GPU as an army of minions: individually simple, but there are thousands of them working exactly in sync. Deep learning is mostly multiplying huge matrices, a task perfectly suited for the army.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "   * **Creation:** You can create tensors directly (`torch.tensor([1, 2])`) or convert from NumPy (`torch.from_numpy(arr)`).\n",
    "   * **Device Management:** Every tensor has a `.device` property. By default, they are created on the `'cpu'`.\n",
    "   * **Moving Data:** You cannot perform operations (like addition or multiplication) between a tensor on the CPU and a tensor on the GPU. They must be on the same device. You move them using `.to(device)` or `.cuda()`.\n",
    "   * **Asynchronous Execution:** CUDA (GPU) operations are asynchronous. When Python tells the GPU to \"multiply these matrices,\" the GPU says \"Okay, I'll get to it\" and Python immediately moves to the next line of code. If you want to time GPU operations accurately, you must force Python to wait until the GPU is finished using a synchronization command.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "A Tensor is just a container for numbers. If the container is in System RAM, the CPU does the math. If you move the container to Video RAM (VRAM), the graphics card does the math—much faster.\n",
    "\n",
    "### Trade-offs & Pitfalls\n",
    "\n",
    "   * **Overhead:** Moving data between CPU and GPU is slow (it travels over the PCIe bus). For small operations (like adding two numbers), the transfer time takes longer than the actual math. GPU is only worth it for *large* matrix operations.\n",
    "   * **VRAM Limits:** If you load too many huge tensors, you will get a \"CUDA Out of Memory\" error.\n",
    "\n",
    "\n",
    "### PyTorch Prerequisites: The Syntax Toolkit\n",
    "\n",
    "PyTorch is designed to look and feel like NumPy, but with extra commands for the GPU. Here are the specific tools you need for this task.\n",
    "\n",
    "#### 1. Checking Hardware\n",
    "\n",
    "To see if your GPU is accessible, PyTorch provides a boolean check:\n",
    "\n",
    "```python\n",
    "# Returns True if GPU is ready, False otherwise\n",
    "status = torch.cuda.is_available()\n",
    "\n",
    "```\n",
    "#### **What is CUDA?**\n",
    "\n",
    "**CUDA** (Compute Unified Device Architecture) is a software layer created by **NVIDIA**.\n",
    "\n",
    "Think of it as a translator.\n",
    "   * **You (Python/PyTorch):** Speak high-level code instructions (\"Multiply this matrix\").\n",
    "   * **Your GPU:** Speaks low-level hardware voltage signals.\n",
    "\n",
    "Without CUDA, your Python code has no way to talk to the graphics card. The GPU is just a rock that draws pixels on your screen.\n",
    "\n",
    "**CUDA** provides the bridge. It allows developers to send general-purpose math problems (not just graphics) to the GPU to be solved.\n",
    "\n",
    "#### **Why do we check for it?**\n",
    "\n",
    "In PyTorch, `torch.cuda.is_available()` is effectively asking:\n",
    "   1. Do you have an NVIDIA GPU?\n",
    "   2. Do you have the correct drivers installed?\n",
    "   3. Can PyTorch \"see\" and talk to that GPU?\n",
    "\n",
    "If the answer is `True`, you unlock the ability to train models 50x to 100x faster than on your CPU. If not try:\n",
    "\n",
    "   `pip uninstall torch torchvision torchaudio`\n",
    "\n",
    "   `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`\n",
    "\n",
    "#### 2. Creating Tensors\n",
    "\n",
    "The syntax is nearly identical to NumPy.\n",
    "   * **NumPy:** `np.random.rand(rows, cols)`\n",
    "   * **PyTorch:** `torch.rand(rows, cols)` (Creates a tensor with random numbers between 0 and 1)\n",
    "\n",
    "#### 3. Moving Data (The `.to()` method)\n",
    "\n",
    "When a tensor is created, it sits in CPU RAM by default. You move it using `.to()` or specific device methods.\n",
    "\n",
    "```python\n",
    "# Create on CPU\n",
    "x = torch.rand(100, 100)\n",
    "\n",
    "# Move to GPU (returns a NEW copy on the device)\n",
    "x_gpu = x.to('cuda')\n",
    "\n",
    "```\n",
    "\n",
    "*Note: You can also use string variables: `device = 'cuda' if torch.cuda.is_available() else 'cpu'`, then `x.to(device)`.*\n",
    "\n",
    "#### 4. Matrix Operations\n",
    "\n",
    "* **Multiplication:** You can use the standard `@` symbol for matrix multiplication, just like in modern NumPy.\n",
    "   * `result = tensor_a @ tensor_b`\n",
    "   * *Constraint:* Both `tensor_a` and `tensor_b` must be on the **same device**. If one is on CPU and one is on GPU, it will crash.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Timing GPU Operations (Crucial)\n",
    "\n",
    "Because Python sends commands to the GPU asynchronously (fire-and-forget), Python might finish its code (stop the timer) while the GPU is still crunching numbers in the background.\n",
    "To get an accurate time, you must force Python to wait:\n",
    "\n",
    "```python\n",
    "# Start Timer\n",
    "# ... do gpu operations ...\n",
    "torch.cuda.synchronize() # Wait for all GPU kernels to finish\n",
    "# Stop Timer\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Now that you have the tools, let's attempt the logic.\n",
    "\n",
    "**Goal:** Prove the speed difference between CPU and GPU on your machine.\n",
    "\n",
    "**Steps to Implement:**\n",
    "\n",
    "1. Import `torch` and `time`.\n",
    "2. Define a `device` variable (use the check from tool #1). Print what device you got.\n",
    "3. Create two large random tensors (e.g., shape `10000, 10000`) on the CPU.\n",
    "4. **CPU Benchmark:**\n",
    "   * Record `start_time`.\n",
    "   * Perform `matrix1 @ matrix2`.\n",
    "   * Record `end_time` and print the duration.\n",
    "\n",
    "5. **GPU Benchmark:**\n",
    "   * Move both tensors to the GPU using `.to('cuda')`.\n",
    "   * **Warm-up:** Run the multiplication once without timing it (GPUs take a moment to \"wake up\").\n",
    "   * Record `start_time`.\n",
    "   * Perform multiplication.\n",
    "   * **Synchronize:** Call `torch.cuda.synchronize()`.\n",
    "   * Record `end_time` and print duration.\n",
    "\n",
    "**Inputs:** None (Generative)\n",
    "\n",
    "**Outputs:** Device name, CPU time, GPU time, Speedup factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6a1ae62-c307-4216-90cd-d5b7a521b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Creating torches\n",
      "===========\n",
      "Start Comparision\n",
      "Time Taken for CPU = 17.051012992858887 sec\n",
      "Start Synchronization\n",
      "Time Taken for GPU = 0.2580296993255615 sec\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"Creating torches\")\n",
    "x_cpu = torch.rand(10000, 10000)\n",
    "y_cpu = torch.rand(10000, 10000)\n",
    "\n",
    "print(\"===========\\nStart Comparision\")\n",
    "start_time = time.time()\n",
    "_ = x_cpu @ y_cpu\n",
    "end_time = time.time()\n",
    "print(f\"Time Taken for CPU = {end_time - start_time} sec\")\n",
    "\n",
    "x_gpu = x_cpu.to(\"cuda\")\n",
    "y_gpu = y_cpu.to(\"cuda\")\n",
    "\n",
    "print(\"Start Synchronization\") # Wait for everything before starting\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "_ = x_gpu @ y_gpu\n",
    "torch.cuda.synchronize() # <--- CRITICAL: Wait for GPU to finish!\n",
    "end_time = time.time()\n",
    "print(f\"Time Taken for GPU = {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698048f-86b9-4d17-b411-579206683263",
   "metadata": {},
   "source": [
    "\n",
    "**16.42s vs 0.25s.**\n",
    "Your GPU is approximately **65x faster** than your CPU for this task. This is why we use GPUs.\n",
    "\n",
    "Now that we know how to hold data (Tensors) and where to put it (GPU), we need to discuss how PyTorch learns.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept 2: Autograd (Automatic Differentiation)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the \"From Scratch\" module, you had to manually derive the gradients (derivatives) for every function using Calculus. If you changed your model architecture, you had to re-do all the calculus on paper and re-write the backward pass code.\n",
    "\n",
    "**Autograd** removes this burden. It is an engine that records every operation you perform on a tensor. When you are done, you press a \"rewind\" button, and it automatically calculates the gradients for you using the Chain Rule.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "1. **The Tape Recorder:** When you create a tensor with `requires_grad=True`, PyTorch starts a log.\n",
    "   * If you do `y = x + 2`, PyTorch remembers: \"To get $y$, we took $x$ and added 2.\"\n",
    "   * It builds a **Computational Graph** (a Directed Acyclic Graph) connecting inputs to outputs.\n",
    "\n",
    "\n",
    "2. **The Backward Pass:** When you call `.backward()` on the final result (usually the Loss), PyTorch walks backward through this graph.\n",
    "3. **The `.grad` attribute:** The calculated derivatives are stored in the `.grad` property of the input tensors.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine you are walking through a maze (the forward pass). You leave a trail of breadcrumbs behind you. When you reach the end (the loss), you follow the breadcrumbs back to the start (backward pass) to figure out which turns led you to the mistake.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Memory:** Storing this graph takes memory. This is why we turn it off (`torch.no_grad()`) when we are just testing/evaluating the model, to save RAM.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "We will verify Autograd against your own calculus knowledge.\n",
    "\n",
    "**Target Equation:**\n",
    "$$y = x^3 + 5$$\n",
    "\n",
    "**Analytical Derivative (Calculus):**\n",
    "$$\\frac{dy}{dx} = 3x^2$$\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Create a tensor `x` with the value `4.0`. **Important:** You must set `requires_grad=True`.\n",
    "2. Calculate `y = x**3 + 5`.\n",
    "3. Print the value of `y`.\n",
    "4. Call `y.backward()`.\n",
    "5. Print the value of `x.grad`.\n",
    "6. Manually calculate $3(4)^2$ in Python and print it to prove they match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fedcbb8d-7c42-49c1-b840-39fa34c79943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = 69.0\n",
      "X Grad = 48.0\n",
      "3(4)**2 = 48\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad = True)\n",
    "y = x ** 3 + 5\n",
    "print(f\"Y = {y}\")\n",
    "y.backward()\n",
    "print(f\"X Grad = {x.grad}\")\n",
    "print(f\"3(4)**2 = {3*(4**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823f34e-92f3-46b5-9cd6-8b460b168dd9",
   "metadata": {},
   "source": [
    "Perfect. You just proved that PyTorch can perform Calculus for you.\n",
    "\n",
    "Now we move to how we feed data into these systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 3: Data Handling (Dataset & DataLoader)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the \"From Scratch\" module, you likely loaded all your data into one giant NumPy array (e.g., `X_train`).\n",
    "**Problem:** What if your dataset is 500GB of images? You can't load that into RAM.\n",
    "**Solution:** The **Dataset** class. It acts like a librarian. It doesn't hold all the books (data) in its hands; it just knows *where* they are on the shelf and how to fetch *one* when asked.\n",
    "\n",
    "The **DataLoader** is the delivery truck. It asks the Librarian for 32 books (a batch), packs them into a box, and delivers them to the GPU.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "To manage data in PyTorch, you almost always create a custom class that inherits from `torch.utils.data.Dataset`.\n",
    "\n",
    "You **must** implement three magic methods:\n",
    "   1. `__init__`: Setup (load file paths, CSVs, etc.).\n",
    "   2. `__len__`: Returns the total number of samples.\n",
    "   3. `__getitem__(index)`: Returns **one specific sample** at the given `index`.\n",
    "\n",
    "### Syntax Toolkit\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the item at index 'idx'\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "```\n",
    "\n",
    "Once the class is defined, you hand it to the loader:\n",
    "\n",
    "```python\n",
    "# batch_size=4 means it groups 4 samples into one big tensor\n",
    "loader = DataLoader(dataset_instance, batch_size=4, shuffle=True)\n",
    "\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a \"Lazy\" dataset that generates random numbers on the fly.\n",
    "\n",
    "**Requirements:**\n",
    "1. Define a class `RandomDataset` inheriting from `Dataset`.\n",
    "   * `__init__`: Accepts an integer `length`. Stores it.\n",
    "   * `__len__`: Returns the `length`.\n",
    "   * `__getitem__`: Ignores the actual `idx`. Instead, it generates a random tensor of shape `(3,)` (the feature) and a random integer (0 or 1) (the label). It returns a tuple: `(feature, label)`.\n",
    "\n",
    "2. Instantiate the dataset with a length of **10**.\n",
    "3. Wrap it in a `DataLoader` with `batch_size=4`.\n",
    "4. Write a loop to iterate through the loader.\n",
    "5. Inside the loop, print the shape of the features batch and the labels batch.\n",
    "\n",
    "**Expected Output intuition:**\n",
    "If batch size is 4 and length is 10, you should see 3 loops.\n",
    "\n",
    "   * Loop 1: 4 items\n",
    "   * Loop 2: 4 items\n",
    "   * Loop 3: 2 items (the remainder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d496841b-bf6f-404d-a7e8-3c7bf9b70a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Interation 1:\n",
      "Features = tensor([[0.6995, 0.8537, 0.7609],\n",
      "        [0.4655, 0.4702, 0.3763],\n",
      "        [0.6756, 0.4313, 0.4652],\n",
      "        [0.9021, 0.4186, 0.5740]])\n",
      "Labels = tensor([1, 1, 1, 0])\n",
      "For Interation 2:\n",
      "Features = tensor([[0.5449, 0.1629, 0.1815],\n",
      "        [0.2859, 0.3900, 0.5773],\n",
      "        [0.1734, 0.2097, 0.5909],\n",
      "        [0.2663, 0.4277, 0.4301]])\n",
      "Labels = tensor([0, 0, 1, 0])\n",
      "For Interation 3:\n",
      "Features = tensor([[0.0786, 0.5816, 0.5636],\n",
      "        [0.3999, 0.5130, 0.2747]])\n",
      "Labels = tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.rand(3)\n",
    "        ran_in = random.randint(0,1)\n",
    "        return (tensor, ran_in)\n",
    "\n",
    "\n",
    "dataset_instance = RandomDataset(10)\n",
    "loader = DataLoader(dataset_instance, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, (f, l) in enumerate(loader):\n",
    "    print(f\"For Interation {i+1}:\\nFeatures = {f}\\nLabels = {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2caee7-41f9-4fcb-b963-938fa1d84b6a",
   "metadata": {},
   "source": [
    "Excellent. You can see how the `DataLoader` automatically stacked your individual tensors into batches (e.g., `[4, 3]` shape). This automation is what makes training on millions of images possible.\n",
    "\n",
    "Now, we need something to consume that data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 4: Model Architecture (nn.Module)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In PyTorch, every neural network is a Python class that inherits from `nn.Module`.\n",
    "Think of this class as a blueprint.\n",
    "   1. **`__init__` (The Inventory):** You list the parts you need (layers). \"I need a Linear layer with 784 inputs, a ReLU activation, etc.\"\n",
    "   2. **`forward` (The Assembly):** You define how data flows through those parts. \"Take input `x`, pass it through layer 1, then apply ReLU, then layer 2.\"\n",
    "This separation allows for complex, non-linear flows (like skipping layers) that you'll need for things like ResNet later.\n",
    "\n",
    "### Mechanics (The Syntax Toolkit)\n",
    "\n",
    "You need three main components for a basic network:\n",
    "   1. **`nn.Flatten()`:** Images are 2D grids (e.g., 28x28). Dense layers (`nn.Linear`) only understand flat lists (vectors). This layer squashes the grid into a single line (28*28 = 784).\n",
    "   2. **`nn.Linear(in_features, out_features)`:** This is the standard \"Dense\" layer you built from scratch in Module 1 ($y = xW^T + b$).\n",
    "   3. **`nn.ReLU()`:** The activation function.\n",
    "\n",
    "**Class Structure:**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define flow here\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Sequential vs. Class:** You *can* use `nn.Sequential` (a list of layers) for simple stacks, but the **Class** structure is mandatory for advanced architectures (Transformers, ResNets). We will stick to the Class structure to build good habits.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Build a Multi-Layer Perceptron (MLP) designed for the MNIST dataset.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "1. Create a class `MNISTClassifier` inheriting from `nn.Module`.\n",
    "2. **`__init__`:**\n",
    "   * Define a `flatten` layer.\n",
    "   * Define a first linear layer: Input **784** (28x28 pixels), Output **128**.\n",
    "   * Define a ReLU activation.\n",
    "   * Define a second linear layer (Output Layer): Input **128**, Output **10** (for digits 0-9).\n",
    "\n",
    "\n",
    "3. **`forward`:** Connect them in order: Flatten $\\to$ Linear1 $\\to$ ReLU $\\to$ Linear2.\n",
    "4. **Test it:**\n",
    "   * Instantiate the model.\n",
    "   * Move it to the GPU (`.to(device)`).\n",
    "   * Create a dummy input tensor representing **one batch of 8 images** (Shape: `(8, 28, 28)`). **Important:** Don't forget to move this input to the GPU too!\n",
    "   * Pass the input through the model.\n",
    "   * Print the output shape.\n",
    "\n",
    "\n",
    "\n",
    "**Expected Output Shape:** `torch.Size([8, 10])` (8 images, 10 class probabilities each).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8554d8-b8a9-4057-973e-3ace6f1ca653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe88915-5e27-471e-9de2-a7b196f72a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3aadfd-7463-4f15-bec9-79551792a6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c428586-f3b6-4ba0-8d9c-425e07cbaea5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d3098-68a6-45d6-b063-a4c6d7af459c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915a993-c85e-4c5d-9d37-935febf0883a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c27f99-58d3-41e1-9c4d-436a64cf7adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078e06a-38bd-4d72-9e1f-2263cfa24080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8983eaf-3f98-4ecc-9ead-eef68e57ffc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf1e7e-a294-48e1-bdd8-f0285d0b5f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b23895-7880-4a13-8670-c7d39af7b1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101765c-9b1b-4476-ab19-7e03f16b3d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
