{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7878f4dc-788a-48fa-b5c4-0868e436fd9d",
   "metadata": {},
   "source": [
    "# **L6: Optimization Techniques**\n",
    "\n",
    "We are now moving into the engine room of deep learning. You've built neural networks (L4) and learned how to build them in PyTorch (L5). Now, we need to discuss **how** they actually learn.\n",
    "\n",
    "A neural network is essentially a massive mathematical function with millions of knobs (parameters). Optimization is the specific algorithm used to turn those knobs to minimize error. Choosing the right optimizer can be the difference between a model that converges in minutes versus one that never learns at all.\n",
    "\n",
    "Here is the roadmap for this session. I have structured this to build from the simplest \"blind\" descent to modern adaptive methods.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "```text\n",
    "L6: Optimization Techniques\n",
    "├── Concept 1: The Baseline (Data & Vanilla SGD)\n",
    "│   ├── The MNIST Dataset (Implicit Prerequisite)\n",
    "│   ├── Stochastic Gradient Descent (SGD) Mechanics\n",
    "│   ├── Intuition: Walking down a hill with small steps\n",
    "│   ├── Simpler Terms: Learning by trial and error using small batches\n",
    "│   └── Task: Setup the Data, Model, and a basic Train function using SGD\n",
    "│\n",
    "├── Concept 2: Momentum\n",
    "│   ├── The problem of Local Minima & Saddle Points\n",
    "│   ├── Velocity vector (accumulating past gradients)\n",
    "│   ├── Intuition: Rolling a heavy ball down a hill (it gathers speed)\n",
    "│   ├── Simpler Terms: Using past speed to power through flat areas\n",
    "│   └── Task: Train the model using SGD with Momentum\n",
    "│\n",
    "├── Concept 3: RMSProp (Root Mean Square Propagation)\n",
    "│   ├── Adaptive Learning Rates\n",
    "│   ├── Handling different scales for different parameters\n",
    "│   ├── Intuition: Slowing down on steep slopes, speeding up on flat ones\n",
    "│   ├── Simpler Terms: Adjusting step size based on how shaky the terrain is\n",
    "│   └── Task: Train the model using RMSProp\n",
    "│\n",
    "├── Concept 4: Adam (Adaptive Moment Estimation)\n",
    "│   ├── Combining Momentum + RMSProp\n",
    "│   ├── Bias correction\n",
    "│   ├── Intuition: The \"Gold Standard\" general-purpose optimizer\n",
    "│   ├── Simpler Terms: Smart speed + Smart steering\n",
    "│   └── Task: Train the model using Adam\n",
    "│\n",
    "├── Concept 5: Learning Rate Schedulers (Cosine Annealing)\n",
    "│   ├── The Learning Rate Decay concept\n",
    "│   ├── Cosine Annealing mechanics\n",
    "│   ├── Intuition: Refining the search as we get closer to the target\n",
    "│   ├── Simpler Terms: Slowing down carefully to park the car perfectly\n",
    "│   └── Task: Implement a scheduler and visualize the learning rate change\n",
    "│\n",
    "└── Mini-Project: Convergence Battle\n",
    "    └── Train all three methods on MNIST and plot the Loss vs. Epochs comparison\n",
    "\n",
    "```\n",
    "\n",
    "**Prerequisite Check:**\n",
    "To compare these properly, we need a consistent environment. I am treating the **Data Loading (MNIST)** and a **Simple MLP Architecture** as part of Concept 1. We will fix the architecture so the only variable changing is the *Optimizer*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c74892-c893-485d-af0b-d70ef23340db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Concept 1: The Baseline (Data & Vanilla SGD)**\n",
    "\n",
    "We begin with the fundamental workhorse of deep learning: Stochastic Gradient Descent (SGD).\n",
    "\n",
    "### Intuition: The \"Drunk\" Descent\n",
    "\n",
    "Imagine you are on a mountain at night (foggy, zero visibility), and your goal is to reach the lowest valley (minimum error).\n",
    "\n",
    "* **Batch Gradient Descent:** You carefully scan the entire mountain (all data) to calculate the exact slope before taking one step. It is precise but incredibly slow.\n",
    "* **Stochastic Gradient Descent (SGD):** You grab a handful of data points (a mini-batch), quickly estimate the slope based on just those, and take a step. Because the sample is small, your estimate is noisy. You might stagger left or right (hence \"drunk\"), but on average, you move downhill much faster because you take thousands of steps in the time it takes Batch GD to take one.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "In strict mathematical terms, \"Stochastic\" means one sample at a time. In practice (Deep Learning), we almost always use **Mini-batch SGD**.\n",
    "\n",
    "The update rule for a parameter $\\theta$ at step  is:\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla L_{batch}(\\theta_t)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\eta$ (Eta): Learning Rate (step size).\n",
    "* $\\nabla L_{batch}$: The gradient calculated over a small batch of data (e.g., 32 or 64 images).\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine learning to identify cars.\n",
    "\n",
    "* **Batch GD:** You look at 10,000 photos of cars, process all of them, and then tweak your brain once.\n",
    "* **SGD:** You look at 64 photos, tweak your brain immediately. Look at the next 64, tweak again. You learn much faster because you are updating your understanding constantly.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Pros:** Computational efficiency (doesn't require all data in RAM), frequent updates result in faster convergence initially. The \"noise\" introduced by random batches can actually help the model jump out of shallow local minima.\n",
    "* **Cons:** The path to the minimum is jagged and oscillates. It can have trouble settling down into the exact bottom of the valley.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task: Set the Stage\n",
    "\n",
    "Since we will compare multiple optimizers, we need a reusable training setup.\n",
    "\n",
    "**Objectives:**\n",
    "\n",
    "1. **Data:** Use `torchvision` to load the MNIST dataset. Transform it to tensors and normalize. Create **DataLoaders** for training and testing with a batch size of 64.\n",
    "2. **Model:** Define a Class `SimpleMLP`.\n",
    "* Input: 784 features (28x28 flattened).\n",
    "* Hidden: 128 units with ReLU activation.\n",
    "* Output: 10 units (classes).\n",
    "\n",
    "\n",
    "3. **Train Function:** Write a function `train_one_epoch(model, train_loader, optimizer, criterion)`.\n",
    "* It should iterate through the loader.\n",
    "* Perform the forward pass, loss computation, backward pass, and optimizer step.\n",
    "* **Crucial:** Remember to zero out gradients before the backward pass.\n",
    "* Return: The average loss for that epoch.\n",
    "\n",
    "\n",
    "\n",
    "**Implementation Details:**\n",
    "\n",
    "* Instantiate the model.\n",
    "* Use `torch.optim.SGD` (no momentum yet, just `lr=0.01`).\n",
    "* Use `nn.CrossEntropyLoss`.\n",
    "* Run it for **1 epoch** just to verify the pipeline works and print the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b368097-12d2-4568-a048-01195f10f0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f016e60-8777-421c-a3e9-2a3431d89c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c123a-4c9e-46a1-876d-d8de0270f082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eaa26a-aa22-4064-b18d-6d8475033e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16914bd8-c0b8-400b-b853-7059ee12f8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c24cd8-f48d-4864-a686-4789498316a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132f5a52-4ff0-4a2c-826e-b9dd6206c945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261f132-6335-4c47-936d-0627933ca3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68f013-dac2-4d6d-bd5a-355e007ca3ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b3f4cf-3543-45aa-b54e-7bc22f0b1768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e115739-ebe5-4711-812f-f54e8d660351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cba9ab8-0cd8-420e-9f4a-1df1a34db50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f77ed7-df5f-41dc-ae84-154175f94e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cbae03-9b22-4879-8cf8-21f1daf6fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2272c73-01f2-4094-9006-c6d951fea17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbdc01-0187-4168-aaa4-3b587afb11c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fa84d-ab8f-49c0-bcd3-1c4af0408c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b67787-b612-4992-96a5-c83fd19cc0b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee376726-eb5f-4b55-8eea-db3b6e6d645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be559fde-6c27-481d-9d47-69ab12cdd402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
