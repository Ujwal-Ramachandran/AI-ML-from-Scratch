{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0605750a-c06c-4f1c-b38f-c9de8db1603b",
   "metadata": {},
   "source": [
    "# **Module 3: NLP & LLM Core**\n",
    "\n",
    "## L13: Text Processing & Embeddings\n",
    "\n",
    "Welcome to the NLP module. We are moving from Computer Vision (pixels) to Natural Language Processing (sequences/tokens). This is the foundation for the Agentic AI and RAG work you have planned later.\n",
    "\n",
    "This lesson focuses on how we convert human language into numerical vectors that machines can \"understand.\" We will progress from simple frequency counts to deep semantic representations.\n",
    "\n",
    "### Topic Breakdown\n",
    "\n",
    "```text\n",
    "L13: Text Processing & Embeddings\n",
    "├── Concept 1: Tokenization (Subword & BPE)\n",
    "│   ├── Word-level vs. Character-level vs. Subword\n",
    "│   ├── The OOV (Out of Vocabulary) Problem\n",
    "│   ├── Byte Pair Encoding (BPE) Intuition\n",
    "│   ├── Explanation: Breaking text into meaningful chunks (tokens)\n",
    "│   └── Task: Use a tokenizer to inspect tokenization differences\n",
    "│\n",
    "├── Concept 2: Sparse Representations (TF-IDF) [Baseline]\n",
    "│   ├── Term Frequency (TF)\n",
    "│   ├── Inverse Document Frequency (IDF)\n",
    "│   ├── Explanation: Weighing words by how \"rare\" and \"informative\" they are\n",
    "│   └── Task: Compute TF-IDF matrix for a mini-corpus using sklearn\n",
    "│\n",
    "├── Concept 3: Static Dense Embeddings (Word2Vec/GloVe Intuition)\n",
    "│   ├── One-Hot vs. Dense Vectors\n",
    "│   ├── Semantic Meaning in Vector Space (King - Man + Woman = Queen)\n",
    "│   ├── Limitation: Context Independence (Polysemy)\n",
    "│   └── Task: Manual Cosine Similarity calculation on mock embedding vectors\n",
    "│\n",
    "├── Concept 4: Transformer Embeddings (Sentence-BERT)\n",
    "│   ├── Contextual Embeddings (Why \"bank\" differs in two sentences)\n",
    "│   ├── The Cross-Encoder vs. Bi-Encoder (Siamese Network) architecture\n",
    "│   ├── Explanation: Capturing the meaning of whole sentences\n",
    "│   └── Task: Load a Sentence-Transformer model and encode text\n",
    "│\n",
    "└── Mini-Project: Semantic Classifier Comparison\n",
    "    ├── Dataset: 20 Newsgroups (Subset) or similar text dataset\n",
    "    ├── Pipeline A: TF-IDF + Logistic Regression\n",
    "    ├── Pipeline B: SBERT Embeddings + Logistic Regression\n",
    "    └── Evaluation: Compare Accuracy/F1 Score\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48a953-00b1-4333-86ff-73911b295246",
   "metadata": {},
   "source": [
    "## **Concept 1: Tokenization (Subword & BPE)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Before a model can process text, it must be broken down into smaller units called **tokens**. The simplest approach is splitting by spaces (Word-level), but this fails when the model encounters a word it hasn't seen before (the \"Out-Of-Vocabulary\" or **OOV** problem). Conversely, splitting by characters (Character-level) solves OOV but results in extremely long sequences where individual units carry little meaning.\n",
    "\n",
    "Modern NLP uses **Subword Tokenization** (e.g., Byte-Pair Encoding or BPE). This is the \"Goldilocks\" zone. It breaks common words into single tokens (e.g., \"apple\") but breaks rare or complex words into meaningful sub-units (e.g., \"tokenization\"  \"token\", \"##iza\", \"##tion\"). This allows the model to process *any* text using a fixed-size vocabulary.\n",
    "\n",
    "### Mechanics: Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE works by iteratively merging the most frequently occurring adjacent pairs of characters (or bytes) in the training corpus.\n",
    "   1. **Initialize:** Vocabulary includes all individual characters.\n",
    "   2. **Count:** Calculate frequency of all symbol pairs (e.g., \"e\" + \"s\" $\\rightarrow$ \"es\").\n",
    "   3. **Merge:** Add the most frequent pair to the vocabulary as a new symbol.\n",
    "   4. **Repeat:** Continue until the vocabulary size reaches a target limit (e.g., 30k or 50k tokens).\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Think of tokens like Lego bricks.\n",
    "   * **Word-level:** Every unique word is a custom-molded brick. If you need a \"microscope\" brick and don't have it, you can't build the sentence.\n",
    "   * **Character-level:** You only have 26 types of tiny 1x1 bricks. You can build anything, but it takes thousands of bricks to build a simple house.\n",
    "   * **Subword (BPE):** You have a set of standard complex shapes (walls, windows) for common structures, but you also keep the tiny 1x1 bricks. If you encounter a rare structure, you build it using the standard shapes and the tiny bricks.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Solves OOV (can represent any string), balances sequence length and meaning.\n",
    "   * **Cons:** Handling the \"sub-tokens\" (like `##ing` in BERT) requires careful implementation. Typos can result in weird subword splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will use the Hugging Face `transformers` library to observe how a subword tokenizer handles known words versus rare words/typos.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Import:** `AutoTokenizer` from `transformers`.\n",
    "   2. **Load:** The tokenizer for `bert-base-uncased`.\n",
    "   3. **Input Text:** \"unaffable\" (a standard word) vs \"unaffabwle\" (a typo/nonsense word).\n",
    "   4. **Action:**\n",
    "      * Tokenize both strings.\n",
    "      * Convert the IDs back to tokens (strings) to see the split.\n",
    "   5. **Output:** Print the list of tokens for both words.\n",
    "\n",
    "**Note:** You might need to install transformers: `pip install transformers`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5a5ca0-d038-4f00-bf8c-56be1cc4e9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ebcc23-66cf-42de-9eef-79061ef20ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For unaffable\n",
      "Token = {'input_ids': [101, 14477, 20961, 3468, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "Back to string = ['[CLS]', 'una', '##ffa', '##ble', '[SEP]']\n",
      "For unaffabwle\n",
      "Token = {'input_ids': [101, 14477, 20961, 2497, 13668, 2063, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "Back to string = ['[CLS]', 'una', '##ffa', '##b', '##wl', '##e', '[SEP]']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "word1 = \"unaffable\"\n",
    "word2 = \"unaffabwle\"\n",
    "\n",
    "t1 = tokenizer(word1)\n",
    "t2 = tokenizer(word2)\n",
    "\n",
    "\n",
    "w1 = tokenizer.convert_ids_to_tokens(t1[\"input_ids\"])\n",
    "w2 = tokenizer.convert_ids_to_tokens(t2[\"input_ids\"])\n",
    "\n",
    "print(f\"For {word1}\\nToken = {tokenizer(word1)}\\nBack to string = {w1}\")\n",
    "print(f\"For {word2}\\nToken = {tokenizer(word2)}\\nBack to string = {w2}\\n==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d755d0-2e95-46f2-b493-b6e447644af9",
   "metadata": {},
   "source": [
    "\n",
    "Excellent. This perfectly illustrates the power of Subword Tokenization.\n",
    "\n",
    "* **\"unaffable\"**: Broken into 3 logical chunks (`una`, `##ffa`, `##ble`). The model can likely infer the meaning is related to \"unable\" or \"affable\" based on these sub-parts.\n",
    "* **\"unaffabwle\"**: The tokenizer didn't panic or crash. It just kept cutting until it found pieces it recognized (`##b`, `##wl`, `##e`). Even though the word is nonsense, the model has a valid input vector to process.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 2: Sparse Representations (TF-IDF)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "After tokenization, we have a list of tokens. The simplest way to turn these into numbers is counting them (\"Bag of Words\"). However, raw counts have a flaw: common words like \"the\", \"is\", and \"and\" appear frequently but carry little information.\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) fixes this by balancing two factors:\n",
    "\n",
    "1. **Frequency:** How often does the word appear in *this specific* document? (More is better).\n",
    "2. **Rarity:** How often does the word appear in *all* documents? (Less is better).\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "The score $w_{t,d}$ for a term $t$ in document $d$ is:\n",
    "$$TF_{t,d} = \\frac{\\text{count of t in d}}{\\text{total terms in d}}$$\n",
    "\n",
    "1. **TF (Term Frequency):**\n",
    "$$w_{t,d} = TF_{t,d} \\times IDF_t$$\n",
    "*(Note: Implementations often use raw count or log normalization)*\n",
    "\n",
    "2. **IDF (Inverse Document Frequency):**\n",
    "$$IDF_t = \\log \\left( \\frac{N}{df_t} \\right)$$\n",
    "Where $N$ is the total number of documents, and $df_t$ is the number of documents containing term $t$.\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine you are scanning a library for books about \"Quantum Physics\".\n",
    "   * The word \"the\" is in every book. $IDF \\approx 0$. It gets a score of 0.\n",
    "   * The word \"Quantum\" appears many times in specific books, but not in cookbooks or novels. It has high TF (in the physics book) and high IDF (rare globally). It gets a high score.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Very fast, interpretable (you know exactly which words triggered the score), works surprisingly well for simple keyword matching.\n",
    "   * **Cons:** **Sparse** (vectors are mostly zeros), **No Semantics** (it doesn't know \"car\" and \"automobile\" are related; they are just different orthogonal dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will manually calculate the TF-IDF matrix using Scikit-Learn to see the sparsity.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Import:** `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
    "   2. **Data:** Create a list of strings:\n",
    "   ```python\n",
    "   corpus = [\n",
    "       \"the cat sat on the mat\",\n",
    "       \"the dog sat on the log\",\n",
    "       \"cats and dogs are great\"\n",
    "   ]\n",
    "   \n",
    "   ```\n",
    "\n",
    "   3. **Action:**\n",
    "      * Initialize the vectorizer.\n",
    "      * Fit and transform the corpus.\n",
    "      * Get the feature names (the vocabulary).\n",
    "      * Convert the result to a dense array (using `.toarray()`) or a DataFrame for readability.\n",
    "   \n",
    "   \n",
    "   4. **Output:** Print the feature names and the resulting matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1c5df-59e7-4951-9e28-4eb2b8ae7c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fe88b-5af3-4f48-a801-5935b73520d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee7ffd-d214-4ead-ba1b-92cb6db7b1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d419dc-24f7-4801-b332-f3e7f443096c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe8d75c-c320-4d82-9dfe-cb7ed2d2bfbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4e56a-db88-49b9-9354-cb47c58f79d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99540b82-7923-413b-aeca-f14837e42484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10766b44-6f01-4b75-ad4e-73d482c8cd18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bc98e-893b-451f-9297-5f80ce0fabe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
