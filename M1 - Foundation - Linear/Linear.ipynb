{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337edf64-1d2d-4f5b-ae86-ecbe86533ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f38bb4d6-9ad2-408b-bfda-68b880d1206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. THE BATCH (3 Pizzas, 3 Features each)\n",
    "# Shape: (3, 3) -> (Batch_Size, Input_Features)\n",
    "inputs = np.array([\n",
    "    [0.9, 0.5, 0.2], # Pizza 1\n",
    "    [0.1, 0.2, 0.9], # Pizza 2\n",
    "    [0.5, 0.5, 0.5]  # Pizza 3\n",
    "])\n",
    "\n",
    "# 2. THE WEIGHTS (1 Critic's preferences)\n",
    "# Shape: (3,) -> (Input_Features,)\n",
    "weights = np.array([5.0, 1.0, 0.5])\n",
    "\n",
    "# 3. THE BIAS\n",
    "bias = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c18a244-90f0-417f-8ccb-d14206d94ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.1 , 2.15, 4.25])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Matrix Multiplication\n",
    "# We still use np.dot. NumPy automatically handles the matrix-vector multiplication.\n",
    "outputs = np.dot(inputs, weights) + bais\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242180a8-fa74-4996-8514-3e9a5ca684f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b298e3-4eda-49e8-9f27-2b4000f9fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. INPUTS (Batch of 3 Pizzas)\n",
    "inputs = np.array([\n",
    "    [0.9, 0.5, 0.2],\n",
    "    [0.1, 0.2, 0.9],\n",
    "    [0.5, 0.5, 0.5]\n",
    "])\n",
    "\n",
    "# 2. WEIGHTS (3 Critics now!)\n",
    "# Column 0: Critic 1 (Loves Cheese)\n",
    "# Column 1: Critic 2 (Loves Crust)\n",
    "# Column 2: Critic 3 (Loves Sauce)\n",
    "weights = np.array([\n",
    "    [5.0, 0.1, 0.2], # Weights for Cheese\n",
    "    [1.0, 5.0, 0.5], # Weights for Crust\n",
    "    [0.5, 0.2, 5.0]  # Weights for Sauce\n",
    "])\n",
    "\n",
    "# 3. BIASES (One bias per Critic)\n",
    "biases = np.array([1.0, 0.5, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffa0b91-751c-4fbb-84b9-ebcadd6821bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.1 , 3.13, 1.53],\n",
       "       [2.15, 1.69, 4.72],\n",
       "       [4.25, 3.15, 2.95]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = np.dot(inputs, weights) + biases\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6278b8-dad6-43c2-886d-897681d62c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76cc8dbf-198a-4bae-9e81-86271c2e880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Task: Write a Python script using numpy that performs exactly what we just did manually.\n",
    "# Initialize the input vector x (shape 1x2).\n",
    "# Initialize the weights matrix W (shape 2x3) with random numbers.\n",
    "# Initialize the bias vector b (shape 1x3) with random numbers.\n",
    "# Perform the dot product.\n",
    "# Add the bias.\n",
    "# Print the final output and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "219a7f8e-c9de-46b5-ae58-49593d12816f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.12 22.16 -1.29]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.2, 5.4])\n",
    "W = np.array([[1.0, -1.2, 0.7],\n",
    "             [-0.2, 4.0, -0.95]])\n",
    "b = np.array([1, 2, 3])\n",
    "output = np.dot(x,W) + b\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297217c-0d72-41d2-9990-e44421f028cb",
   "metadata": {},
   "source": [
    "### **The \"Shape Error\" Analysis**\n",
    "\n",
    "**Question:** What happens if you change `inputs` to have 4 numbers but keep `weights` at 3? Try it and tell me the error message. This is the \"Shape Error\" your worst enemy in AI, and we need to meet it today.\n",
    "\n",
    "In AI Engineering, we call this a **Dimension Mismatch**.\n",
    "When you tried to dot product a vector of size `(4,)` with weights of size `(3,)`, NumPy looked for a partner for the 4th number and found nothing.\n",
    "\n",
    "**The Rule of Algebra:**\n",
    "To multiply two things, their \"inner\" dimensions must match.\n",
    "$$A (M \\times \\textcolor{red}{N}) \\cdot B (\\textcolor{red}{N} \\times K) = C (M \\times K)$$\n",
    "\n",
    "If the inner numbers ($\\textcolor{red}{N}$ and $\\textcolor{red}{N}$) are different, the code explodes.\n",
    "\n",
    "**Question for you:**\n",
    "Look at the `outputs` variable.\n",
    "You fed in **3** pizzas.\n",
    "Did you get **1** score or **3** scores out?\n",
    "Why does this result make sense for a neural network processing a batch of images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63061f2-4d1e-4a4b-935b-50da97dbc665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "213c881f-9898-4d4e-b423-27e1781d2461",
   "metadata": {},
   "source": [
    "\n",
    "You got **3 scores** because you processed a **Batch of 3**.\n",
    "In Deep Learning, we never feed one image at a time. We feed a batch (e.g., 32 images) to the GPU, and the GPU performs this exact matrix multiplication to give us 32 predictions instantly.\n",
    "\n",
    "\n",
    "### **Level Up: The \"Dense Layer\" (Multiple Neurons)**\n",
    "\n",
    "Real neural networks don't just have one neuron (one critic). They have many.\n",
    "Imagine we now have **3 Critics** rating the same pizzas:\n",
    "\n",
    "1.  **Critic 1:** Loves Cheese.\n",
    "2.  **Critic 2:** Loves Crust.\n",
    "3.  **Critic 3:** Loves Sauce.\n",
    "\n",
    "This means our `Weights` are no longer a vector. They become a **Matrix** too.\n",
    "\n",
    "**The Math of Shapes:**\n",
    "\n",
    "  * Inputs: `(3, 3)` [3 Pizzas, 3 Features]\n",
    "  * Weights: `(3, 3)` [3 Features, 3 Critics]\n",
    "  * **Result:** `(3, 3)` [3 Pizzas, 3 Scores (one from each critic)]\n",
    "\n",
    "**Your Task:**\n",
    "Modify your code to handle multiple critics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56ba57-dfd4-4c28-bad6-4148f5299a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f1e04d4-4d93-40c4-96d6-765287150b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.12  22.16  -1.29 ]\n",
      " [ 1.48   1.8    3.255]\n",
      " [-0.4   11.2    0.4  ]]\n"
     ]
    }
   ],
   "source": [
    "X_batch = np.array([[1.2, 5.4], [0.5, 0.1], [-1.0, 2.0]])\n",
    "W = np.array([[1.0, -1.2, 0.7],\n",
    "             [-0.2, 4.0, -0.95]])\n",
    "b = np.array([1, 2, 3])\n",
    "output = np.dot(X_batch,W) + b\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee8791f-e423-4bdf-8fab-488164657e8b",
   "metadata": {},
   "source": [
    "**Crucial Question**: You don't need to change W or b. Why does the math still work?\n",
    "\n",
    "**Answer**:\n",
    "The maths works because of the shape sizes.\n",
    "```\n",
    "X_batch = 3x2\n",
    "W       = 2x3\n",
    "Output  = 3x3\n",
    "b       = 3x1\n",
    "```\n",
    "\n",
    "### üõ†Ô∏è One Critical Correction (The Shape of Bias)\n",
    "The bias `b` corresponds to the **Neurons**, not the **Samples**.\n",
    "* We have **3 Neurons**, so we have **3 Biases**.\n",
    "* We have **3 Samples** (the batch), but they **share** the same neurons (and thus the same biases).\n",
    "\n",
    "If `b` were `3x1` (a vertical column), NumPy would try to add `bias[0]` to the first *sample*, `bias[1]` to the second *sample*, etc.\n",
    "Instead, `b` acts as **`1x3`** (a row). NumPy takes that single row of biases and \"copies\" (broadcasts) it down to match the 3 rows of your input batch.\n",
    "\n",
    "$$\n",
    "\\text{Output (3x3)} = \\text{Dot Product (3x3)} + \\text{Bias (1x3 } \\xrightarrow{\\text{broadcast}} \\text{ 3x3)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a63f1-6094-4624-b1cb-8e4433ce23bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8d85a17-02a2-4336-ad22-580ae4b25847",
   "metadata": {},
   "source": [
    "## **The Switch (Activation Functions)**\n",
    "### 1. The Problem: The \"Linearity Trap\"\n",
    "\n",
    "Right now, your neural network is just doing this:\n",
    "$$Output = \\text{Input} \\times \\text{Weight} + \\text{Bias}$$\n",
    "\n",
    "This is a **Linear Equation** (like $y = mx + c$). It draws a straight line.\n",
    "\n",
    "**The Fatal Flaw:**\n",
    "If you stack 100 of these layers on top of each other, mathematics plays a trick on you.\n",
    "* Layer 1: Multiply by 2.\n",
    "* Layer 2: Multiply by 3.\n",
    "* **Result:** You just multiplied by 6.\n",
    "\n",
    "No matter how deep you make the network, if it's only linear layers, it collapses into **one single linear layer**. It can only draw straight lines. It cannot learn curves, shapes, or complex boundaries (like distinguishing a cat from a dog).\n",
    "\n",
    "\n",
    "\n",
    "### 2. The Solution: The \"Non-Linear\" Switch\n",
    "\n",
    "To fix this, we insert a \"Switch\" (Activation Function) after every linear layer.\n",
    "\n",
    "This switch adds a \"bend\" or a \"kink\" to the math. It allows the neural network to warp space and draw complex shapes.\n",
    "\n",
    "### 3. Meet ReLU (Rectified Linear Unit)\n",
    "\n",
    "In modern AI (including the LLMs we use today), the most popular switch is **ReLU**.\n",
    "\n",
    "Despite the complex name, it is shockingly simple. It mimics a biological neuron:\n",
    "* **Biological Neuron:** Does it receive enough electrical signal?\n",
    "    * **No:** Stay silent.\n",
    "    * **Yes:** Fire!\n",
    "* **ReLU:** Is the number positive?\n",
    "    * **No (Negative):** Return 0 (Silence).\n",
    "    * **Yes (Positive):** Return the number (Fire).\n",
    "\n",
    "**The Graph:**\n",
    "\n",
    "\n",
    "Notice the shape? It's not a straight line anymore. It's a \"hinge.\" By combining millions of these hinges, a neural network can approximate *any* shape in the universe.\n",
    "\n",
    "### 4. Applying it to your Matrix\n",
    "\n",
    "Let's look at the `Output` matrix you calculated earlier:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1.12 & 22.16 & \\mathbf{-1.29} \\\\\n",
    "1.48 & 1.80 & 3.255 \\\\\n",
    "\\mathbf{-0.4} & 11.2 & 0.40\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "I've highlighted the **negative numbers**. In a biological sense, these neurons are receiving \"inhibitory\" signals. They shouldn't be firing.\n",
    "\n",
    "**The ReLU Operation:**\n",
    "\n",
    "It sounds fancy, but it is the simplest function in Deep Learning:\n",
    "$$f(x) = \\max(0, x)$$\n",
    "\n",
    "* If $x > 0$, keep $x$.\n",
    "* If $x \\le 0$, make it $0$.\n",
    "* \n",
    "We pass every single number through $max(0, x)$.\n",
    "\n",
    "1.  Take `22.16`. Is it $> 0$? **Yes.** Keep it: `22.16`.\n",
    "2.  Take `-1.29`. Is it $> 0$? **No.** Kill it: `0`.\n",
    "\n",
    "**Your Final Task for Week 1:**\n",
    "\n",
    "Take your previous `output` matrix (the 3x3 one).\n",
    "Write a single line of NumPy code to apply the ReLU activation to it.\n",
    "\n",
    "1.  What happens to the negative values (like `-1.29` and `-0.4`)?\n",
    "2.  Print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44383f3d-11b7-4391-93f2-e8d7ea624691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.12 , 22.16 ,  0.   ],\n",
       "       [ 1.48 ,  1.8  ,  3.255],\n",
       "       [ 0.   , 11.2  ,  0.4  ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(0, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d55f9-31f5-4526-9e58-7d228a8d656f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13606dfb-5c35-4792-bd5d-1fd4ede089cb",
   "metadata": {},
   "source": [
    "### üìù The Test Specification: \"The 3-Layer Mini-Network\"\n",
    "\n",
    "Instead of just one layer, I want you to build a small \"feed-forward\" pass through a network with **3 distinct layers**. This mimics a real deep learning structure.\n",
    "\n",
    "**The Architecture:**\n",
    "1.  **Input Data ($X$):** Batch size of `4`, with `5` features per sample. (Shape: `4x5`)\n",
    "2.  **Layer 1 (Hidden):** `10` neurons. Activation: **ReLU**.\n",
    "3.  **Layer 2 (Hidden):** `5` neurons. Activation: **ReLU**.\n",
    "4.  **Layer 3 (Output):** `2` neurons. Activation: **None** (Linear output, often called \"logits\").\n",
    "\n",
    "**Requirements:**\n",
    "1.  **Random Initialization:** Initialize weights ($W$) and biases ($b$) for all 3 layers using `np.random.randn`.\n",
    "2.  **The Forward Function:** Write a function `forward_pass(X, weights, biases)` (or similar structure) that takes the input and passes it through all three layers.\n",
    "3.  **Shapes Matter:** Ensure the matrix multiplication shapes align correctly at every step.\n",
    "4.  **Output:** Print the final output matrix and its shape.\n",
    "\n",
    "**Expected Final Shape:**\n",
    "Since you have a batch of `4` and the last layer has `2` neurons, your final output *must* be `(4, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7e01bbbe-6179-4423-bbe5-7c4828278c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [1, 5, 1, 2, 6],\n",
    "    [2, 2, 3, 9, 0],\n",
    "    [-1, 6, 3, -5, 9],\n",
    "    [4, 6, 8, 0, -3]\n",
    "])\n",
    "## Function, direct from theory\n",
    "def forward_pass(X, weights, biases):\n",
    "    dot = np.dot(X, weights) + biases\n",
    "    return np.maximum(0, dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c53c4799-31d8-453c-a7dc-bfc346234024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 5) (5, 10)\n",
      "(4, 10) \n",
      " [[ 0.          3.72950843  1.30907066 13.16673361  3.37015803  2.11177928\n",
      "   2.75738917  0.62562173  2.95360309  6.8628299 ]\n",
      " [ 0.          4.67728786  0.23982357  5.16344835 12.7527942   5.15526872\n",
      "   0.          6.66838567 11.18854154 22.26352515]\n",
      " [ 0.          7.31886963  4.33816347 13.28614583  0.          0.\n",
      "   6.72953664  0.          0.          0.        ]\n",
      " [10.03889561  6.0929095  10.80423429  2.02475444  8.37290042  0.\n",
      "   8.80705251  0.          0.          7.73938613]]\n"
     ]
    }
   ],
   "source": [
    "# ## L1 - 10 neurons, so bias = 10\n",
    "w = np.random.randn(5, 10) ## Some random array as per requirements, which can be multiplied with a 4x5 matrix\n",
    "b = np.random.randn(1, 10)\n",
    "L1 = forward_pass(x, W, b)\n",
    "print(np.shape(x), np.shape(W))\n",
    "print(np.shape(L1), \"\\n\", L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e916cafd-7a35-4a8f-9d05-5f636ec0bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10) (10, 5)\n",
      "(4, 5) \n",
      " [[ 21.37681674   0.           0.           0.          37.59631992]\n",
      " [ 52.62467193   8.19712596   0.           5.71111985 100.99535762]\n",
      " [  1.04399736   1.37717937   0.           0.           9.97131074]\n",
      " [ 10.48526949  27.47192144   0.           0.          53.5324209 ]]\n"
     ]
    }
   ],
   "source": [
    "# ## L2 - 5 neurons, so bias = 5\n",
    "W = np.random.randn(10, 5) ## Should be multipliable with a 4x10 matrix\n",
    "b = np.random.randn(1, 5)\n",
    "L2 = forward_pass(L1, W, b)\n",
    "print(np.shape(L1), np.shape(W))\n",
    "print(np.shape(L2), \"\\n\", L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db1c07c8-251c-4882-8f0b-e01c1bd19855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (5, 2)\n",
      "(4, 2) \n",
      " [[ -59.48792607  -17.02371587]\n",
      " [-161.55987726  -34.33465641]\n",
      " [ -15.74368026   -5.15299628]\n",
      " [ -85.4833848    -7.70420187]]\n"
     ]
    }
   ],
   "source": [
    "# ## L3 - 2 neurons, so bias = 2\n",
    "W = np.random.randn(5, 2)\n",
    "b = np.random.randn(1, 2)\n",
    "L3 = np.dot(L2, W) + b ## We do not use forward_pass(L2, W, b) as the activation in input is None\n",
    "print(np.shape(L3), np.shape(W))\n",
    "print(np.shape(L3), \"\\n\", L3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffda51f-c36b-4091-bc15-5b11a6654f40",
   "metadata": {},
   "source": [
    "This is **excellent**.\n",
    "\n",
    "You correctly identified the shape transformations, handled the data flow from one layer to the next, and‚Äîcrucially‚Äîyou spotted the requirement for **Layer 3** (no activation) and wrote custom code for it instead of using your function.\n",
    "\n",
    "### Code Review: 9/10\n",
    "\n",
    "You have passed the test. The logic is flawless. I have one minor \"clean code\" observation and one optimization tip for your future career.\n",
    "\n",
    "**Optimization: The \"Class\" Structure**\n",
    "Right now, you are managing `W1`, `b1`, `W2`, `b2`, etc., manually. As networks get deeper, this becomes a nightmare. This is why libraries like PyTorch use **Classes**.\n",
    "\n",
    "*(No need to code this now, just read it to see where we are heading in Week 3)*:\n",
    "\n",
    "```python\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.W = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.b = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return np.dot(inputs, self.W) + self.b\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### üèÅ Week 1 Debrief & Next Step\n",
    "\n",
    "You have successfully:\n",
    "\n",
    "1.  Understood the Dot Product & Broadcasting.\n",
    "2.  Implemented a Deep Neural Network forward pass from scratch.\n",
    "3.  Debugged dimension mismatches (the \\#1 error in Deep Learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fd9a6a-9d61-4211-98cb-8c76d157d724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
