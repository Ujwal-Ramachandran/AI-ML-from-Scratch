{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a833bf-e511-4230-b619-c8e286091b6a",
   "metadata": {},
   "source": [
    "# **PyTorch Fundamentals**\n",
    "\n",
    "\n",
    "You are transitioning from manual gradient calculations to using a framework that handles the heavy lifting, allowing you to focus on architecture and data.\n",
    "\n",
    "Here is the breakdown for **L5: PyTorch Fundamentals**.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "I have structured this to ensure you understand the \"PyTorch Way\" of doing things before we assemble the full MNIST classifier.\n",
    "\n",
    "```text\n",
    "L5: PyTorch Fundamentals\n",
    "├── Concept 1: Tensors & Device Management\n",
    "│   ├── Tensor creation and operations (vs NumPy)\n",
    "│   ├── GPU/CUDA context (Hardware Check)\n",
    "│   ├── Purpose: The fundamental data structure of deep learning\n",
    "│   ├── Simple terms: N-dimensional arrays that live on video cards\n",
    "│   └── Task: Create tensors, perform math, and move them to your RTX 4060\n",
    "│\n",
    "├── Concept 2: Autograd (Automatic Differentiation)\n",
    "│   ├── Computational Graphs (DAGs)\n",
    "│   ├── .requires_grad and .backward()\n",
    "│   ├── Purpose: Automating the chain rule\n",
    "│   ├── Simple terms: The engine that remembers your math to calculate gradients later\n",
    "│   └── Task: Manually compute gradients for a simple equation using Autograd\n",
    "│\n",
    "├── Concept 3: Data Handling (Dataset & DataLoader)\n",
    "│   ├── torch.utils.data.Dataset (Custom Class structure)\n",
    "│   ├── torch.utils.data.DataLoader (Batching, Shuffling)\n",
    "│   ├── Purpose: Decoupling data loading from training logic\n",
    "│   ├── Simple terms: An organized conveyor belt feeding data to your model\n",
    "│   └── Task: Implement a dummy Custom Dataset and iterate through it\n",
    "│\n",
    "├── Concept 4: Model Architecture (nn.Module)\n",
    "│   ├── nn.Module class structure (__init__, forward)\n",
    "│   ├── nn.Sequential (Container)\n",
    "│   ├── Purpose: Encapsulating state (weights) and behavior (forward pass)\n",
    "│   ├── Simple terms: Blueprints for your neural network layers\n",
    "│   └── Task: Define a simple Multilayer Perceptron (MLP) for MNIST input\n",
    "│\n",
    "├── Concept 5: Loss & Optimization (Implicit Prerequisite)\n",
    "│   ├── torch.nn Loss functions (CrossEntropyLoss)\n",
    "│   ├── torch.optim (SGD/Adam)\n",
    "│   ├── Purpose: Measuring error and updating weights\n",
    "│   ├── Simple terms: The scoreboard (loss) and the coach (optimizer) correcting the players\n",
    "│   └── Task: Initialize loss and optimizer for the model\n",
    "│\n",
    "└── Concept 6: The Training Loop (The \"Build\")\n",
    "    ├── The Standard Cycle: Forward → Loss → Backward → Step → Zero Grad\n",
    "    ├── Purpose: Orchestrating the learning process\n",
    "    ├── Simple terms: The actual practice session where learning happens\n",
    "    └── Mini-Project: Complete MNIST Digit Classifier Training\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a77c0-9fe9-4e50-8234-467db521b79f",
   "metadata": {},
   "source": [
    "## **Concept 1: Tensors & Device Management**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the world of PyTorch, the **Tensor** is the primary citizen. While it looks and behaves almost exactly like a NumPy array (n-dimensional grid of numbers), it has two superpowers that NumPy lacks:\n",
    "   1. **GPU Acceleration:** Tensors can live on the GPU (VRAM) rather than just the CPU (RAM). This allows for massive parallel processing, which is critical for deep learning.\n",
    "   2. **Autograd Compatibility:** Tensors can track the history of operations performed on them to automatically calculate gradients later (we will cover this in Concept 2).\n",
    "\n",
    "Think of a CPU as a Professor: extremely smart, capable of complex logic, but can only do one or two things at a time. Think of a GPU as an army of minions: individually simple, but there are thousands of them working exactly in sync. Deep learning is mostly multiplying huge matrices, a task perfectly suited for the army.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "   * **Creation:** You can create tensors directly (`torch.tensor([1, 2])`) or convert from NumPy (`torch.from_numpy(arr)`).\n",
    "   * **Device Management:** Every tensor has a `.device` property. By default, they are created on the `'cpu'`.\n",
    "   * **Moving Data:** You cannot perform operations (like addition or multiplication) between a tensor on the CPU and a tensor on the GPU. They must be on the same device. You move them using `.to(device)` or `.cuda()`.\n",
    "   * **Asynchronous Execution:** CUDA (GPU) operations are asynchronous. When Python tells the GPU to \"multiply these matrices,\" the GPU says \"Okay, I'll get to it\" and Python immediately moves to the next line of code. If you want to time GPU operations accurately, you must force Python to wait until the GPU is finished using a synchronization command.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "A Tensor is just a container for numbers. If the container is in System RAM, the CPU does the math. If you move the container to Video RAM (VRAM), the graphics card does the math—much faster.\n",
    "\n",
    "### Trade-offs & Pitfalls\n",
    "\n",
    "   * **Overhead:** Moving data between CPU and GPU is slow (it travels over the PCIe bus). For small operations (like adding two numbers), the transfer time takes longer than the actual math. GPU is only worth it for *large* matrix operations.\n",
    "   * **VRAM Limits:** If you load too many huge tensors, you will get a \"CUDA Out of Memory\" error.\n",
    "\n",
    "\n",
    "### PyTorch Prerequisites: The Syntax Toolkit\n",
    "\n",
    "PyTorch is designed to look and feel like NumPy, but with extra commands for the GPU. Here are the specific tools you need for this task.\n",
    "\n",
    "#### 1. Checking Hardware\n",
    "\n",
    "To see if your GPU is accessible, PyTorch provides a boolean check:\n",
    "\n",
    "```python\n",
    "# Returns True if GPU is ready, False otherwise\n",
    "status = torch.cuda.is_available()\n",
    "\n",
    "```\n",
    "#### **What is CUDA?**\n",
    "\n",
    "**CUDA** (Compute Unified Device Architecture) is a software layer created by **NVIDIA**.\n",
    "\n",
    "Think of it as a translator.\n",
    "   * **You (Python/PyTorch):** Speak high-level code instructions (\"Multiply this matrix\").\n",
    "   * **Your GPU:** Speaks low-level hardware voltage signals.\n",
    "\n",
    "Without CUDA, your Python code has no way to talk to the graphics card. The GPU is just a rock that draws pixels on your screen.\n",
    "\n",
    "**CUDA** provides the bridge. It allows developers to send general-purpose math problems (not just graphics) to the GPU to be solved.\n",
    "\n",
    "#### **Why do we check for it?**\n",
    "\n",
    "In PyTorch, `torch.cuda.is_available()` is effectively asking:\n",
    "   1. Do you have an NVIDIA GPU?\n",
    "   2. Do you have the correct drivers installed?\n",
    "   3. Can PyTorch \"see\" and talk to that GPU?\n",
    "\n",
    "If the answer is `True`, you unlock the ability to train models 50x to 100x faster than on your CPU. If not try:\n",
    "\n",
    "   `pip uninstall torch torchvision torchaudio`\n",
    "\n",
    "   `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`\n",
    "\n",
    "#### 2. Creating Tensors\n",
    "\n",
    "The syntax is nearly identical to NumPy.\n",
    "   * **NumPy:** `np.random.rand(rows, cols)`\n",
    "   * **PyTorch:** `torch.rand(rows, cols)` (Creates a tensor with random numbers between 0 and 1)\n",
    "\n",
    "#### 3. Moving Data (The `.to()` method)\n",
    "\n",
    "When a tensor is created, it sits in CPU RAM by default. You move it using `.to()` or specific device methods.\n",
    "\n",
    "```python\n",
    "# Create on CPU\n",
    "x = torch.rand(100, 100)\n",
    "\n",
    "# Move to GPU (returns a NEW copy on the device)\n",
    "x_gpu = x.to('cuda')\n",
    "\n",
    "```\n",
    "\n",
    "*Note: You can also use string variables: `device = 'cuda' if torch.cuda.is_available() else 'cpu'`, then `x.to(device)`.*\n",
    "\n",
    "#### 4. Matrix Operations\n",
    "\n",
    "* **Multiplication:** You can use the standard `@` symbol for matrix multiplication, just like in modern NumPy.\n",
    "   * `result = tensor_a @ tensor_b`\n",
    "   * *Constraint:* Both `tensor_a` and `tensor_b` must be on the **same device**. If one is on CPU and one is on GPU, it will crash.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. Timing GPU Operations (Crucial)\n",
    "\n",
    "Because Python sends commands to the GPU asynchronously (fire-and-forget), Python might finish its code (stop the timer) while the GPU is still crunching numbers in the background.\n",
    "To get an accurate time, you must force Python to wait:\n",
    "\n",
    "```python\n",
    "# Start Timer\n",
    "# ... do gpu operations ...\n",
    "torch.cuda.synchronize() # Wait for all GPU kernels to finish\n",
    "# Stop Timer\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Now that you have the tools, let's attempt the logic.\n",
    "\n",
    "**Goal:** Prove the speed difference between CPU and GPU on your machine.\n",
    "\n",
    "**Steps to Implement:**\n",
    "\n",
    "1. Import `torch` and `time`.\n",
    "2. Define a `device` variable (use the check from tool #1). Print what device you got.\n",
    "3. Create two large random tensors (e.g., shape `10000, 10000`) on the CPU.\n",
    "4. **CPU Benchmark:**\n",
    "   * Record `start_time`.\n",
    "   * Perform `matrix1 @ matrix2`.\n",
    "   * Record `end_time` and print the duration.\n",
    "\n",
    "5. **GPU Benchmark:**\n",
    "   * Move both tensors to the GPU using `.to('cuda')`.\n",
    "   * **Warm-up:** Run the multiplication once without timing it (GPUs take a moment to \"wake up\").\n",
    "   * Record `start_time`.\n",
    "   * Perform multiplication.\n",
    "   * **Synchronize:** Call `torch.cuda.synchronize()`.\n",
    "   * Record `end_time` and print duration.\n",
    "\n",
    "**Inputs:** None (Generative)\n",
    "\n",
    "**Outputs:** Device name, CPU time, GPU time, Speedup factor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a1ae62-c307-4216-90cd-d5b7a521b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"Creating torches\")\n",
    "x_cpu = torch.rand(10000, 10000)\n",
    "y_cpu = torch.rand(10000, 10000)\n",
    "\n",
    "print(\"===========\\nStart Comparision\")\n",
    "start_time = time.time()\n",
    "_ = x_cpu @ y_cpu\n",
    "end_time = time.time()\n",
    "print(f\"Time Taken for CPU = {end_time - start_time} sec\")\n",
    "\n",
    "x_gpu = x_cpu.to(\"cuda\")\n",
    "y_gpu = y_cpu.to(\"cuda\")\n",
    "\n",
    "print(\"Start Synchronization\") # Wait for everything before starting\n",
    "torch.cuda.synchronize()\n",
    "start_time = time.time()\n",
    "_ = x_gpu @ y_gpu\n",
    "torch.cuda.synchronize() # <--- CRITICAL: Wait for GPU to finish!\n",
    "end_time = time.time()\n",
    "print(f\"Time Taken for GPU = {end_time - start_time} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698048f-86b9-4d17-b411-579206683263",
   "metadata": {},
   "source": [
    "\n",
    "**16.42s vs 0.25s.**\n",
    "Your GPU is approximately **65x faster** than your CPU for this task. This is why we use GPUs.\n",
    "\n",
    "Now that we know how to hold data (Tensors) and where to put it (GPU), we need to discuss how PyTorch learns.\n",
    "\n",
    "---\n",
    "\n",
    "## Concept 2: Autograd (Automatic Differentiation)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the \"From Scratch\" module, you had to manually derive the gradients (derivatives) for every function using Calculus. If you changed your model architecture, you had to re-do all the calculus on paper and re-write the backward pass code.\n",
    "\n",
    "**Autograd** removes this burden. It is an engine that records every operation you perform on a tensor. When you are done, you press a \"rewind\" button, and it automatically calculates the gradients for you using the Chain Rule.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "1. **The Tape Recorder:** When you create a tensor with `requires_grad=True`, PyTorch starts a log.\n",
    "   * If you do `y = x + 2`, PyTorch remembers: \"To get $y$, we took $x$ and added 2.\"\n",
    "   * It builds a **Computational Graph** (a Directed Acyclic Graph) connecting inputs to outputs.\n",
    "\n",
    "\n",
    "2. **The Backward Pass:** When you call `.backward()` on the final result (usually the Loss), PyTorch walks backward through this graph.\n",
    "3. **The `.grad` attribute:** The calculated derivatives are stored in the `.grad` property of the input tensors.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine you are walking through a maze (the forward pass). You leave a trail of breadcrumbs behind you. When you reach the end (the loss), you follow the breadcrumbs back to the start (backward pass) to figure out which turns led you to the mistake.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Memory:** Storing this graph takes memory. This is why we turn it off (`torch.no_grad()`) when we are just testing/evaluating the model, to save RAM.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "We will verify Autograd against your own calculus knowledge.\n",
    "\n",
    "**Target Equation:**\n",
    "$$y = x^3 + 5$$\n",
    "\n",
    "**Analytical Derivative (Calculus):**\n",
    "$$\\frac{dy}{dx} = 3x^2$$\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Create a tensor `x` with the value `4.0`. **Important:** You must set `requires_grad=True`.\n",
    "2. Calculate `y = x**3 + 5`.\n",
    "3. Print the value of `y`.\n",
    "4. Call `y.backward()`.\n",
    "5. Print the value of `x.grad`.\n",
    "6. Manually calculate $3(4)^2$ in Python and print it to prove they match.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedcbb8d-7c42-49c1-b840-39fa34c79943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad = True)\n",
    "y = x ** 3 + 5\n",
    "print(f\"Y = {y}\")\n",
    "y.backward()\n",
    "print(f\"X Grad = {x.grad}\")\n",
    "print(f\"3(4)**2 = {3*(4**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6823f34e-92f3-46b5-9cd6-8b460b168dd9",
   "metadata": {},
   "source": [
    "Perfect. You just proved that PyTorch can perform Calculus for you.\n",
    "\n",
    "Now we move to how we feed data into these systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 3: Data Handling (Dataset & DataLoader)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In the \"From Scratch\" module, you likely loaded all your data into one giant NumPy array (e.g., `X_train`).\n",
    "**Problem:** What if your dataset is 500GB of images? You can't load that into RAM.\n",
    "**Solution:** The **Dataset** class. It acts like a librarian. It doesn't hold all the books (data) in its hands; it just knows *where* they are on the shelf and how to fetch *one* when asked.\n",
    "\n",
    "The **DataLoader** is the delivery truck. It asks the Librarian for 32 books (a batch), packs them into a box, and delivers them to the GPU.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "To manage data in PyTorch, you almost always create a custom class that inherits from `torch.utils.data.Dataset`.\n",
    "\n",
    "You **must** implement three magic methods:\n",
    "   1. `__init__`: Setup (load file paths, CSVs, etc.).\n",
    "   2. `__len__`: Returns the total number of samples.\n",
    "   3. `__getitem__(index)`: Returns **one specific sample** at the given `index`.\n",
    "\n",
    "### Syntax Toolkit\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the item at index 'idx'\n",
    "        sample = self.data[idx]\n",
    "        return sample\n",
    "\n",
    "```\n",
    "\n",
    "Once the class is defined, you hand it to the loader:\n",
    "\n",
    "```python\n",
    "# batch_size=4 means it groups 4 samples into one big tensor\n",
    "loader = DataLoader(dataset_instance, batch_size=4, shuffle=True)\n",
    "\n",
    "```\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Create a \"Lazy\" dataset that generates random numbers on the fly.\n",
    "\n",
    "**Requirements:**\n",
    "1. Define a class `RandomDataset` inheriting from `Dataset`.\n",
    "   * `__init__`: Accepts an integer `length`. Stores it.\n",
    "   * `__len__`: Returns the `length`.\n",
    "   * `__getitem__`: Ignores the actual `idx`. Instead, it generates a random tensor of shape `(3,)` (the feature) and a random integer (0 or 1) (the label). It returns a tuple: `(feature, label)`.\n",
    "\n",
    "2. Instantiate the dataset with a length of **10**.\n",
    "3. Wrap it in a `DataLoader` with `batch_size=4`.\n",
    "4. Write a loop to iterate through the loader.\n",
    "5. Inside the loop, print the shape of the features batch and the labels batch.\n",
    "\n",
    "**Expected Output intuition:**\n",
    "If batch size is 4 and length is 10, you should see 3 loops.\n",
    "\n",
    "   * Loop 1: 4 items\n",
    "   * Loop 2: 4 items\n",
    "   * Loop 3: 2 items (the remainder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496841b-bf6f-404d-a7e8-3c7bf9b70a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.rand(3)\n",
    "        ran_in = random.randint(0,1)\n",
    "        return (tensor, ran_in)\n",
    "\n",
    "\n",
    "dataset_instance = RandomDataset(10)\n",
    "loader = DataLoader(dataset_instance, batch_size=4, shuffle=True)\n",
    "\n",
    "for i, (f, l) in enumerate(loader):\n",
    "    print(f\"For Interation {i+1}:\\nFeatures = {f}\\nLabels = {l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2caee7-41f9-4fcb-b963-938fa1d84b6a",
   "metadata": {},
   "source": [
    "Excellent. You can see how the `DataLoader` automatically stacked your individual tensors into batches (e.g., `[4, 3]` shape). This automation is what makes training on millions of images possible.\n",
    "\n",
    "Now, we need something to consume that data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 4: Model Architecture (nn.Module)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In PyTorch, every neural network is a Python class that inherits from `nn.Module`.\n",
    "Think of this class as a blueprint.\n",
    "   1. **`__init__` (The Inventory):** You list the parts you need (layers). \"I need a Linear layer with 784 inputs, a ReLU activation, etc.\"\n",
    "   2. **`forward` (The Assembly):** You define how data flows through those parts. \"Take input `x`, pass it through layer 1, then apply ReLU, then layer 2.\"\n",
    "This separation allows for complex, non-linear flows (like skipping layers) that you'll need for things like ResNet later.\n",
    "\n",
    "### Mechanics (The Syntax Toolkit)\n",
    "\n",
    "You need three main components for a basic network:\n",
    "   1. **`nn.Flatten()`:** Images are 2D grids (e.g., 28x28). Dense layers (`nn.Linear`) only understand flat lists (vectors). This layer squashes the grid into a single line (28*28 = 784).\n",
    "   2. **`nn.Linear(in_features, out_features)`:** This is the standard \"Dense\" layer you built from scratch in Module 1 ($y = xW^T + b$).\n",
    "   3. **`nn.ReLU()`:** The activation function.\n",
    "\n",
    "**Class Structure:**\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define layers here\n",
    "        self.layer1 = nn.Linear(10, 5)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define flow here\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "```\n",
    "\n",
    "`super().__init__()` is often blindly added to the code, but it serves a critical purpose.\n",
    "\n",
    "### The Short Answer\n",
    "\n",
    "`nn.Module` is a complex parent class that does a lot of hidden \"magic\" behind the scenes. If you don't call `super().__init__()`, that magic is never turned on, and your model will be broken.\n",
    "\n",
    "### The \"Magic\" (Why you need it)\n",
    "\n",
    "When you assign a layer like `self.layer1 = nn.Linear(...)`, PyTorch automatically detects it and adds its weights to a master list of **trainable parameters**.\n",
    "\n",
    "The `nn.Module` parent class has an internal dictionary (think of it as a registry) to track these parameters.\n",
    "\n",
    "* **With `super().__init__()`:** The registry is created. When you add a layer, PyTorch registers it.\n",
    "* **Without it:** The registry doesn't exist. You will likely get an `AttributeError` saying `_modules` or `_parameters` is missing, or your optimizer won't find any weights to update.\n",
    "\n",
    "### Intuitive Metaphor\n",
    "\n",
    "Imagine `nn.Module` is a general **Building Contractor** and your class `MNISTClassifier` is a specific **House Design**.\n",
    "   * The Contractor knows how to install plumbing, electricity, and the foundation.\n",
    "   * Your Design specifies where the walls and windows go.\n",
    "\n",
    "Calling `super().__init__()` is like paying the Contractor to pour the foundation and hook up the power **before** you start putting up your specific walls. If you skip it, you are building walls on dirt with no electricity nothing will work.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Sequential vs. Class:** You *can* use `nn.Sequential` (a list of layers) for simple stacks, but the **Class** structure is mandatory for advanced architectures (Transformers, ResNets). We will stick to the Class structure to build good habits.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Build a Multi-Layer Perceptron (MLP) designed for the MNIST dataset.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "1. Create a class `MNISTClassifier` inheriting from `nn.Module`.\n",
    "2. **`__init__`:**\n",
    "   * Define a `flatten` layer.\n",
    "   * Define a first linear layer: Input **784** (28x28 pixels), Output **128**.\n",
    "   * Define a ReLU activation.\n",
    "   * Define a second linear layer (Output Layer): Input **128**, Output **10** (for digits 0-9).\n",
    "\n",
    "\n",
    "3. **`forward`:** Connect them in order: Flatten $\\to$ Linear1 $\\to$ ReLU $\\to$ Linear2.\n",
    "4. **Test it:**\n",
    "   * Instantiate the model.\n",
    "   * Move it to the GPU (`.to(device)`).\n",
    "   * Create a dummy input tensor representing **one batch of 8 images** (Shape: `(8, 28, 28)`). **Important:** Don't forget to move this input to the GPU too!\n",
    "   * Pass the input through the model.\n",
    "   * Print the output shape.\n",
    "\n",
    "\n",
    "\n",
    "**Expected Output Shape:** `torch.Size([8, 10])` (8 images, 10 class probabilities each).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8554d8-b8a9-4057-973e-3ace6f1ca653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer1 = nn.Linear(28*28, 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = MNISTClassifier().to(device)\n",
    "    model_input = torch.rand(8, 28, 28).to(device)\n",
    "    output = model(model_input)\n",
    "    print(output.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe88915-5e27-471e-9de2-a7b196f72a02",
   "metadata": {},
   "source": [
    "Perfect. You have a working brain (the Model) and a working body (the Hardware). Now we need to teach it how to learn.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5: Loss & Optimization**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "To learn, a model needs two things:\n",
    "   1. **A Scoreboard (Loss Function):** Tells the model how terrible its guess was.\n",
    "   2. **A Coach (Optimizer):** Looks at the mistakes (gradients) and updates the weights to do better next time.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "#### 1. The Loss Function: `nn.CrossEntropyLoss`\n",
    "\n",
    "For multi-class classification (like 10 digits), this is the standard choice.\n",
    "   * **Important:** PyTorch's `CrossEntropyLoss` expects **raw logits** (the raw numbers coming out of your final Linear layer). It applies Softmax **internally**.\n",
    "   * **Pitfall:** If you add `nn.Softmax` to the end of your model class and *then* use CrossEntropyLoss, you are applying Softmax twice. This destroys training. Your model correctly ends with `Linear`, so you are safe.\n",
    "\n",
    "> \"Is it like actual - guessed value?\"\n",
    "\n",
    "For **Regression** (predicting a price like $50.50), yes! We use Mean Squared Error (`(actual - guess)^2`).\n",
    "\n",
    "For **Classification** (predicting \"Is this a Cat or Dog?\"), it is different.\n",
    "You cannot subtract \"Cat\" from \"Dog\".\n",
    "Instead, the model outputs probabilities: `[0.9 Dog, 0.1 Cat]`.\n",
    "If the actual answer is \"Cat\", the **CrossEntropyLoss** punishes the model heavily for being 90% confident in the wrong answer.\n",
    "   * If the model says 99% Dog and it is a Dog -> Low Loss (near 0).\n",
    "   * If the model says 99% Cat and it is a Dog -> Huge Loss (approaches infinity).\n",
    "\n",
    "#### 2. The Optimizer: `torch.optim`\n",
    "\n",
    "The optimizer holds the model's parameters and updates them.\n",
    "   * **SGD (Stochastic Gradient Descent):** The classic approach.\n",
    "   * **Adam:** A smarter, adaptive version of SGD. It's the default \"just make it work\" optimizer for most modern tasks.\n",
    "\n",
    "> \"Ideally it will eventually make the loss 0?\"\n",
    "\n",
    "In a perfect world, yes. In reality, a loss of exactly 0 usually means the model has memorized the data (Overfitting), which is bad. We aim for the **Global Minimum**—the lowest point on the error curve where the model still understands general patterns, not just memorized answers.\n",
    "\n",
    "The optimizer uses the gradients (calculated by `backward()`) to nudge the weights \"downhill\" towards that minimum.\n",
    "\n",
    "**Syntax Toolkit:**\n",
    "\n",
    "```python\n",
    "# 1. Select Loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Select Optimizer\n",
    "# You must pass the list of parameters it needs to manage\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "```\n",
    "\n",
    "> \"Does it import all parameters?\"\n",
    "\n",
    "Yes. Remember the `super().__init__()` magic? When you defined `self.layer1 = nn.Linear(...)`,  PyTorch automatically registered the weights ($W$) and biases ($b$) of that layer into a list.\n",
    "Calling `model.parameters()` simply grabs that list and hands it to the optimizer so it knows **which** variables it is allowed to change.\n",
    "\n",
    "> \"If I wanted a different optimizer... I just change it?\"\n",
    "\n",
    "Exactly.\n",
    "   * `torch.optim.Adam(model.parameters(), lr=0.001)`\n",
    "   * `torch.optim.SGD(model.parameters(), lr=0.01)`\n",
    "   * `torch.optim.RMSprop(model.parameters(), lr=0.01)`\n",
    "The syntax is identical.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Verify the \"Backward\" plumbing works by manually running one pass.\n",
    "\n",
    "**Requirements:**\n",
    "   1. Define the `loss_fn` (CrossEntropyLoss) and `optimizer` (Adam with learning rate `1e-3`).\n",
    "   2. Create a **target** tensor (the \"correct answers\"):\n",
    "      * Shape: `(8,)` (matching your batch size of 8).\n",
    "      * Values: Random integers between 0 and 9.\n",
    "      * Device: **Must be on GPU** (same as your output).\n",
    "   \n",
    "   3. Calculate the loss: `loss = loss_fn(output, target)`.\n",
    "   4. Print the loss value.\n",
    "   5. **Trigger Autograd:** Call `loss.backward()`.\n",
    "   6. **Verify Gradients:** Print the shape of the gradients for the final layer's weights: `model.layer2.weight.grad.shape`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3aadfd-7463-4f15-bec9-79551792a6bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2805752754211426\n",
      "Gradient shape for model.layer2.weight: torch.Size([10, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MNISTClassifier().to(device)\n",
    "model_inp = torch.rand(8, 28, 28).to(device)\n",
    "output = model(model_inp)\n",
    "\n",
    "target = torch.randint(0, 10, (8,)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "loss = loss_fn(output, target)\n",
    "print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "loss.backward()\n",
    "print(f\"Gradient shape for model.layer2.weight: {model.layer2.weight.grad.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c428586-f3b6-4ba0-8d9c-425e07cbaea5",
   "metadata": {},
   "source": [
    "## **Concept 6: The Training Loop**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "This is the heartbeat of Deep Learning. A \"Training Loop\" is just a standard Python loop that repeats the process you just performed (Forward $\\to$ Backward $\\to$ Update) thousands of times.\n",
    "\n",
    "The \"Mode Switch\": `model.train()` vs `model.eval()`\n",
    "\n",
    "Neural networks have two modes of existence: **Learning** and **Performing**.\n",
    "   *1. **`model.train()`:** Tells PyTorch \"We are learning right now.\"\n",
    "      * **Effect:** Enables layers that work differently during training, such as **Dropout** (randomly turning off neurons) and **Batch Normalization** (calculating statistics based on the current batch).\n",
    "   *2. **`model.eval()`:** Tells PyTorch \"We are testing/predicting.\"\n",
    "      * **Effect:** Disables Dropout (uses all neurons), locks Batch Normalization statistics.\n",
    "\n",
    "`MNISTClassifier` only uses `Linear` and `ReLU`. These two layers behave exactly the same way in both modes. But as soon as you add a `Dropout` layer (which we will later), omitting this line will destroy your model's performance. So we will not use `model.train()` here\n",
    "\n",
    "### Mechanics: The 5 Steps\n",
    "\n",
    "Inside the loop, you must perform these 5 steps in this **exact order** for every batch of data:\n",
    "   1. **Forward Pass:** Pass data through the model.\n",
    "         ```python\n",
    "         pred = model(X)\n",
    "         loss = loss_fn(pred, y)\n",
    "         \n",
    "         ```\n",
    "         * **Action:** The input `X` travels through the layers.\n",
    "         * **Memory:** PyTorch builds the **Computational Graph** on the fly. It stores the input values at every layer in memory (VRAM). It needs these \"activations\" later to calculate the derivative.\n",
    "         * **Result:** You get a loss value (e.g., 2.3). The graph is now fully built, connecting `loss` back to every weight in the model.  \n",
    "\n",
    "   2. **Calculate Loss:** Compare prediction vs. truth.\n",
    "      1. **The Inputs**\n",
    "      \n",
    "      The function `loss_fn(prediction, target)` takes two very different looking tensors:\n",
    "         * **`prediction` (from the model):**\n",
    "            * **Shape:** `[Batch_Size, 10]` (e.g., `[64, 10]`).\n",
    "            * **Content:** Raw scores (logits). For a single image, it might look like: `[ -2.0, 5.1, 0.1, ... ]`. The higher the number, the more the model believes that index is the correct digit.\n",
    "         * **`target` (from the dataset):**\n",
    "            * **Shape:** `[Batch_Size]` (e.g., `[64]`).\n",
    "            * **Content:** The correct integer indices. For that same image: `1` (meaning the digit \"1\").\n",
    "      \n",
    "      2. **The Internal Mechanics (CrossEntropyLoss)**\n",
    "         When you run `loss = loss_fn(prediction, target)`, PyTorch performs three mathematical operations instantly:\n",
    "            1. **Softmax:** It squashes the raw scores into probabilities (0.0 to 1.0) so they sum up to 100%.\n",
    "            2. **Log:** It takes the logarithm of those probabilities (to penalize wrong answers more heavily).\n",
    "            3. **Selection (NLL):** It looks at the **target** index. If the target is `1`, it grabs the probability the model assigned to index `1`.\n",
    "               * If the model assigned 0.9 (High confidence) $\\to$ **Low Loss**.\n",
    "               * If the model assigned 0.1 (Low confidence) $\\to$ **High Loss**.\n",
    "      \n",
    "      3. **The Output**\n",
    "      \n",
    "      The result is a **single scalar number** (a float), like `2.34`.\n",
    "         * This number represents the *average error* of the entire batch of 64 images.\n",
    "         * **Crucial:** This number acts as the \"root\" of the tree. When you call `.backward()` later, it starts here and spreads out to all 64 images' calculations.\n",
    "   \n",
    "   3. **Zero Gradients:** `optimizer.zero_grad()`\n",
    "      * **Why?** PyTorch accumulates gradients by default (adds them up). If you don't reset them to zero, the gradients from Batch 1 will be added to Batch 2, creating a mess.\n",
    "          ```python\n",
    "          optimizer.zero_grad()\n",
    "          \n",
    "          ```\n",
    "          * **The Weird Part:** PyTorch accumulates gradients. If you calculate gradients for Batch 1, they are stored in `.grad`. If you immediately run Batch 2 and calculate gradients, PyTorch **adds** the new gradients to the old ones.\n",
    "          * **Why?** This is useful for \"Gradient Accumulation\" (simulating a large batch size on small hardware), but fatal for standard training.\n",
    "          * **Action:** This command goes through every parameter in the model and sets `.grad = 0`. It wipes the slate clean for the current batch.\n",
    "   \n",
    "   4. **Backward Pass:** `loss.backward()` (Calculate gradients).\n",
    "         ```python\n",
    "         loss.backward()\n",
    "         \n",
    "         ```\n",
    "         * **Action:** This is the \"Chain Rule\" engine. It starts at the `loss` and walks backward along the graph we built in Step A.\n",
    "         * **Math:** For every weight $w$, it computes $\\frac{\\partial Loss}{\\partial w}$ (how much the error changes if we wiggle this weight).\n",
    "         * **Result:** It fills the `.grad` attribute of every parameter with a specific number (a float).\n",
    "         * **Memory:** Once this is done, the graph (and the saved activations from Step A) are usually freed to save memory.   \n",
    "   \n",
    "   5. **Optimizer Step:** `optimizer.step()` (Update weights).\n",
    "        ```python\n",
    "        optimizer.step()\n",
    "        \n",
    "        ```\n",
    "        * **Action:** The optimizer looks at the `.data` (the weight value) and the `.grad` (the calculated error slope) for every parameter.\n",
    "        * **Math (SGD example):** It applies the update rule:\n",
    "          $$w_{new} = w_{old} - (\\text{learning\\_rate} \\times \\text{gradient})$$\n",
    "        * **Result:** The model is now slightly smarter than it was a millisecond ago.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine painting a portrait.\n",
    "\n",
    "1. Paint a stroke (Forward).\n",
    "2. Compare it to the subject (Loss).\n",
    "3. **Wipe your brush** (Zero Grad). *If you don't, the old paint mixes with the new paint!*\n",
    "4. Decide how to fix the stroke (Backward).\n",
    "5. Move your hand to correct it (Step).\n",
    "\n",
    "---\n",
    "\n",
    "## Mini-Project: Fake-MNIST Training Loop\n",
    "\n",
    "We will put everything together. Since downloading the real MNIST dataset can be messy with file permissions, we will update your `RandomDataset` to generate \"Fake MNIST\" data (random noise images) to prove the loop works.\n",
    "\n",
    "**Objective:** Train your model on generated data for 3 epochs.\n",
    "\n",
    "### Specifications\n",
    "\n",
    "**1. Data Preparation**\n",
    "   * Redefine `RandomDataset`:\n",
    "      * `__getitem__` must now return a tuple: `(image, label)`.\n",
    "      * `image`: Random tensor of shape `(28, 28)` (representing a 28x28 pixel image).\n",
    "      * `label`: Random integer `0-9`.\n",
    "   * Create a `DataLoader` with `batch_size=64` and `length=1000`.\n",
    "\n",
    "**2. Model & Setup**\n",
    "   * Instantiate `MNISTClassifier`.\n",
    "   * Move it to the **GPU**.\n",
    "   * Define `CrossEntropyLoss` and `Adam` optimizer (lr=0.001).\n",
    "\n",
    "**3. The Training Loop**\n",
    "   * Write a loop for `3` Epochs.\n",
    "   * Inside, iterate through the `DataLoader`.\n",
    "   * **Crucial:** Move `images` and `labels` to the GPU *inside* the loop.\n",
    "   * Perform the 5 standard steps (Forward, Loss, Zero, Backward, Step).\n",
    "   * **Logging:** Print the loss value **every 100 batches** (e.g., \"Epoch 1, Batch 100, Loss: 2.30\").\n",
    "\n",
    "**Forbidden:** Do not use `torchvision` or `sklearn`. Use only the tools we discussed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d1d3098-68a6-45d6-b063-a4c6d7af459c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 5, Loss: 2.3696\n",
      "Epoch 1, Batch 10, Loss: 2.3022\n",
      "Epoch 1, Batch 15, Loss: 2.3867\n",
      "Epoch 2, Batch 5, Loss: 2.2850\n",
      "Epoch 2, Batch 10, Loss: 2.3002\n",
      "Epoch 2, Batch 15, Loss: 2.2887\n",
      "Epoch 3, Batch 5, Loss: 2.2729\n",
      "Epoch 3, Batch 10, Loss: 2.3181\n",
      "Epoch 3, Batch 15, Loss: 2.3166\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.rand(28, 28)\n",
    "        label = random.randint(0, 9)\n",
    "        return (image, label)\n",
    "\n",
    "dataset_instance = RandomDataset(1000)\n",
    "loader = DataLoader(dataset_instance, batch_size=64, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MNISTClassifier().to(device)\n",
    "model_input = torch.rand(8, 28, 28).to(device)\n",
    "output = model(model_input)\n",
    "target = torch.randint(0, 10, (8,)).to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(1, 4):\n",
    "    for idx, (image, label) in enumerate(loader):\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 5  == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {idx + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b23895-7880-4a13-8670-c7d39af7b1ee",
   "metadata": {},
   "source": [
    "### Project Status: **PASSED** ✅\n",
    "\n",
    "Congratulations. You have successfully built a fully functional PyTorch training pipeline from scratch.\n",
    "\n",
    "### The \"Why\" behind your output\n",
    "\n",
    "You might be wondering why your loss stayed stuck around **2.3** and didn't decrease.\n",
    "\n",
    "* **The Math:** You have 10 classes.\n",
    "* **Random Chance:** The probability of guessing correctly by pure luck is  or .\n",
    "* **The Formula:** CrossEntropyLoss calculates .\n",
    "* **The Calculation:** .\n",
    "\n",
    "Because we used **random noise** inputs and **random** labels, the model *cannot* learn a pattern. It is doing exactly what it should: guessing randomly. If the loss had dropped to 0.5, it would have meant your code had a bug (or your random number generator wasn't random)!\n",
    "\n",
    "### Code Hygiene Review\n",
    "\n",
    "You have a few lines of \"dead code\" left over from previous steps that are no longer needed. In a real script, you would delete these to save memory:\n",
    "\n",
    "```python\n",
    "# These lines before the loop are unused because the loop handles data loading\n",
    "# model_input = torch.rand(8, 28, 28).to(device)  <-- DELETE\n",
    "# output = model(model_input)                     <-- DELETE\n",
    "# target = torch.randint(0, 10, (8,)).to(device)  <-- DELETE\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson L5 Complete.\n",
    "\n",
    "You have mastered the \"PyTorch Way\":\n",
    "\n",
    "1. **Tensors:** The data containers.\n",
    "2. **Autograd:** The automatic calculus engine.\n",
    "3. **Datasets/Loaders:** The data delivery system.\n",
    "4. **nn.Module:** The architecture blueprint.\n",
    "5. **The Loop:** The 5-step cycle of learning.\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that the engine is running, we need to make it drive somewhere real. The next logical step in **M1: Foundations** is to apply this to **Real Data** and add **Validation**.\n",
    "\n",
    "**Ready to move to L6: Training on Real MNIST & Validation Loops?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101765c-9b1b-4476-ab19-7e03f16b3d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
