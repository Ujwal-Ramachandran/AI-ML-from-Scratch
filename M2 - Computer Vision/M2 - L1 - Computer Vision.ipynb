{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce535ed-3750-4fc0-8c4c-c46987acf596",
   "metadata": {},
   "source": [
    "#**Computer Vision**\n",
    "\n",
    "Welcome to Module 2: Computer Vision. We are shifting gears significantly here. In Module 1 (Foundations), you dealt with tabular data where rows were independent samples and columns were features.\n",
    "\n",
    "In Computer Vision, \"features\" aren't given to us in columns; they are hidden in the spatial arrangement of pixels. **Convolutional Neural Networks (CNNs)** are the tools we use to automatically extract these spatial features (edges, textures, shapes) without manually engineering them.\n",
    "\n",
    "Here is the roadmap for this session. We will start by building the mechanics \"by hand\" (using Python loops) to ensure you understand exactly what the sliding window does, then switch to PyTorch for the heavy lifting.\n",
    "\n",
    "### **Phase 1: Topic Breakdown**\n",
    "\n",
    "```text\n",
    "L9: CNN Fundamentals\n",
    "├── Concept 1: The Convolution Operation (The Sliding Window)\n",
    "│   ├── Kernel/Filter (The Feature Detector)\n",
    "│   ├── Dot Product (The Matching Mechanism)\n",
    "│   ├── Purpose: Detecting local patterns (edges, lines) regardless of position.\n",
    "│   ├── Simpler Terms: Sliding a small flashlight over an image to find matches.\n",
    "│   └── Task: Implement a 2D convolution using nested loops (No PyTorch layers).\n",
    "│\n",
    "├── Concept 2: Stride & Padding (Controlling Geometry)\n",
    "│   ├── Stride (Step size of the slide)\n",
    "│   ├── Padding (Adding borders to preserve size)\n",
    "│   ├── Output Dimension Formula\n",
    "│   ├── Purpose: Controlling the spatial reduction and handling image borders.\n",
    "│   ├── Simpler Terms: How big a step we take, and adding a frame so we don't lose the corners.\n",
    "│   └── Task: Update manual implementation to support stride and zero-padding.\n",
    "│\n",
    "├── Concept 3: Pooling Layers (Downsampling)\n",
    "│   ├── Max Pooling vs Average Pooling\n",
    "│   ├── Purpose: Reducing computational load and adding translation invariance.\n",
    "│   ├── Simpler Terms: Summarizing a region by taking the loudest voice (Max) or the group consensus (Avg).\n",
    "│   └── Task: Implement a manual Max Pool function.\n",
    "│\n",
    "├── Concept 4: Receptive Fields & Architecture\n",
    "│   ├── The \"Cone of Vision\"\n",
    "│   ├── Deep vs Shallow layers\n",
    "│   ├── Purpose: Understanding how deeper neurons see larger parts of the original image.\n",
    "│   ├── Simpler Terms: How much of the original photo one specific pixel in the output cares about.\n",
    "│   └── Task: Calculate output dimensions and parameters for a specific architecture.\n",
    "│\n",
    "├── Concept 5: PyTorch CNN Components (Transition to Framework)\n",
    "│   ├── nn.Conv2d, nn.MaxPool2d, nn.Flatten\n",
    "│   ├── Channel dimensions (N, C, H, W)\n",
    "│   └── Task: Define a class-based CNN architecture in PyTorch.\n",
    "│\n",
    "├── Concept 6: Training on CIFAR-10 (The Application)\n",
    "│   ├── Data Normalization for Images\n",
    "│   ├── CrossEntropyLoss in Multiclass classification\n",
    "│   └── Task: Train the defined model on the CIFAR-10 dataset.\n",
    "│\n",
    "└── Concept 7: Feature Map Visualization (Peeking inside the Black Box)\n",
    "    ├── Extracting intermediate activations\n",
    "    ├── Purpose: Verifying what filters are actually learning.\n",
    "    └── Task: Visualize the output of the first convolutional layer.\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6e03c-1c78-4090-b7db-4209bd99b15f",
   "metadata": {},
   "source": [
    "## **Concept 1: The Convolution Operation (The Sliding Window)**\n",
    "\n",
    "### Intuition: The \"Flashlight\" Search\n",
    "\n",
    "Imagine you are looking for a specific pattern, like a vertical edge or a diagonal line, in a large image. Instead of checking the whole image at once, you use a small \"scanner\" (the **Kernel** or **Filter**) and slide it across the image.\n",
    "\n",
    "At every position, you compare the scanner with the part of the image underneath it.\n",
    "\n",
    "* If the image pixels match the scanner's pattern, the result is a high number (strong activation).\n",
    "* If they don't match, the result is low or negative.\n",
    "\n",
    "This creates a **Feature Map**: a map showing *where* in the image that specific pattern was found.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "Mathematically, a convolution is a sum of element-wise products.\n",
    "\n",
    "Given:\n",
    "   * **Input Image ($I$):** A 2D matrix of pixel values.\n",
    "   * **Kernel ($K$):** A smaller 2D matrix (e.g., $3 \\times 3$) containing weights.\n",
    "\n",
    "For every position $(i, j)$ where the kernel fits entirely on the image:\n",
    "   1. **Overlay** the kernel on top of the image patch.\n",
    "   2. **Multiply** every kernel weight by the corresponding pixel value underneath it.\n",
    "   3. **Sum** all these products to get a single number.\n",
    "   4. **Place** this number in the output matrix at position $(i, j)$.\n",
    "   \n",
    "$$Output[i, j] = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} I[i+m, j+n] \\times K[m, n]$$\n",
    "\n",
    "*(Note: In deep learning, this is technically \"cross-correlation\" because we don't flip the kernel, but everyone calls it convolution.)*\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Think of the Kernel as a stencil. You slide the stencil over a piece of paper (the image). At each spot, you calculate how much the drawing on the paper looks like the shape cut into the stencil. You write that \"similarity score\" down on a new sheet of grid paper. That new sheet is your Convolution Output.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Pros:** **Translation Invariance** (mostly). A vertical edge detector will find a vertical edge whether it's in the top-left or bottom-right.\n",
    "* **Cons:** Computational cost. You are doing many multiplications for every single pixel.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "**Implement the 2D Convolution logic using Python loops.**\n",
    "\n",
    "We will stick to 2D arrays (grayscale, single channel) for this specific exercise to isolate the sliding window logic.\n",
    "\n",
    "**Specifications:**\n",
    "\n",
    "   1. **Function Name:** `convolution_2d_manual(input_matrix, kernel)`\n",
    "   2. **Inputs:**\n",
    "      * `input_matrix`: A numpy array of shape $(H, W)$.\n",
    "      * `kernel`: A numpy array of shape $(K, K)$.\n",
    "   \n",
    "   3. **Logic:**\n",
    "      * Calculate the output dimensions. Formula: $Output\\_Dim = Input\\_Dim - Kernel\\_Dim + 1$ (Assuming Stride=1, No Padding).\n",
    "      * Initialize an output matrix of zeros.\n",
    "      * Use nested loops to iterate through valid positions of the input.\n",
    "      * Perform the element-wise multiplication and sum.\n",
    "   \n",
    "   4. **Constraints:**\n",
    "      * **No** `scipy.signal.convolve` or `torch.nn`.\n",
    "      * **Use** `numpy`.\n",
    "      * Assume Stride = 1 and No Padding.\n",
    "\n",
    "**Test Case to Verify:**\n",
    "   * Input: A $5 \\times 5$ matrix of all ones.\n",
    "   * Kernel: A $3 \\times 3$ matrix of all ones.\n",
    "   * **Expected Output:** A $3 \\times 3$ matrix where every value is 9 (since $3 \\times 3$ summed 9 times = 9).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51385a72-652b-4e56-bcc6-ea8052af1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convolution_2d_manual(input_matrix, kernel):\n",
    "    im_n, im_m = input_matrix.shape\n",
    "    k_n, k_m = kernel.shape\n",
    "    output_n = im_n - k_n + 1 \n",
    "    output_m = im_m - k_m +1\n",
    "    \n",
    "    output = np.zeros((output_n, output_m))\n",
    "\n",
    "    for i in range(output_n):\n",
    "        for j in range(output_m):\n",
    "            region = input_matrix[i:i + k_n, j:j + k_m]\n",
    "            output[i, j] = np.sum(region * kernel)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a1476-c8a3-4c15-8ef0-a5b1e2b841b5",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 2: Stride & Padding (Controlling Geometry)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "1. **Stride (Stepping):**\n",
    "   * In the previous concept, we slid the window 1 pixel at a time.\n",
    "   * **Stride** is simply taking bigger steps. If Stride = 2, you skip every other pixel.\n",
    "   * **Why?** It drastically reduces the size of the output (downsampling) and saves computation.\n",
    "\n",
    "\n",
    "2. **Padding (The Frame):**\n",
    "   * Notice how `output_n = im_n - k_n + 1`? The output always gets *smaller* than the input. If you apply many layers, your image eventually disappears!\n",
    "   * Also, pixels in the center are scanned many times, but corners are scanned only once.\n",
    "   * **Padding** adds a border of \"dummy\" pixels (usually zeros) around the input image. This allows the kernel to \"hang off the edge\" and maintains the image size.\n",
    "\n",
    "\n",
    "### Mechanics (The Master Formula)\n",
    "\n",
    "This is the most important formula in CNN arithmetic. Memorize this.\n",
    "$$Output = \\lfloor \\frac{Input - Kernel + 2 \\times Padding}{Stride} \\rfloor + 1$$\n",
    "\n",
    "   * **Input:** Input size ($W$ or $H$)\n",
    "   * **Kernel:** Filter size ($K$)\n",
    "   * **Padding:** Pixels added to *each* side ($P$)\n",
    "   * **Stride:** Step size ($S$)\n",
    "\n",
    "### Simpler Explanation\n",
    "   * **Stride:** Instead of walking heel-to-toe (Stride 1), you jump over cracks in the sidewalk (Stride 2).\n",
    "   * **Padding:** Putting a mat around a painting so you can paint right up to the very edge without getting paint on the floor.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Stride > 1:** Faster, but you might lose fine-grained details.\n",
    "   * **Padding:** Keeps spatial dimensions constant (helpful for deep networks), but adds \"fake\" data (zeros) at the edges.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Update your `convolution_2d_manual` function to support **Stride** and **Padding**.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Update Function Signature:** `convolution_2d_manual(input_matrix, kernel, stride=1, padding=0)`\n",
    "   2. **Logic Update:**\n",
    "      * **Apply Padding First:** Create a new, larger matrix with zeros around the border. (Hint: `np.pad` is useful, or create a larger zeros matrix and slot the image into the center).\n",
    "      * **Update Output Dimensions:** Use the \"Master Formula\" above.\n",
    "      * **Update Loops:** The loop range must use the new output dimensions.\n",
    "      * **Update Slicing:** When calculating where the `region` is, you need to multiply the loop index by the stride. (e.g., if `i=1` and `stride=2`, the window starts at index 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b2c7f-df80-43d4-a2d4-93cc0c45b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cconvolution_2d_manual(input_matrix, kernel, stride=1, padding=0):\n",
    "    im_n, im_n = input_matrix.shape\n",
    "    k_m, k_n = kernel.shape\n",
    "\n",
    "    output = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4731dba-31d0-4705-a99f-6577e512a8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221b4ed-b746-4e64-83e9-b0b415275284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09b2fe-0550-4aa6-ad7d-1cdecd14443f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2093f4f7-e24f-49af-a727-d0bd590fc247",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7faa2-ff04-4d94-8e61-d1ad599b5c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ee4cd-b2a2-4d41-b71d-f93541ece6aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba76c2-35c5-4c29-9d26-a988ae5fa38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa33b75-78fc-4152-b9aa-4a86a0e10f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bddf68f-b856-41bf-a857-4b44bd38f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05342d42-1c71-44db-bdfe-32ce85e2da3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7f302-5d91-490a-93de-6c7b369a2050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad5f44-b53b-4133-80d5-cbedc61af33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8d3b1-b550-4219-a29e-009a37873943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977baeeb-2b92-422d-a771-d2da3c59171f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3272c4-e874-444d-9891-8017daf0319a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
