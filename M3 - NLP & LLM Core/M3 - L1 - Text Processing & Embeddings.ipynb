{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0605750a-c06c-4f1c-b38f-c9de8db1603b",
   "metadata": {},
   "source": [
    "# **Module 3: NLP & LLM Core**\n",
    "\n",
    "## L13: Text Processing & Embeddings\n",
    "\n",
    "Welcome to the NLP module. We are moving from Computer Vision (pixels) to Natural Language Processing (sequences/tokens). This is the foundation for the Agentic AI and RAG work you have planned later.\n",
    "\n",
    "This lesson focuses on how we convert human language into numerical vectors that machines can \"understand.\" We will progress from simple frequency counts to deep semantic representations.\n",
    "\n",
    "### Topic Breakdown\n",
    "\n",
    "```text\n",
    "L13: Text Processing & Embeddings\n",
    "├── Concept 1: Tokenization (Subword & BPE)\n",
    "│   ├── Word-level vs. Character-level vs. Subword\n",
    "│   ├── The OOV (Out of Vocabulary) Problem\n",
    "│   ├── Byte Pair Encoding (BPE) Intuition\n",
    "│   ├── Explanation: Breaking text into meaningful chunks (tokens)\n",
    "│   └── Task: Use a tokenizer to inspect tokenization differences\n",
    "│\n",
    "├── Concept 2: Sparse Representations (TF-IDF) [Baseline]\n",
    "│   ├── Term Frequency (TF)\n",
    "│   ├── Inverse Document Frequency (IDF)\n",
    "│   ├── Explanation: Weighing words by how \"rare\" and \"informative\" they are\n",
    "│   └── Task: Compute TF-IDF matrix for a mini-corpus using sklearn\n",
    "│\n",
    "├── Concept 3: Static Dense Embeddings (Word2Vec/GloVe Intuition)\n",
    "│   ├── One-Hot vs. Dense Vectors\n",
    "│   ├── Semantic Meaning in Vector Space (King - Man + Woman = Queen)\n",
    "│   ├── Limitation: Context Independence (Polysemy)\n",
    "│   └── Task: Manual Cosine Similarity calculation on mock embedding vectors\n",
    "│\n",
    "├── Concept 4: Transformer Embeddings (Sentence-BERT)\n",
    "│   ├── Contextual Embeddings (Why \"bank\" differs in two sentences)\n",
    "│   ├── The Cross-Encoder vs. Bi-Encoder (Siamese Network) architecture\n",
    "│   ├── Explanation: Capturing the meaning of whole sentences\n",
    "│   └── Task: Load a Sentence-Transformer model and encode text\n",
    "│\n",
    "└── Mini-Project: Semantic Classifier Comparison\n",
    "    ├── Dataset: 20 Newsgroups (Subset) or similar text dataset\n",
    "    ├── Pipeline A: TF-IDF + Logistic Regression\n",
    "    ├── Pipeline B: SBERT Embeddings + Logistic Regression\n",
    "    └── Evaluation: Compare Accuracy/F1 Score\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48a953-00b1-4333-86ff-73911b295246",
   "metadata": {},
   "source": [
    "## **Concept 1: Tokenization (Subword & BPE)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Before a model can process text, it must be broken down into smaller units called **tokens**. The simplest approach is splitting by spaces (Word-level), but this fails when the model encounters a word it hasn't seen before (the \"Out-Of-Vocabulary\" or **OOV** problem). Conversely, splitting by characters (Character-level) solves OOV but results in extremely long sequences where individual units carry little meaning.\n",
    "\n",
    "Modern NLP uses **Subword Tokenization** (e.g., Byte-Pair Encoding or BPE). This is the \"Goldilocks\" zone. It breaks common words into single tokens (e.g., \"apple\") but breaks rare or complex words into meaningful sub-units (e.g., \"tokenization\"  \"token\", \"##iza\", \"##tion\"). This allows the model to process *any* text using a fixed-size vocabulary.\n",
    "\n",
    "### Mechanics: Byte-Pair Encoding (BPE)\n",
    "\n",
    "BPE works by iteratively merging the most frequently occurring adjacent pairs of characters (or bytes) in the training corpus.\n",
    "   1. **Initialize:** Vocabulary includes all individual characters.\n",
    "   2. **Count:** Calculate frequency of all symbol pairs (e.g., \"e\" + \"s\" $\\rightarrow$ \"es\").\n",
    "   3. **Merge:** Add the most frequent pair to the vocabulary as a new symbol.\n",
    "   4. **Repeat:** Continue until the vocabulary size reaches a target limit (e.g., 30k or 50k tokens).\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Think of tokens like Lego bricks.\n",
    "   * **Word-level:** Every unique word is a custom-molded brick. If you need a \"microscope\" brick and don't have it, you can't build the sentence.\n",
    "   * **Character-level:** You only have 26 types of tiny 1x1 bricks. You can build anything, but it takes thousands of bricks to build a simple house.\n",
    "   * **Subword (BPE):** You have a set of standard complex shapes (walls, windows) for common structures, but you also keep the tiny 1x1 bricks. If you encounter a rare structure, you build it using the standard shapes and the tiny bricks.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Solves OOV (can represent any string), balances sequence length and meaning.\n",
    "   * **Cons:** Handling the \"sub-tokens\" (like `##ing` in BERT) requires careful implementation. Typos can result in weird subword splits.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will use the Hugging Face `transformers` library to observe how a subword tokenizer handles known words versus rare words/typos.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Import:** `AutoTokenizer` from `transformers`.\n",
    "   2. **Load:** The tokenizer for `bert-base-uncased`.\n",
    "   3. **Input Text:** \"unaffable\" (a standard word) vs \"unaffabwle\" (a typo/nonsense word).\n",
    "   4. **Action:**\n",
    "      * Tokenize both strings.\n",
    "      * Convert the IDs back to tokens (strings) to see the split.\n",
    "   5. **Output:** Print the list of tokens for both words.\n",
    "\n",
    "**Note:** You might need to install transformers: `pip install transformers`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5a5ca0-d038-4f00-bf8c-56be1cc4e9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ebcc23-66cf-42de-9eef-79061ef20ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For unaffable\n",
      "Token = {'input_ids': [101, 14477, 20961, 3468, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}\n",
      "Back to string = ['[CLS]', 'una', '##ffa', '##ble', '[SEP]']\n",
      "For unaffabwle\n",
      "Token = {'input_ids': [101, 14477, 20961, 2497, 13668, 2063, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
      "Back to string = ['[CLS]', 'una', '##ffa', '##b', '##wl', '##e', '[SEP]']\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "word1 = \"unaffable\"\n",
    "word2 = \"unaffabwle\"\n",
    "\n",
    "t1 = tokenizer(word1)\n",
    "t2 = tokenizer(word2)\n",
    "\n",
    "\n",
    "w1 = tokenizer.convert_ids_to_tokens(t1[\"input_ids\"])\n",
    "w2 = tokenizer.convert_ids_to_tokens(t2[\"input_ids\"])\n",
    "\n",
    "print(f\"For {word1}\\nToken = {tokenizer(word1)}\\nBack to string = {w1}\")\n",
    "print(f\"For {word2}\\nToken = {tokenizer(word2)}\\nBack to string = {w2}\\n==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d755d0-2e95-46f2-b493-b6e447644af9",
   "metadata": {},
   "source": [
    "\n",
    "Excellent. This perfectly illustrates the power of Subword Tokenization.\n",
    "\n",
    "* **\"unaffable\"**: Broken into 3 logical chunks (`una`, `##ffa`, `##ble`). The model can likely infer the meaning is related to \"unable\" or \"affable\" based on these sub-parts.\n",
    "* **\"unaffabwle\"**: The tokenizer didn't panic or crash. It just kept cutting until it found pieces it recognized (`##b`, `##wl`, `##e`). Even though the word is nonsense, the model has a valid input vector to process.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 2: Sparse Representations (TF-IDF)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "After tokenization, we have a list of tokens. The simplest way to turn these into numbers is counting them (\"Bag of Words\"). However, raw counts have a flaw: common words like \"the\", \"is\", and \"and\" appear frequently but carry little information.\n",
    "\n",
    "**TF-IDF** (Term Frequency - Inverse Document Frequency) fixes this by balancing two factors:\n",
    "\n",
    "1. **Frequency:** How often does the word appear in *this specific* document? (More is better).\n",
    "2. **Rarity:** How often does the word appear in *all* documents? (Less is better).\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "The score $w_{t,d}$ for a term $t$ in document $d$ is:\n",
    "$$TF_{t,d} = \\frac{\\text{count of t in d}}{\\text{total terms in d}}$$\n",
    "\n",
    "1. **TF (Term Frequency):**\n",
    "$$w_{t,d} = TF_{t,d} \\times IDF_t$$\n",
    "*(Note: Implementations often use raw count or log normalization)*\n",
    "\n",
    "2. **IDF (Inverse Document Frequency):**\n",
    "$$IDF_t = \\log \\left( \\frac{N}{df_t} \\right)$$\n",
    "Where $N$ is the total number of documents, and $df_t$ is the number of documents containing term $t$.\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine you are scanning a library for books about \"Quantum Physics\".\n",
    "   * The word \"the\" is in every book. $IDF \\approx 0$. It gets a score of 0.\n",
    "   * The word \"Quantum\" appears many times in specific books, but not in cookbooks or novels. It has high TF (in the physics book) and high IDF (rare globally). It gets a high score.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Very fast, interpretable (you know exactly which words triggered the score), works surprisingly well for simple keyword matching.\n",
    "   * **Cons:** **Sparse** (vectors are mostly zeros), **No Semantics** (it doesn't know \"car\" and \"automobile\" are related; they are just different orthogonal dimensions).\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will manually calculate the TF-IDF matrix using Scikit-Learn to see the sparsity.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Import:** `TfidfVectorizer` from `sklearn.feature_extraction.text`.\n",
    "   2. **Data:** Create a list of strings:\n",
    "   ```python\n",
    "   corpus = [\n",
    "       \"the cat sat on the mat\",\n",
    "       \"the dog sat on the log\",\n",
    "       \"cats and dogs are great\"\n",
    "   ]\n",
    "   \n",
    "   ```\n",
    "\n",
    "   3. **Action:**\n",
    "      * Initialize the vectorizer.\n",
    "      * Fit and transform the corpus.\n",
    "      * Get the feature names (the vocabulary).\n",
    "      * Convert the result to a dense array (using `.toarray()`) or a DataFrame for readability.\n",
    "   \n",
    "   \n",
    "   4. **Output:** Print the feature names and the resulting matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47a1c5df-59e7-4951-9e28-4eb2b8ae7c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>are</th>\n",
       "      <th>cat</th>\n",
       "      <th>cats</th>\n",
       "      <th>dog</th>\n",
       "      <th>dogs</th>\n",
       "      <th>great</th>\n",
       "      <th>log</th>\n",
       "      <th>mat</th>\n",
       "      <th>on</th>\n",
       "      <th>sat</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427554</td>\n",
       "      <td>0.325166</td>\n",
       "      <td>0.325166</td>\n",
       "      <td>0.650331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.427554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.325166</td>\n",
       "      <td>0.325166</td>\n",
       "      <td>0.650331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and       are       cat      cats       dog      dogs     great  \\\n",
       "0  0.000000  0.000000  0.427554  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.427554  0.000000  0.000000   \n",
       "2  0.447214  0.447214  0.000000  0.447214  0.000000  0.447214  0.447214   \n",
       "\n",
       "        log       mat        on       sat       the  \n",
       "0  0.000000  0.427554  0.325166  0.325166  0.650331  \n",
       "1  0.427554  0.000000  0.325166  0.325166  0.650331  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs are great\"\n",
    "]\n",
    "\n",
    "tfiv = TfidfVectorizer()\n",
    "\n",
    "X = tfiv.fit_transform(corpus)\n",
    "\n",
    "feature_names = tfiv.get_feature_names_out()\n",
    "\n",
    "dense_matrix = X.toarray()\n",
    "df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fe88b-5af3-4f48-a801-5935b73520d6",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "\n",
    "Notice the **Sparsity**:\n",
    "   * \"cat\" and \"cats\" are treated as completely different words (columns). The model doesn't know they are related.\n",
    "   * \"dog\" and \"cat\" have 0 overlap. In this vector space, they are as different as \"dog\" and \"refrigerator\".\n",
    "   * The matrix is mostly zeros (empty space).\n",
    "\n",
    "This lack of relationship is what Dense Embeddings solve.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 3: Static Dense Embeddings (Word2Vec/GloVe Intuition)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "To fix the \"cat vs cats\" problem, we need **Dense Vectors**. instead of a vector of size 10,000 (vocabulary size) with one `1` and mostly `0`s (One-Hot), we compress the meaning into a smaller vector (e.g., size 300) of continuous numbers (floats like 0.2, -0.9, 0.5).\n",
    "\n",
    "In this \"embedding space,\" words with similar meanings are pushed closer together.\n",
    "   * **Word2Vec:** Learns by predicting the neighbors of a word (e.g., \"The quick brown ____ jumps\").\n",
    "   * **GloVe:** Learns by analyzing global co-occurrence counts across the entire corpus.\n",
    "\n",
    "### Mechanics: Vector Arithmetic\n",
    "\n",
    "The most famous property of these embeddings is that they capture semantic relationships algebraically:\n",
    "$$\\vec{King} - \\vec{Man} + \\vec{Woman} \\approx \\vec{Queen}$$\n",
    "This works because the \"direction\" you travel to go from \"Man\" to \"Woman\" (gender dimension) is the same direction you travel to go from \"King\" to \"Queen\".\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Pros:** Captures semantic similarity (synonyms are close), efficient (dense).\n",
    "* **Cons:** **Context Independent (Static)**. The word \"bank\" has only **one** vector, even if it refers to a river bank or a financial bank. It averages all meanings into one messy vector.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will perform \"Semantic Arithmetic\" manually using NumPy to understand how similarity works.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Define Mock Vectors:**\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   # Simplified 3D vectors for visualization\n",
    "   king  = np.array([0.5, 0.7, 0.2])\n",
    "   man   = np.array([0.5, 0.1, 0.2])\n",
    "   woman = np.array([0.5, 0.1, 0.8])\n",
    "   queen = np.array([0.5, 0.7, 0.8])\n",
    "   \n",
    "   ```\n",
    "   \n",
    "   2. **Vector Math:** Calculate a `target` vector: `king - man + woman`.\n",
    "   3. **Similarity:** Calculate the **Cosine Similarity** between your `target` vector and the `queen` vector.\n",
    "      * **Formula:** $\\text{Similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||}$\n",
    "      * Where $A \\cdot B$ is the dot product and $||A||$ is the L2 norm (magnitude).\n",
    "   4. **Constraint:** You **must** write the cosine similarity formula yourself using `np.dot` and `np.linalg.norm`. Do not use `sklearn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aee7ffd-d214-4ead-ba1b-92cb6db7b1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Simplified 3D vectors for visualization\n",
    "king  = np.array([0.5, 0.7, 0.2])\n",
    "man   = np.array([0.5, 0.1, 0.2])\n",
    "woman = np.array([0.5, 0.1, 0.8])\n",
    "queen = np.array([0.5, 0.7, 0.8])\n",
    "\n",
    "target = king - man + woman\n",
    "sim = np.dot(target, queen)/(np.linalg.norm(target) * np.linalg.norm(queen))\n",
    "sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d419dc-24f7-4801-b332-f3e7f443096c",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "You got a result of **1.0**. In this idealized example, the math worked perfectly: the \"gender direction\" you added to King landed exactly on Queen. In real-world data (like GloVe), it's rarely 1.0, but it will be the *closest* vector in the space.\n",
    "\n",
    "This proves that **math can represent meaning**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 4: Transformer Embeddings (Sentence-BERT)**\n",
    "\n",
    "### Intuition: The Context Problem\n",
    "\n",
    "Word2Vec is **static**.\n",
    "   * Sentence A: \"I went to the **bank** to deposit money.\"\n",
    "   * Sentence B: \"I sat on the river **bank**.\"\n",
    "\n",
    "In Word2Vec, the word \"bank\" has the **exact same vector** in both sentences. This confuses the model.\n",
    "\n",
    "**Transformers (like BERT)** are **dynamic**. They use an \"Attention Mechanism\" to look at the whole sentence at once. The vector for \"bank\" changes based on the words around it (\"money\" vs \"river\").\n",
    "\n",
    "**Sentence-BERT (SBERT)** takes this further. Standard BERT gives you a vector for every token. SBERT is fine-tuned to output a single, high-quality vector **for the entire sentence** that is mathematically optimized for similarity search (cosine similarity).\n",
    "\n",
    "### Mechanics: Siamese Networks\n",
    "\n",
    "SBERT isn't just one BERT; during training, it uses **Siamese Networks** (twin networks).\n",
    "   1. Feed Sentence A into BERT A.\n",
    "   2. Feed Sentence B into BERT B (identical copy).\n",
    "   3. Compare their outputs using Cosine Similarity.\n",
    "   4. Backpropagate to ensure similar sentences have similar vectors and dissimilar ones are far apart.\n",
    "\n",
    "This results in embeddings where `distance = semantic_difference`.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** State-of-the-art accuracy for semantic search and clustering. Understands context deeply.\n",
    "   * **Cons:** Slower to compute than TF-IDF or Word2Vec. Heavy memory usage.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will use the `sentence-transformers` library to generate these context-aware embeddings.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Install:** `pip install sentence-transformers` (if needed).\n",
    "   2. **Import:** `SentenceTransformer` from `sentence_transformers`.\n",
    "   3. **Load Model:** Load the model named `'all-MiniLM-L6-v2'` (This is a small, fast, industry-standard model).\n",
    "   4. **Data:**\n",
    "      ```python\n",
    "      sentences = [\n",
    "          \"That is a happy dog\",\n",
    "          \"That is a very happy person\",\n",
    "          \"Today is a sunny day\"\n",
    "      ]\n",
    "      \n",
    "      ```\n",
    "   5. **Action:**\n",
    "      * Encode the sentences into embeddings.\n",
    "      * Print the **shape** of the resulting embedding matrix.\n",
    "      * Print the first 5 values of the first sentence's vector (to see what they look like).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abe8d75c-c320-4d82-9dfe-cb7ed2d2bfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix = (3, 384)\n",
      "First 5 values = [ 0.00504993  0.06316978  0.01415724  0.02694938 -0.06023403]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\n",
    "    \"That is a happy dog\",\n",
    "    \"That is a very happy person\",\n",
    "    \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "encode = model.encode(sentences)\n",
    "print(f\"Shape of matrix = {encode.shape}\")\n",
    "print(f\"First 5 values = {encode[0][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4e56a-db88-49b9-9354-cb47c58f79d6",
   "metadata": {},
   "source": [
    "### Review\n",
    "\n",
    "\n",
    "**Analysis of your Output:**\n",
    "   * **Shape `(3, 384)`:** You have 3 sentences, and each sentence is represented by a vector of **384 dimensions**.\n",
    "   * Unlike TF-IDF (where dimensions = vocabulary size, often 10,000+), this is a compact, dense representation.\n",
    "   * Unlike the 3D toy example (King/Queen), this 384-dimensional space captures subtle nuances of grammar, tone, and meaning.\n",
    "   * **Values:** These floats (`0.005...`) are the coordinates in that high-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mini-Project: Semantic Classifier Showdown**\n",
    "\n",
    "**Objective:**\n",
    "Build two parallel text classification pipelines to classify news articles into 4 topics. You will prove whether \"understanding meaning\" (Embeddings) beats \"counting words\" (TF-IDF).\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Dataset:**\n",
    "      * Use `sklearn.datasets.fetch_20newsgroups`.\n",
    "      * **Categories:** `['sci.space', 'comp.graphics', 'rec.sport.hockey', 'talk.politics.mideast']`.\n",
    "      * **Cleaning:** Set `remove=('headers', 'footers', 'quotes')` (Critical: this forces the model to read the actual text, not just email headers).\n",
    "      * **Subset:** Use `subset='all'` (fetches both train and test for simplicity, we will split manually).\n",
    "  \n",
    "   2. **Preprocessing:**\n",
    "      * Split data into **Train (80%)** and **Test (20%)** using `train_test_split` (random_state=42).\n",
    "   \n",
    "   3. **Pipeline A (The Baseline):**\n",
    "      * Vectorize text using `TfidfVectorizer`.\n",
    "      * Train a `LogisticRegression` classifier on the TF-IDF vectors.\n",
    "      * Predict on Test set.\n",
    "   \n",
    "   4. **Pipeline B (The Challenger):**\n",
    "      * Encode text using `SentenceTransformer('all-MiniLM-L6-v2')`.\n",
    "      * Train a **new** `LogisticRegression` classifier on these dense embeddings.\n",
    "      * Predict on Test set.\n",
    "   \n",
    "   5. **Evaluation:**\n",
    "      * Print the **Accuracy Score** for both pipelines.\n",
    "      * (Optional but recommended) Print a `classification_report` for both.\n",
    "\n",
    "**Forbidden Shortcuts:**\n",
    "\n",
    "* Do not use raw `CountVectorizer`.\n",
    "* Do not skip the train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99540b82-7923-413b-aeca-f14837e42484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TfidfPipeline...\n",
      "Evaluating TfidfPipeline...\n",
      "Accuracy: 0.9064102564102564\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91       192\n",
      "           1       0.94      0.88      0.91       199\n",
      "           2       0.82      0.94      0.88       193\n",
      "           3       0.96      0.90      0.93       196\n",
      "\n",
      "    accuracy                           0.91       780\n",
      "   macro avg       0.91      0.91      0.91       780\n",
      "weighted avg       0.91      0.91      0.91       780\n",
      "\n",
      "Training SemanticPipeline...\n",
      "Evaluating SemanticPipeline...\n",
      "Accuracy: 0.9179487179487179\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93       192\n",
      "           1       0.97      0.89      0.93       199\n",
      "           2       0.85      0.93      0.89       193\n",
      "           3       0.92      0.93      0.93       196\n",
      "\n",
      "    accuracy                           0.92       780\n",
      "   macro avg       0.92      0.92      0.92       780\n",
      "weighted avg       0.92      0.92      0.92       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Base Class for Text Classifiers\n",
    "class BaseTextClassifier:\n",
    "    def __init__(self):\n",
    "        self.classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    def extract_features(self, texts, is_training=False):\n",
    "        \"\"\"Convert list of strings to numpy array/matrix.\"\"\"\n",
    "        raise NotImplementedError(\"Child class must implement this\")\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        print(f\"Training {self.__class__.__name__}...\")\n",
    "        # 1. Extract features (is_training=True)\n",
    "        X_train_features = self.extract_features(X_train, is_training=True)\n",
    "        # 2. Fit the classifier\n",
    "        self.classifier.fit(X_train_features, y_train)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        print(f\"Evaluating {self.__class__.__name__}...\")\n",
    "        # 1. Extract features (is_training=False)\n",
    "        X_test_features = self.extract_features(X_test, is_training=False)\n",
    "        # 2. Predict using the classifier\n",
    "        y_pred = self.classifier.predict(X_test_features)\n",
    "        # 3. Print accuracy and classification report\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Pipeline A: TF-IDF + Logistic Regression\n",
    "class TfidfPipeline(BaseTextClassifier):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize the TfidfVectorizer\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def extract_features(self, texts, is_training=False):\n",
    "        if is_training:\n",
    "            # Fit and transform on training data\n",
    "            return self.vectorizer.fit_transform(texts)\n",
    "        else:\n",
    "            # Transform the test data (use the fitted vectorizer)\n",
    "            return self.vectorizer.transform(texts)\n",
    "\n",
    "# Pipeline B: Sentence Embeddings + Logistic Regression\n",
    "class SemanticPipeline(BaseTextClassifier):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        super().__init__()\n",
    "        # Initialize the SentenceTransformer model\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def extract_features(self, texts, is_training=False):\n",
    "        # Extract dense embeddings (this is independent of is_training)\n",
    "        return self.model.encode(texts)\n",
    "\n",
    "# Usage Example\n",
    "\n",
    "# Sample dataset (for demonstration purposes)\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset (fetching only the 4 specified categories)\n",
    "categories = ['sci.space', 'comp.graphics', 'rec.sport.hockey', 'talk.politics.mideast']\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split the data into train and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups_data.data, newsgroups_data.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Train and Evaluate TfidfPipeline ---\n",
    "tfidf_pipeline = TfidfPipeline()\n",
    "tfidf_pipeline.train(X_train, y_train)\n",
    "tfidf_pipeline.evaluate(X_test, y_test)\n",
    "\n",
    "# --- Train and Evaluate SemanticPipeline ---\n",
    "semantic_pipeline = SemanticPipeline()\n",
    "semantic_pipeline.train(X_train, y_train)\n",
    "semantic_pipeline.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cbf6c1-183b-47e8-8725-fa1437761429",
   "metadata": {},
   "source": [
    "### Code Review & Analysis\n",
    "\n",
    "**The Verdict:**\n",
    "   * **TF-IDF Accuracy:** ~90.6%\n",
    "   * **SBERT Accuracy:** ~91.8%\n",
    "\n",
    "**Why did SBERT win (slightly)?**\n",
    "                                                                                                                                                                                                                                \n",
    "Even though the margin is small (~1.2%), SBERT wins because it understands **context**.\n",
    "\n",
    "   * If a space article says \"The vessel left the atmosphere,\" TF-IDF might miss it if it only looks for \"rocket\" or \"shuttle.\"\n",
    "   * SBERT knows \"vessel\" + \"atmosphere\"  \"space travel.\"\n",
    "\n",
    "**Why was TF-IDF so competitive?**\n",
    "                                                                                            \n",
    "The *20 Newsgroups* dataset is very **keyword-heavy**.\n",
    "   * Space articles are full of \"orbit,\" \"nasa,\" \"launch.\"\n",
    "   * Hockey articles are full of \"puck,\" \"goal,\" \"team.\"\n",
    "   * TF-IDF thrives here. If we were classifying subtle sentiment (e.g., sarcasm), TF-IDF would likely fail miserably while SBERT would shine.\n",
    "\n",
    "### Optimizations for the Future\n",
    "   1. **Batching:** In `SemanticPipeline`, for huge datasets, `model.encode(texts)` might run out of memory. You would want to pass `batch_size=32`.\n",
    "   2. **Freezing:** Currently, we used the embeddings as fixed features. In later modules (M3 Week 15), we will \"Fine-Tune\" BERT, allowing the embeddings themselves to shift to better fit our specific data.\n",
    "\n",
    "---\n",
    "\n",
    "### Module Progress\n",
    "\n",
    "You have successfully completed **L13: Text Processing & Embeddings**.\n",
    "\n",
    "**Summary of Skills Acquired:**\n",
    "\n",
    "1. **Tokenization:** You know why we split words into subwords (`##ing`) to handle OOVs.\n",
    "2. **Sparse Vectors:** You built a TF-IDF baseline and saw its sparsity.\n",
    "3. **Dense Vectors:** You understand how semantic meaning is captured in vector arithmetic (`King - Man + Woman`).\n",
    "4. **Transformers:** You implemented a production-grade embedding pipeline using Sentence-BERT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bb3220-6431-4d8b-ad85-562baa2bda92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d12825f-06a4-43b8-a0fa-8a473d61395f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa356e27-66b2-402b-8fb1-84d8c4b4bfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f35ec-6a4e-45ae-8f83-563878b5e226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d508b75-17c7-4141-b551-38639a6787d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
