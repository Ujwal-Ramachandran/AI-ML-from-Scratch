{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0df165-e7b2-43f1-84db-32129520a844",
   "metadata": {},
   "source": [
    "# **Vector Search Fundamentals**\n",
    "\n",
    "We are now transitioning from generating embeddings (which we will cover more deeply in M3) to **managing and searching** them at scale. This is the backbone of RAG and modern retrieval systems.\n",
    "\n",
    "We will focus on the fundamental geometry of vector spaces and how to traverse them efficiently using the **FAISS** library.\n",
    "\n",
    "Here is the breakdown for today's session.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "```text\n",
    "L17: Vector Search Fundamentals\n",
    "├── Concept 1: Vector Spaces & High-Dimensional Data\n",
    "│   ├── Embeddings as coordinates\n",
    "│   ├── The Matrix (N vectors x D dimensions)\n",
    "│   ├── Simple Explanation: Finding points in a massive hyper-cube\n",
    "│   └── Task: Generate synthetic vector dataset (NumPy)\n",
    "│\n",
    "├── Concept 2: Distance Metrics\n",
    "│   ├── Euclidean Distance (L2) - Physical distance\n",
    "│   ├── Cosine Similarity - Angular distance\n",
    "│   ├── Intuition: Magnitude vs. Orientation\n",
    "│   └── Task: Manual calculation of metrics using NumPy\n",
    "│\n",
    "├── Concept 3: ANN Theory (Approximate Nearest Neighbors)\n",
    "│   ├── The Scaling Problem (O(N) Complexity)\n",
    "│   ├── IVF (Inverted File Index) - Partitioning/Clustering\n",
    "│   ├── HNSW (Hierarchical Navigable Small World) - Graph Traversal\n",
    "│   └── Task: Conceptual Check (Trade-offs)\n",
    "│\n",
    "├── Concept 4: FAISS Basics (The Tool)\n",
    "│   ├── What is FAISS? (Facebook AI Similarity Search)\n",
    "│   ├── The Index Object (IndexFlatL2)\n",
    "│   ├── The Workflow: Index -> Add -> Search\n",
    "│   └── Task: Implement \"Brute Force\" search in FAISS\n",
    "│\n",
    "└── Mini-Project: The Search Benchmark\n",
    "    ├── Setup Large Dataset\n",
    "    ├── Implement IndexFlatL2 (Exact)\n",
    "    ├── Implement IndexIVFFlat (Approximate)\n",
    "    ├── Train the Index (Clustering)\n",
    "    └── Compare speed (latency) and recall\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f8c4-4f60-40cf-bb35-bce6fc31dbc1",
   "metadata": {},
   "source": [
    "## **Concept 1: Vector Spaces & High-Dimensional Data**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In traditional databases, we search for exact matches (e.g., `SELECT * FROM users WHERE name = \"Alice\"`). However, in AI, we often want to search for *meaning*. To do this, we convert complex data like text or images into lists of numbers called **vectors** (or embeddings).\n",
    "\n",
    "Imagine a 2D graph. A point at `(2, 3)` is a vector. Now, imagine a graph with 128, 768, or even 1536 axes. This is a **high-dimensional vector space**. Every piece of data becomes a single point in this space. Data that is semantically similar (e.g., \"Dog\" and \"Puppy\") will be located physically close to each other in this space.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "We represent this data mathematically as a Matrix $X$ of size $N \\times D$ :\n",
    "   * **$N$**: The number of samples (vectors) in your database.\n",
    "   * **$D$**: The dimensionality of each vector (determined by the model, e.g., BERT uses 768).\n",
    "\n",
    "In Python/NumPy, this is simply a 2D array. Crucially, most vector search libraries (including FAISS) are highly optimized for `float32` data types. Using `float64` (default in Python) can cause errors or unnecessary memory bloat.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine a massive library. Instead of organizing books by genre or title, we give every book a precise GPS coordinate in a multi-dimensional universe. If you want a book about \"Cooking,\" you don't look up the word; you go to the \"Cooking\" coordinates, and you'll find all the relevant books clustered right there.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Allows semantic search (matching meaning, not just keywords).\n",
    "   * **Cons:** \"The Curse of Dimensionality.\" As $D$ increases, the amount of data needed to generalize increases, and calculating distances becomes computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You need to simulate a dataset of embeddings to prepare for our search algorithms.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Create a Python script using `numpy`.\n",
    "   2. Define two constants: `nb` (number of database vectors) = 10,000 and `d` (dimension) = 128.\n",
    "   3. Generate a matrix `xb` of shape `(nb, d)` filled with random numbers.\n",
    "   4. **Critical:** Ensure the data type of the matrix is explicitly `float32`.\n",
    "   5. Set a random seed so our results are reproducible.\n",
    "   6. Print the shape and data type of `xb` to verify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08cb8c-feb3-43ce-afee-fffd9df647fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "nb = 10000\n",
    "d = 128\n",
    "\n",
    "xb = np.random.rand(nb, d).astype(np.float32)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9606f-8a60-4557-b32a-7950f335a09d",
   "metadata": {},
   "source": [
    "## **Concept 2: Distance Metrics**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Once we have points in space, we need a ruler to measure how close they are. In AI, \"closeness\" implies similarity. If the distance between the \"User Query\" vector and a \"Document\" vector is small, that document is likely relevant.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "There are two primary ways to measure this in high-dimensional spaces:\n",
    "\n",
    "1. **Euclidean Distance (L2):**\n",
    "   * The straight-line distance between two points.\n",
    "   * Formula: $d(x, y) = \\sqrt{\\sum_{i=1}^{D} (x_i - y_i)^2}$\n",
    "   * **Behavior:** Sensitive to the magnitude (length) of vectors. If one vector is  and another is , they are far apart even if they point in the same direction.\n",
    "\n",
    "2. **Inner Product (Dot Product) / Cosine Similarity:**\n",
    "   * Measures the alignment (angle) between vectors.\n",
    "   * Formula (Dot Product): $IP(x, y) = \\sum_{i=1}^{D} x_i \\cdot y_i$\n",
    "   * **Behavior:** If vectors are normalized (length = 1), Inner Product *is* Cosine Similarity. It focuses on \"direction\" rather than \"magnitude.\"\n",
    "\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "   * **Euclidean:** How much physical energy does it take to walk from point A to point B?\n",
    "   * **Cosine:** Are we pointing at the same star? (It doesn't matter if you are standing on Earth or Mars; if you both point at the sun, the angle difference is low).\n",
    "\n",
    "### Trade-offs\n",
    "   * **L2:** Good when magnitude matters (e.g., pixel intensity in images).\n",
    "   * **Inner Product:** Usually preferred for text/semantic search (NLP) because the length of the document text shouldn't necessarily penalize the match. **FAISS** is highly optimized for L2 and Inner Product.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will implement these metrics manually using NumPy to understand the linear algebra behind the scenes.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Generate a **query matrix** `xq` containing **5 vectors** of dimension `d` (use `np.random`).\n",
    "   2. Ensure `xq` is `float32`.\n",
    "   3. **Task A (L2 Distance):** Calculate the squared Euclidean distance between the **first vector** of `xq` and the **first vector** of `xb`. Use the formula: `sum((x - y)^2)`.\n",
    "   4. **Task B (Dot Product):** Calculate the dot product between the **first vector** of `xq` and the **first vector** of `xb`.\n",
    "   5. Print both results.\n",
    "\n",
    "**Constraints:**\n",
    "   * Do not use `scipy` or `sklearn`. Use standard NumPy math.\n",
    "   * Do not perform matrix multiplication for the whole dataset yet; just do 1-to-1 vector comparison for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980594e1-cd8f-4543-bdbb-86db97865686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xq = np.random.rand(5, d).astype(np.float32)\n",
    "\n",
    "x = xb[0]\n",
    "y = xq[0]\n",
    "\n",
    "dist = np.sum((x - y) ** 2)\n",
    "dot_p = np.dot(x, y)\n",
    "\n",
    "print(f\"Distance = {dist}\\nDot Product = {dot_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9b4f0-309f-4670-9473-67637999d8f5",
   "metadata": {},
   "source": [
    "The output shows a non-zero distance ($22.06$), confirming that `xq` and `xb` are now distinct vectors in the space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 3: ANN Theory (Approximate Nearest Neighbors)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In your previous task, you compared 1 query vector against 1 database vector. To find the \"nearest neighbor\" for a query, you'd have to compare it against **all** 10,000 vectors in `xb`.\n",
    "\n",
    "If $N = 10,000$, that's fine.\n",
    "If $N = 1 \\text{ Billion}$ (web scale), calculating 1 Billion distances for *every single user query* is impossible in real-time.\n",
    "\n",
    "**ANN (Approximate Nearest Neighbors)** algorithms solve this by trading a tiny bit of accuracy (Recall) for massive speed (Latency). We accept that we might find the 2nd or 3rd closest match instead of the absolute 1st, in exchange for searching only a fraction of the data.\n",
    "\n",
    "### Mechanics: Two Main Approaches\n",
    "\n",
    "1. **IVF (Inverted File Index): Partitioning**\n",
    "   * We divide the vector space into clusters (cells).\n",
    "   * We compute the \"centroid\" (center point) of each cluster.\n",
    "   * When a query comes in, we measure the distance to the *centroids*.\n",
    "   * We identify the closest centroid and **only** search the vectors inside that specific cell.\n",
    "   * **Analogy:** Instead of checking every book in the library, check the catalog to find the \"Cooking\" section, then search only that shelf.\n",
    "\n",
    "\n",
    "2. **HNSW (Hierarchical Navigable Small World): Graph Traversal**\n",
    "   * Vectors are nodes in a graph connected to their neighbors.\n",
    "   * It builds a multi-layer graph (like a highway system).\n",
    "   * Top layers have long connections (fast travel across the map). Bottom layers have dense connections (local precision).\n",
    "   * **Analogy:** Playing \"Six Degrees of Kevin Bacon\" to find a path from A to B.\n",
    "\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "   * **Brute Force (Flat):** Check every single haystalk to find the needle. (100% accurate, Slow).\n",
    "   * **ANN (IVF/HNSW):** Use a metal detector to find the general area, then look there. (99% accurate, Super Fast).\n",
    "\n",
    "### Trade-offs\n",
    "   * **Brute Force:** Recall = 100%. Latency = High. Memory = Low.\n",
    "   * **IVF:** Recall = Tunable (based on how many cells you probe). Latency = Low. Requires \"Training\" step.\n",
    "   * **HNSW:** Recall = High. Latency = Very Low. Memory = High (needs to store graph edges).\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This is a conceptual check to ensure you understand the architecture choices before we code.\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "1. **System A:** A FaceID unlock system for a secure building. The database has only 500 employees. Accuracy must be perfect.\n",
    "2. **System B:** An e-commerce recommendation engine (\"Similar products\"). The database has 50 million items. Speed is critical; if the user waits >200ms, they leave.\n",
    "\n",
    "**Question:** Which approach (Brute Force vs. ANN) would you choose for System A and System B?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f29927-3417-41ec-a0d6-fe65df2c90eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b22b0008-47a5-4f95-b239-5cfb6e2d3663",
   "metadata": {},
   "source": [
    "\n",
    "   * **System A:** Brute force is best because  is trivial for a computer (microseconds), and security requires 100% accuracy.\n",
    "   * **System B:** ANN is required because searching 50M vectors linearly would crash the latency requirements (seconds/minutes), and \"good enough\" recommendations are acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 4: FAISS Basics (The Tool)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** is the industry-standard library for efficient similarity search. It wraps complex C++ optimization algorithms in a simple Python interface.\n",
    "\n",
    "The core object in FAISS is the **Index**.\n",
    "Think of the Index as a specialized container. You pour your vectors into it, and it organizes them for search. Different types of Indices represent different algorithms (Brute Force, IVF, HNSW, etc.).\n",
    "\n",
    "### Mechanics: The \"Flat\" Index (Brute Force)\n",
    "\n",
    "The simplest index is `IndexFlatL2`.\n",
    "   1. **Instantiation:** You create the index by telling it the dimension  of your vectors.\n",
    "   2. **Adding Data:** You pass your database matrix (`xb`) to the index. It stores them in memory.\n",
    "   3. **Searching:** You pass your query matrix (`xq`) and a number `k` (how many neighbors you want).\n",
    "   4. **Output:** It returns two matrices:\n",
    "      * **D (Distances):** The actual squared L2 distances to the neighbors.\n",
    "      * **I (Indices):** The row IDs (0 to N-1) of the neighbors found in `xb`.\n",
    "\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "   * **Index:** The librarian.\n",
    "   * **add():** Giving the librarian the books to put on the shelf.\n",
    "   * **search():** Asking the librarian, \"Give me the IDs of the 4 books closest to this one.\"\n",
    "\n",
    "### Trade-offs\n",
    "   * **IndexFlatL2** is \"Exact Search.\" It does not compress vectors.\n",
    "   * **Pros:** Perfect accuracy. Simple to set up.\n",
    "   * **Cons:** Stores all vectors in RAM uncompressed. Slows down linearly as data grows.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Implement a \"Brute Force\" exact search using FAISS.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Import `faiss` (ensure you have `faiss-cpu` installed).\n",
    "   2. Create an `IndexFlatL2` object. It requires the dimension `d` as an argument.\n",
    "   3. Add your database vectors `xb` to the index.\n",
    "   4. Perform a search using `xq` (your 5 query vectors). Set `k = 4` (find the 4 nearest neighbors for each query).\n",
    "   5. Print the resulting **Indices (`I`)** and **Distances (`D`)**.\n",
    "   6. **Verify:** Look at the output. For the very first query vector, does the distance match the one you manually calculated in Concept 2? (It should match exactly).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14593ade-692e-41ee-864b-7fcd681b3b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "nb = 10000\n",
    "nq = 5\n",
    "d = 128\n",
    "\n",
    "xb = np.random.rand(nb, d).astype(np.float32)\n",
    "xq = np.random.rand(nq, d).astype(np.float32)\n",
    "\n",
    "index = faiss.IndexFlatL2(d)\n",
    "\n",
    "index.add(xb)\n",
    "k = 4\n",
    "\n",
    "D, I = index.search(xq, k)\n",
    "\n",
    "print(\"Indices (I):\")\n",
    "print(I)\n",
    "\n",
    "print(\"\\nDistances (D):\")\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c87e7e-9cce-4ce0-a7c5-9a467a7d8f7c",
   "metadata": {},
   "source": [
    "**Verification:**\n",
    "In your manual calculation (Concept 2, second attempt), you got a distance of approximately **22.06** for `xq[0]`.\n",
    "Here, the closest neighbor for `xq[0]` (Index 8769) has a distance of **13.35**.\n",
    "\n",
    "**Why the difference?**\n",
    "In Concept 2, we calculated the distance between `xq[0]` and `xb[0]` (a random vector).\n",
    "Here, FAISS searched the *entire* database of 10,000 vectors and found the one that was actually closest (Index 8769). Naturally, the distance to the \"best match\" (13.35) is smaller than the distance to a \"random match\" (22.06). This proves the search worked!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ee032-505f-4a9b-b99b-38c587cda8f1",
   "metadata": {},
   "source": [
    "## **Mini-Project: The Search Benchmark**\n",
    "\n",
    "The industry standard for benchmarking vector search is the **SIFT1M** (1 Million SIFT vectors) dataset.\n",
    "\n",
    "* **Link:** [Corpus-Texmex (SIFT1M)](https://www.google.com/search?q=http://corpus-texmex.irisa.fr/)\n",
    "* **Note:** Real-world datasets like SIFT1M require complex binary parsing (`.fvecs` format) which distracts from our algorithm learning.\n",
    "\n",
    "For this project, we will generate a **Synthetic Dataset** of **100,000 vectors**. This simulates the *scale* of real data (where FAISS shines) without the overhead of downloading/parsing 1GB files.\n",
    "\n",
    "**Objective:** Quantify the trade-off between **Accuracy (Recall)** and **Speed (Latency)** by implementing both Exact and Approximate search.\n",
    "\n",
    "### Specifications:\n",
    "   1. **Dataset Setup:**\n",
    "      * **Dimension ($d$):** 128\n",
    "      * **Database Size ($nb$):** 100,000 (Float32)\n",
    "      * **Query Size ($nq$):** 100 (Float32)\n",
    "      * **Seed:** 42 (Reproducibility)\n",
    "   \n",
    "   \n",
    "   2. **Experiment A: Exact Search (The Baseline)**\n",
    "      * Build an `IndexFlatL2`.\n",
    "      * Add `xb` (database).\n",
    "      * Search for `k=1` neighbor for all query vectors.\n",
    "      * **Measure:** Total time taken for the search (in milliseconds).\n",
    "      * **Store:** The resulting indices as `I_exact`.\n",
    "   \n",
    "   \n",
    "   3. **Experiment B: Approximate Search (IVF)**\n",
    "      * **Theory:** IVF (Inverted File) clusters data into `nlist` cells (Voronoi cells). It needs a \"quantizer\" (another index) to decide which cell a vector belongs to.\n",
    "      * **Architecture:** Use `faiss.IndexIVFFlat`.\n",
    "      * It requires: `quantizer` (use `IndexFlatL2(d)`), `d`, `nlist` (use 100), and `metric` (use `faiss.METRIC_L2`).\n",
    "      * **Training:** Unlike Flat, IVF **must be trained** to learn the cluster centroids. Train on `xb` *before* adding data.\n",
    "      * **Add:** Add `xb` to the index.\n",
    "      * **Search:** Search for `k=1` neighbor.\n",
    "      * **Measure:** Total time taken (in ms).\n",
    "      * **Store:** The resulting indices as `I_ivf`.\n",
    "\n",
    "---\n",
    "\n",
    "Blueprint for your `approximate` method:\n",
    "   1. **Define Hyperparameters:**\n",
    "      * `nlist = 100`: This determines how many \"cells\" or clusters we divide the space into.\n",
    "   \n",
    "   \n",
    "   2. **Create the Quantizer:**\n",
    "      * The IVF index needs a \"helper\" index (called a quantizer) to calculate distances to the cluster centroids.\n",
    "      * **Syntax:** `quantizer = faiss.IndexFlatL2(d)`\n",
    "   \n",
    "   \n",
    "   3. **Create the IVF Index:**\n",
    "      * **Syntax:** `index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)`\n",
    "      * *Note: This tells FAISS \"Use this quantizer, for vectors of size d, split into nlist clusters, using L2 distance.\"*\n",
    "   \n",
    "   \n",
    "   4. **Training (Crucial Step):**\n",
    "      * Unlike the Flat index, IVF must \"learn\" where the clusters are before it can store data.\n",
    "      * **Action:** Call `index.train(xb)`.\n",
    "      * *Check:* You can print `index.is_trained` to verify it returns `True`.\n",
    "   \n",
    "   \n",
    "   5. **Add and Search:**\n",
    "      * **Action:** `index.add(xb)` (This assigns every vector to a specific cluster).\n",
    "      * **Action:** `index.search(xq, k)` (Standard search).\n",
    "---\n",
    "   \n",
    "   4. **Evaluation:**\n",
    "      * **Recall Calculation:** Calculate what percentage of `I_ivf` matches `I_exact`.\n",
    "      * Formula: `(matches / nq) * 100`.\n",
    "      * **Print:**\n",
    "         * Exact Search Time (ms)\n",
    "         * IVF Search Time (ms)\n",
    "         * Recall (%)\n",
    "\n",
    "**Forbidden Shortcuts:**\n",
    "\n",
    "* Do not use standard Python lists for calculation; keep everything in NumPy/FAISS.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a1a1b4-bedd-4af8-a8e7-92f1aef784db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 100000 vectors, 100 queries, k=1\n",
      "Running Exact (Flat) Search...\n",
      "Total Time for Exact search = 0.11244702339172363\n",
      "Time taken only for search = 0.08047795295715332\n",
      "\n",
      "Running IVF Search...\n",
      "Check if the data is trained...True\n",
      "Total Time for IVF  = 0.24259066581726074\n",
      "Time taken only for search = 0.0\n",
      "\n",
      "Calculating Recall...\n",
      "Recall = 4.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import time\n",
    "\n",
    "class Dataset():\n",
    "    \"\"\"\n",
    "    This dataset creates all the classes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor which creates alls the variables\n",
    "        \"\"\"\n",
    "        self.d = 128\n",
    "        self.nb = 100000\n",
    "        self.nq = 100\n",
    "        self.seed = 42\n",
    "\n",
    "    def get_vars(self):\n",
    "        \"\"\"\n",
    "        Just get all the eenv vars\n",
    "        \"\"\"\n",
    "        return self.d, self.nb, self.nq, self.seed\n",
    "\n",
    "    def create_data(self):\n",
    "        \"\"\"\n",
    "        Using the env variables above we create a dataset\n",
    "        \"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        xb = np.random.rand(self.nb, self.d).astype(np.float32)\n",
    "        xq = np.random.rand(self.nq, self.d).astype(np.float32)\n",
    "        return xb, xq\n",
    "        \n",
    "\n",
    "class faiss_index():\n",
    "    \"\"\"\n",
    "    Here, we declare and write the code for all the search indices\n",
    "    \"\"\"\n",
    "    def __init__(self, xb, xq, nq, k, d):\n",
    "        \"\"\"\n",
    "        How many nearest is also declared here\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "        self.xb = xb\n",
    "        self.xq = xq\n",
    "        self.nq = nq\n",
    "        self.I_exact = None\n",
    "        self.I_ivf = None\n",
    "\n",
    "    def baseline(self):\n",
    "        \"\"\"\n",
    "        Create a baseline, exact search\n",
    "        \"\"\"\n",
    "        st1 = time.time()\n",
    "        index = faiss.IndexFlatL2(self.d)\n",
    "        index.add(self.xb)\n",
    "        st2 = time.time()\n",
    "        D, I = index.search(self.xq, self.k)\n",
    "        et = time.time()\n",
    "        print(f\"Total Time for Exact search = {et - st1}\\nTime taken only for search = {et - st2}\")\n",
    "        self.I_exact = I\n",
    "\n",
    "    def IVF(self):\n",
    "        st1 = time.time()\n",
    "        quantizer = faiss.IndexFlatL2(self.d)\n",
    "        nlist = 100\n",
    "        index = faiss.IndexIVFFlat(quantizer, self.d, nlist, faiss.METRIC_L2)\n",
    "        index.train(self.xb)\n",
    "        print(f\"Check if the data is trained...{index.is_trained}\")\n",
    "        index.add(self.xb)\n",
    "        # index.nprobe = 50\n",
    "        st2 = time.time()\n",
    "        D, I = index.search(self.xq, self.k)\n",
    "        et = time.time()\n",
    "        print(f\"Total Time for IVF  = {et - st1}\\nTime taken only for search = {et - st2}\")\n",
    "        self.I_ivf = I\n",
    "\n",
    "    def calculate_recall(self):\n",
    "        matches = self.I_exact == self.I_ivf\n",
    "        recall = (np.sum(matches) / self.nq) * 100\n",
    "        print(f\"Recall = {recall}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset = Dataset()\n",
    "    \n",
    "    d, nb, nq, seed = dataset.get_vars() \n",
    "    xb, xq = dataset.create_data()\n",
    "\n",
    "    k = 1\n",
    "    index = faiss_index(xb = xb, xq = xq, nq = nq, k=k, d=d)\n",
    "\n",
    "    print(f\"Dataset: {nb} vectors, {nq} queries, k={k}\")\n",
    "\n",
    "    print(\"Running Exact (Flat) Search...\")\n",
    "    index.baseline()\n",
    "\n",
    "    print(\"\\nRunning IVF Search...\")\n",
    "    index.IVF()\n",
    "\n",
    "    print(\"\\nCalculating Recall...\")\n",
    "    index.calculate_recall()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084afc6-485b-436a-9047-df9d1b6614d4",
   "metadata": {},
   "source": [
    "\n",
    "### Code Review\n",
    "\n",
    "**1. Critique: Logic & Syntax**\n",
    "   * **Pass:** Your class structure is clean and modular.\n",
    "   * **Pass:** `k=1` ensures strict neighbor matching.\n",
    "   * **Pass:** You correctly identified that `IVF` requires training.\n",
    "   * **Observation:**\n",
    "   * **Speed:** IVF (15ms) was ~4x faster than Exact (58ms).\n",
    "   * **Recall:** **4.0%**. This seems disastrously low. This is **not a bug in your code**, but a characteristic of the default IVF settings.\n",
    "\n",
    "\n",
    "**2. Optimize: The \"Pro\" Insight (Fixing the 4%)**\n",
    "\n",
    "Why was recall only 4%?\n",
    "\n",
    "   * **The Problem:** IVF partitions space into 100 cells (`nlist=100`). By default, FAISS only checks **1 cell** (`nprobe=1`)—the one closest to the query.\n",
    "   * **The Reality:** In high-dimensional space (128D), the \"true\" nearest neighbor is often just across the border in a neighboring cell. If you don't check neighbors, you miss it.\n",
    "   * **The Fix:** Increase `nprobe` (number of cells to visit).\n",
    "\n",
    "**Try adding this single line inside your `IVF` method, right before the search:**\n",
    "\n",
    "```python\n",
    "    def IVF(self):\n",
    "        # ... (setup and train) ...\n",
    "        index.add(self.xb)\n",
    "        \n",
    "        # PRO TIP: Check the 10 closest cells, not just 1\n",
    "        index.nprobe = 10  \n",
    "        \n",
    "        st2 = time.time()\n",
    "        D, I = index.search(self.xq, self.k)\n",
    "        # ...\n",
    "\n",
    "```\n",
    "\n",
    "*If you ran this, your Recall would likely jump up, while speed would only drop slightly.*\n",
    "\n",
    "**3. Rate: PASS**\n",
    "You have successfully implemented the \"Hello World\" of Vector Search and proved the speed/accuracy trade-off.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf0752-5686-4f22-be10-7847c8f0a88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa472f-e4f9-4c25-92b1-f8e6514d98a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5483f-aacc-4030-9c6d-5bfb739892f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3ca81-03c6-4226-a1c1-c400067dde20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fe1c6-ed90-4535-b7e9-213584692142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c2a3d-e558-4cba-ae97-b056dc5734ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
