{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6edde21c-1a2f-4ce8-8f89-f547b01f3272",
   "metadata": {},
   "source": [
    "# **Vector Databases & RAG**. \n",
    "\n",
    "Today, we focus on the \"Storage\" layer of the RAG (Retrieval-Augmented Generation) pipeline. You cannot feed a 500-page PDF into an LLM context window efficiently. Instead, we must break it down, convert it into mathematical representations (vectors), and store it in a database optimized for similarity search, not exact keyword matching.\n",
    "\n",
    "Here is the roadmap to build your local Knowledge Base ingestion engine.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "```text\n",
    "L18: Vector Databases & Ingestion\n",
    "├── Concept 1: Vector Database Architecture (ChromaDB)\n",
    "│   ├── Dense Vectors vs. Sparse Vectors\n",
    "│   ├── Indexing (HNSW) vs. Exact Search\n",
    "│   ├── Intuition: High-dimensional space navigation\n",
    "│   ├── Simpler Terms: A library organized by \"meaning\" rather than \"alphabet\"\n",
    "│   └── Task: Initialize a persistent ChromaDB client\n",
    "│\n",
    "├── Concept 2: Document Loading (PDF Parsing)\n",
    "│   ├── Unstructured Data Extraction\n",
    "│   ├── Intuition: Turning binary PDF format into raw string data\n",
    "│   └── Task: Extract raw text from a PDF file using a library (e.g., pypdf)\n",
    "│\n",
    "├── Concept 3: Text Splitting (RecursiveCharacterTextSplitter)\n",
    "│   ├── Context Window Constraints\n",
    "│   ├── Semantic boundary preservation\n",
    "│   ├── Chunk Overlap intuition\n",
    "│   └── Task: Implement the splitting logic with overlap\n",
    "│\n",
    "├── Concept 4: Embedding Generation (The Bridge)\n",
    "│   ├── The role of the Embedding Model (e.g., OpenAI, HuggingFace)\n",
    "│   ├── Input (Text) -> Output (List of Floats)\n",
    "│   └── Task: Generate dummy or real embeddings for a text sample\n",
    "│\n",
    "├── Concept 5: Ingestion (Collections & Upsert)\n",
    "│   ├── Collections (Tables equivalent)\n",
    "│   ├── IDs, Embeddings, Documents, and Metadata association\n",
    "│   └── Task: Insert chunks into the ChromaDB collection\n",
    "│\n",
    "├── Concept 6: Metadata Filtering\n",
    "│   ├── Pre-filtering vs. Post-filtering\n",
    "│   ├── Hybrid search basics\n",
    "│   └── Task: Query the DB with a specific metadata constraint\n",
    "│\n",
    "└── Mini-Project: PDF Knowledge Base Builder\n",
    "    └── Build a script that takes a PDF path, processes it, and makes it searchable.\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85b4c7-dba0-47d3-9aba-9a22e06b3222",
   "metadata": {},
   "source": [
    "## Concept 1: Vector Database Architecture (ChromaDB)\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Traditional relational databases (SQL) are designed for precision. If you query `SELECT * FROM items WHERE color = 'red'`, the database checks for an exact string match. It either is \"red\" or it isn't.\n",
    "\n",
    "However, language is messy. \"Crimson\", \"Ruby\", and \"Scarlet\" are all semantically similar to \"Red\", but an SQL query would miss them. Vector Databases are designed to solve this. Instead of storing data as just text or numbers, they store data as **Vectors** (long lists of floating-point numbers) in a multi-dimensional space.\n",
    "\n",
    "In this space, concepts with similar meanings are located physically close to each other. Searching involves finding the \"Nearest Neighbors\" to a query vector, rather than exact row matching.\n",
    "\n",
    "### Mechanics: HNSW\n",
    "\n",
    "If you have 1 million vectors, calculating the distance between your query and every single vector (linear scan) is too slow for production.\n",
    "ChromaDB (and others like Weaviate/Pinecone) use **Approximate Nearest Neighbor (ANN)** algorithms. The most common is **HNSW (Hierarchical Navigable Small World)**.\n",
    "   * **Structure:** It builds a multi-layer graph. Top layers act like express highways (long jumps across the data), while bottom layers allow for fine-grained navigation.\n",
    "   * **Process:** The search starts at the top, zooms in on the general neighborhood, and descends until it finds the closest points in the bottom layer.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine a massive library organized by \"Vibes\" instead of the Dewey Decimal System.\n",
    "   * **SQL** is like looking up a book by its exact ISBN number. You either find it or you don't.\n",
    "   * **Vector Search** is like saying, \"I want a book that feels like 'Harry Potter'\". The librarian (the Algorithm) walks to the Fantasy section (express jump), then looks at the shelf next to Rowling (local search) and hands you \"Percy Jackson\". It’s not the exact same book, but it's the closest match in meaning.\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Pros:** Enables semantic search (searching by meaning, not keywords).\n",
    "* **Cons:** It is **Approximate**. There is a small chance the algorithm misses the absolute \"closest\" vector in favor of speed. It is also computationally heavier than simple keyword lookups.\n",
    "\n",
    "### Context\n",
    "\n",
    "In RAG (Retrieval-Augmented Generation), we use the Vector DB to find the 3-5 paragraphs from a PDF that are most relevant to the user's question, then send only those paragraphs to the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You need to set up the infrastructure for our local knowledge base.\n",
    "\n",
    "**Requirements:**\n",
    "   1. Import the `chromadb` library.\n",
    "   2. Create a class named `VectorStore`.\n",
    "   3. In the `__init__` method, initialize a **Persistent Client**. This ensures that if you restart the script, the data isn't lost.\n",
    "      * *Hint:* You need to specify a path where the data will be saved.\n",
    "   4. Create (or get) a **Collection** named `my_knowledge_base`. A Collection is roughly equivalent to a Table in SQL.\n",
    "   \n",
    "   **Inputs:** None (Hardcoded path for now is fine, e.g., `./chroma_db`)\n",
    "   \n",
    "   **Outputs:** Print the collection object or its name to confirm creation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225224b7-cd62-4409-a568-4e38b126e1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path = r\"Data/\")\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        collection = self.client.get_or_create_collection(name = \"my_knowledge_base\")\n",
    "        return collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9986ce-b224-4d2e-a66c-ffdf0deef892",
   "metadata": {},
   "source": [
    "This setup ensures your data survives between runs, which is critical for the \"Build Once, Query Many\" pattern of RAG applications.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 2: Document Loading (PDF Parsing)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "A PDF file is not a simple stream of text like a `.txt` file. It is a set of instructions for a printer (e.g., \"draw character 'A' at coordinates x:50, y:100\").\n",
    "\n",
    "Because of this, extracting text from a PDF is often \"messy.\" You lose structural elements like columns, headers, or tables unless you use specialized OCR or layout-aware parsers. For basic RAG, we simply need to extract the raw text content so we can process it.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "We use libraries (like `pypdf`, `PyMuPDF`, or `pdfplumber`) to iterate through the binary pages of the file and attempt to reconstruct the string of text contained within.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine a PDF is a painting of a letter. You can't just copy-paste the paint. You need a tool that looks at the painting and writes down the words it sees into a notepad (String).\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Allows us to unlock the vast amount of knowledge stored in business documents.\n",
    "   * **Cons:** Formatting is often lost. \"Page numbers\", \"headers\", and \"footers\" get mixed in with the actual content, which can confuse the AI later if not cleaned (though for this exercise, we will stick to raw extraction).\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Add a method `load_pdf` to your `VectorStore` class (or keep it as a standalone helper function, your choice, but helper function is usually cleaner for ingestion scripts).\n",
    "\n",
    "**Specifications:**\n",
    "   1. Input: A file path string (e.g., `\"sample.pdf\"`).\n",
    "   2. Logic:\n",
    "      * Open the file in binary read mode (`rb`).\n",
    "      * Use a PDF library (standard choice is `pypdf` or `PyPDF2`) to read the file.\n",
    "      * Iterate through every page.\n",
    "      * Extract text from the page and append it to a single result string.\n",
    "   3. Output: Return the full text of the PDF as one long string.\n",
    "\n",
    "**Note:** If you don't have a specific PDF library installed, you might need to `pip install pypdf` first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb2b3a-e6d0-4d57-9b28-f0c6ba318bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path = r\"Data/\")\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        collection = self.client.get_or_create_collection(name = \"my_knowledge_base\")\n",
    "        return collection\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        \"\"\"\n",
    "        Loads all the text of the pdf into a string. Splits each page with a \\n\\n\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "        return \"\\n\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12009bc3-bb57-4dd1-aa58-f18636789401",
   "metadata": {},
   "source": [
    "\n",
    "## **Concept 3: Text Splitting (Recursive Strategies)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "You cannot feed a whole 50-page PDF into an LLM for two reasons:\n",
    "   1. **Token Limits:** LLMs have a maximum context window.\n",
    "   2. **Precision:** If you ask \"What is the revenue?\", you want the specific *paragraph* about revenue, not the entire annual report. Feeding the whole document dilutes the \"signal\" with too much \"noise.\"\n",
    "\n",
    "We must break the text into smaller **Chunks**.\n",
    "\n",
    "### Mechanics: Recursive Character Splitting\n",
    "\n",
    "We don't just chop the text every 1000 characters blindly. If we did, we might cut a sentence in half:\n",
    "   * *Chunk 1:* \"...the revenue was $5\"\n",
    "   * *Chunk 2:* \"million.\"\n",
    "This destroys the meaning.\n",
    "\n",
    "**Recursive Splitting** tries to keep related text together:\n",
    "   1. First, try splitting by **Paragraphs** (`\\n\\n`).\n",
    "   2. If a paragraph is still too big, split it by **Lines** (`\\n`).\n",
    "   3. If a line is too big, split by **Spaces** (` `).\n",
    "   4. Finally, force a split by characters.\n",
    "\n",
    "### The \"Overlap\"\n",
    "\n",
    "We also add an **Overlap** (e.g., 100 characters). The end of Chunk 1 is repeated as the start of Chunk 2.\n",
    "   * *Reason:* This ensures that if a concept spans across the cut, the context is preserved in at least one of the chunks.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "For this exercise, we will implement a simplified **Sliding Window Splitter** manually to understand the math of \"Overlap\" (building a full recursive splitter is complex regex work).\n",
    "\n",
    "**Specifications:**\n",
    "   1. Add a method `split_text` to your class (or as a helper).\n",
    "   2. **Inputs:** `text` (str), `chunk_size` (int, default 1000), `chunk_overlap` (int, default 200).\n",
    "   3. **Logic:**\n",
    "      * Create a loop that slices the text.\n",
    "      * Start at index `0`.\n",
    "      * Slice from `start` to `start + chunk_size`.\n",
    "      * Move the start index forward by `chunk_size - chunk_overlap`.   \n",
    "   4. **Output:** Return a list of text chunks (List[str]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea2942-6d9f-4c03-b057-d4a693a35542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path = r\"Data/\")\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        collection = self.client.get_or_create_collection(name = \"my_knowledge_base\")\n",
    "        return collection\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        \"\"\"\n",
    "        Loads all the text of the pdf into a string. Splits each page with a \\n\\n\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "        return \"\\n\\n\".join(texts)\n",
    "\n",
    "    def split_text(self, text, chunk_size = 1000, chunk_overlap = 200):\n",
    "        \"\"\"\n",
    "        Sliding Window Splitter manually to understand the math of \"Overlap\"\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start: start + chunk_size])\n",
    "            start += chunk_size - chunk_overlap\n",
    "        return chunks\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bf8fd-b3f2-4b6c-af18-a00bfa59c8f9",
   "metadata": {},
   "source": [
    "The `while` loop gives you precise control over the index, ensuring the overlap is calculated correctly (jumping forward by `chunk_size - overlap`).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 4: Embedding Generation (The Bridge)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Computers cannot understand the string `\"Revenue increased\"`. They only understand numbers.\n",
    "An **Embedding Model** acts as a translator. It accepts text and outputs a fixed-length list of floating-point numbers (a vector).\n",
    "   * **Input:** \"Apple\"\n",
    "   * **Output:** `[0.12, -0.98, 0.05, ...]` (e.g., 384 dimensions)\n",
    "\n",
    "Crucially, this translation is **semantic**.\n",
    "   * The vector for \"Apple\" will be mathematically closer to \"Banana\" (both fruits) than to \"Microsoft\" (tech company).\n",
    "   * However, \"Apple\" (the company) would be closer to \"Microsoft\" based on context. High-quality models capture this nuance.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "We typically use pre-trained Transformers (like BERT or RoBERTa). We feed the text in, and instead of asking for a classification (Cat/Dog), we intercept the numbers at the **last hidden layer**. That array of numbers *is* the embedding.\n",
    "\n",
    "For this lesson, we will use ChromaDB's built-in default utility, which uses the `all-MiniLM-L6-v2` model (a very fast, lightweight model).\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Before we ingest data, I want you to \"see\" a vector to demystify it.\n",
    "   1. Import `embedding_functions` from `chromadb.utils`.\n",
    "   2. Instantiate a `DefaultEmbeddingFunction`.\n",
    "   3. Run this function on the string `\"Hello world\"` and print the **length** of the resulting vector (to see the dimensionality) and the **first 5 numbers**.\n",
    "\n",
    "*Note: This might download a small model file on the first run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8075f-621e-4778-b1cb-07f78dfb0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils.embedding_functions import DefaultEmbeddingFunction\n",
    "\n",
    "embed = DefaultEmbeddingFunction()\n",
    "text_embed = embed(\"Hello World\")\n",
    "text_embed[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9b4486-e5b2-42d7-a3d4-9f46621bde19",
   "metadata": {},
   "source": [
    "That confirms your environment is set up correctly. The default model (`all-MiniLM-L6-v2`) produces 384-dimensional vectors. This means every piece of text you ingest becomes a point in a 384-dimensional coordinate system.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 5: Ingestion (Collections & Upsert)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Now we combine everything. Ingestion is the pipeline of:\n",
    "**Raw PDF  Text String  Chunks  Vectors  Storage.**\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "ChromaDB's `collection.add()` method handles the heavy lifting.\n",
    "You provide:\n",
    "   1. **Documents:** The list of text chunks (strings).\n",
    "   2. **IDs:** A unique identifier for each chunk (e.g., \"pdf1_chunk0\", \"pdf1_chunk1\").\n",
    "   3. **Embeddings:** (Optional) If you don't provide them, Chroma runs the default embedding function (from Concept 4) automatically on the documents.\n",
    "   4. **Metadatas:** (Optional) Dictionaries carrying extra info (e.g., `{\"source\": \"annual_report.pdf\", \"page\": 10}`).\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "This is the \"Data Entry\" phase. We are taking the messy pile of paper (PDF), cutting it into index cards (Chunks), writing a summary number on the back (Vector), and filing them into the cabinet (Collection).\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Update your `VectorStore` class by adding a method `ingest_pdf`.\n",
    "\n",
    "**Specifications:**\n",
    "   1. **Input:** `pdf_path` (str).\n",
    "   2. **Workflow:**\n",
    "      * Call `load_pdf` to get the raw text.\n",
    "      * Call `split_text` to get the list of chunks.\n",
    "      * **Generate IDs:** Create a list of unique IDs matching the number of chunks (e.g., using `f\"id_{i}\"` in a loop or comprehension).\n",
    "      * **Add to DB:** Call `self.collection.add` with `documents` and `ids`.\n",
    "      * *Note:* You might need to ensure `self.collection` is defined in your `__init__` or called via `get_or_create_collection` before adding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01d85c8-193a-4c3d-bb92-518b5767ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "import os\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path = r\"Data/\")\n",
    "        self.collection = None\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        collection = self.client.get_or_create_collection(name = \"my_knowledge_base\")\n",
    "        self.collection = collection\n",
    "        return collection\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        \"\"\"\n",
    "        Loads all the text of the pdf into a string. Splits each page with a \\n\\n\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "        return \"\\n\\n\".join(texts)\n",
    "\n",
    "    def split_text(self, text, chunk_size = 1000, chunk_overlap = 200):\n",
    "        \"\"\"\n",
    "        Sliding Window Splitter manually to understand the math of \"Overlap\"\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start: start + chunk_size])\n",
    "            start += chunk_size - chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def ingest_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Full ingestion pipeline:\n",
    "        - Load PDF\n",
    "        - Split into chunks\n",
    "        - Generate IDs\n",
    "        - Add to ChromaDB\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"collection\") or self.collection is None:\n",
    "            self.collection = self.get_or_create_collection()\n",
    "        \n",
    "        all_texts = self.load_pdf(pdf_path)\n",
    "        chunks = self.split_text(all_texts)\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        ids = [f\"F_{filename}_id_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        self.collection.add(documents = chunks, ids = ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfbc7f-f28e-436f-84ca-a58d93779373",
   "metadata": {},
   "source": [
    "**Alternative ID strategies:**\n",
    "Instead of F_{filename}_id_{i}, you can use UUIDs or hashes.\n",
    "   * UUIDs (uuid.uuid4()) guarantee uniqueness across all ingestions and are safest at scale, but they’re not human-readable and make debugging harder.\n",
    "   * Hashes (e.g., hashlib.md5(chunk_text)) produce deterministic IDs (same text → same ID), which helps avoid re-ingesting identical chunks, but they have a small collision risk and depend on chunk content staying identical.\n",
    "   * Filename + index (your current approach) is readable and great for learning/debugging, but requires care to avoid re-ingesting the same file twice.\n",
    "\n",
    "This is functional and robust. You have successfully built a pipeline that goes from Disk $\\rightarrow$ Memory $\\rightarrow$ Vector Database.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 6: Metadata Filtering**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Imagine you ingest 100 different PDFs—HR policies, IT manuals, and Financial Reports.\n",
    "If a user asks \"What is the refund policy?\", the Vector Search might return a snippet from the *IT manual* about \"refunding software licenses\" when the user actually wanted the *HR policy* on travel expenses.\n",
    "\n",
    "**Metadata Filtering** allows you to narrow the search space *before* (or sometimes after) the vector comparison. You can say: \"Only search for vectors WHERE `category == 'HR'`\".\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "When you add data to ChromaDB, you can attach a dictionary to every chunk:\n",
    "   `metadata={\"source\": \"hr_policy.pdf\", \"year\": 2024}`.\n",
    "\n",
    "When you query, you pass a filter dictionary:\n",
    "   `where={\"source\": \"hr_policy.pdf\"}`.\n",
    "\n",
    "\n",
    "#### Mechanics: Querying\n",
    "\n",
    "To ask the database a question, you use the `.query()` method. It automatically converts your text into a vector using the same model used during ingestion.\n",
    "\n",
    "**Syntax:**\n",
    "   ```python\n",
    "   results = collection.query(\n",
    "       query_texts=[\"Your question here\"],  # Must be a list, even for one question\n",
    "       n_results=5,                         # How many matches to return\n",
    "       where={\"source\": \"filename.pdf\"}     # Optional: Metadata filter\n",
    "   )\n",
    "   \n",
    "   ```\n",
    "\n",
    "**The Output:**\n",
    "The `results` variable is a dictionary containing lists. Since you can pass multiple query texts, the results are lists of lists.\n",
    "   * `results['documents'][0]` -> List of the matching text chunks.\n",
    "   * `results['distances'][0]` -> List of similarity scores (lower is usually better/closer in Chroma's default metric).\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "   1. **Modify `ingest_pdf`**: Update the `self.collection.add` call to include metadata.\n",
    "      * Create a list of metadata dictionaries.\n",
    "      * Each dictionary should look like: `{\"source\": filename}`.\n",
    "      * The list must have the same length as `chunks` (one dict per chunk).\n",
    "   \n",
    "   \n",
    "   2. **Add `query` method**: Create a method `query_db(query_text, n_results=5, filter_dict=None)`.\n",
    "      * It calls `self.collection.query`.\n",
    "      * **Inputs:** `query_texts=[query_text]`, `n_results=n_results`.\n",
    "      * **Conditional:** If `filter_dict` is provided, pass it to the `where` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c9b616-a3df-4677-ba09-2887e6209239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "import os\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path = r\"Data/\")\n",
    "        self.collection = None\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        collection = self.client.get_or_create_collection(name = \"my_knowledge_base\")\n",
    "        self.collection = collection\n",
    "        return collection\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        \"\"\"\n",
    "        Loads all the text of the pdf into a string. Splits each page with a \\n\\n\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "        return \"\\n\\n\".join(texts)\n",
    "\n",
    "    def split_text(self, text, chunk_size = 1000, chunk_overlap = 200):\n",
    "        \"\"\"\n",
    "        Sliding Window Splitter manually to understand the math of \"Overlap\"\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start: start + chunk_size])\n",
    "            start += chunk_size - chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def ingest_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Full ingestion pipeline:\n",
    "        - Load PDF\n",
    "        - Split into chunks\n",
    "        - Generate IDs\n",
    "        - Add to ChromaDB\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"collection\") or self.collection is None:\n",
    "            self.collection = self.get_or_create_collection()\n",
    "        \n",
    "        all_texts = self.load_pdf(pdf_path)\n",
    "        chunks = self.split_text(all_texts)\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        \n",
    "        ids = [f\"F_{filename}_id_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        metadatas = [{\"source\" : filename} for _ in range(len(chunks))]\n",
    "\n",
    "        self.collection.add(documents = chunks, ids = ids, metadatas = metadatas)\n",
    "\n",
    "\n",
    "    def query_db(self, query_text, n_results = 5, filter_dict = None):\n",
    "        \"\"\"\n",
    "        It calls self.collection.query.\n",
    "        Inputs: query_texts=[query_text], n_results=n_results.\n",
    "        Conditional: If filter_dict is provided, pass it to the where parameter.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"collection\") or self.collection is None:\n",
    "            self.collection = self.get_or_create_collection()\n",
    "    \n",
    "        query = {\n",
    "            \"query_texts\" : [query_text],\n",
    "            \"n_results\" : n_results\n",
    "        }\n",
    "    \n",
    "        if filter_dict is not None:\n",
    "            query[\"where\"] = filter_dict\n",
    "    \n",
    "        return self.collection.query(**query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b2722-5709-47b4-abc7-ecfd43ea5fa4",
   "metadata": {},
   "source": [
    "If you use `self.collection.query(query)` you get a **Function Argument Error**: You are calling self.collection.query(query).\n",
    "    \n",
    "The query() method expects separate arguments like query_texts=[\"...\"], n_results=5, etc.\n",
    "    \n",
    "It does not accept a single dictionary object as the first input.\n",
    "\n",
    "Fix: You either need to pass the arguments explicitly (e.g., query(query_texts=..., n_results=...)) OR use Python's \"dictionary unpacking\" operator (**) to convert your dictionary keys into function arguments.\n",
    "\n",
    "---\n",
    "\n",
    "You now have a fully functional Vector Store class that can:\n",
    "   1. Persist data to disk.\n",
    "   2. Parse PDFs.\n",
    "   3. Split text with overlap.\n",
    "   4. Ingest data with metadata.\n",
    "   5. Query with filters.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "**Objective:** Combine all your previous work into a single script that acts as a command-line tool. You will ingest a PDF and then ask questions about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ec00cd-f25c-40ea-b6b4-de7ed430d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    vdb = VectorStore()\n",
    "    pdf = r\"Data\\Test.pdf\"\n",
    "    col = vdb.get_or_create_collection()\n",
    "    texts = vdb.load_pdf(pdf)\n",
    "    chunks = vdb.split_text(texts)\n",
    "    vdb.ingest_pdf(pdf)\n",
    "    result = vdb.query_db(\"Who is RAG?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd100e94-96d4-4e51-83a6-01ac1e2f2136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['F_Test.pdf_id_3',\n",
       "   'F_Test.pdf_id_9',\n",
       "   'F_Test.pdf_id_8',\n",
       "   'F_Test.pdf_id_6',\n",
       "   'F_Test.pdf_id_7']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['tion is an architectural pattern that enhances text \\ngeneration by retrieving relevant external information at inference time and \\nincorporating it into the model’s response. \\nIn simple terms: \\nRAG = Retrieve relevant documents → Generate an answer using both the query and \\nthe retrieved content \\nInstead of asking a language model to answer a question from memory alone, a RAG \\nsystem first looks up relevant information from a database, document store, or \\nknowledge base. The retrieved information is then provided as context to the language \\nmodel, which uses it to produce a more accurate and grounded answer. \\n \\n4. Core Components of a RAG System \\nA typical RAG pipeline consists of several key components: \\n4.1 Knowledge Source \\nThis is the external data repository that the system retrieves from. It can include: \\n• PDFs, Word documents, or manuals \\n• Websites or wikis \\n• Databases \\n• Internal company documentation \\n• Research papers \\n• Logs or structured records \\n4.2 Chunking and Indexin',\n",
       "   's and Limitations of RAG \\nDespite its advantages, RAG is not without challenges. \\n9.1 Retrieval Quality \\nIf the retriever fails to find relevant documents, the generated answer will still be poor. \\n“Garbage in, garbage out” applies strongly. \\n9.2 Context Window Limits \\nLLMs have limits on how much text they can process at once. Selecting the most \\nrelevant chunks is critical. \\n9.3 Latency \\nRetrieval adds an extra step, which can increase response time compared to a \\nstandalone model. \\n9.4 Data Maintenance \\nKnowledge bases must be curated, updated, and cleaned to avoid outdated or \\nconflicting information. \\n9.5 Security and Privacy \\n\\nWhen using sensitive data, access control and data isolation become essential to \\nprevent leakage. \\n \\n10. Best Practices for Building RAG Systems \\n• Use high-quality embeddings to improve semantic retrieval. \\n• Chunk documents intelligently to preserve meaning. \\n• Limit retrieved context to the most relevant passages. \\n• Include citations in outputs when po',\n",
       "   'ants \\nEmployees can query internal documents, policies, and procedures using natural \\nlanguage. RAG ensures answers reflect official documentation. \\n8.2 Customer Support \\nRAG systems can retrieve product manuals, FAQs, and troubleshooting guides to \\nprovide accurate and consistent support responses. \\n8.3 Healthcare and Life Sciences \\nClinicians and researchers can query medical literature, treatment guidelines, or \\npatient records (with proper safeguards) to receive evidence-based answers. \\n8.4 Legal Research \\nLawyers can ask questions about case law, statutes, or contracts and receive answers \\ngrounded in specific legal documents. \\n8.5 Education and Research \\nStudents and researchers can query textbooks, lecture notes, or academic papers to \\nget context-aware explanations. \\n \\n9. Challenges and Limitations of RAG \\nDespite its advantages, RAG is not without challenges. \\n9.1 Retrieval Quality \\nIf the retriever fails to find relevant documents, the generated answer will still be poor. \\n“G',\n",
       "   ' \\ncontext. \\n6. Response \\nThe user receives a grounded, context-aware answer that reflects the actual \\nsource documents. \\n \\n6. Why RAG Models Help \\n\\nRAG models provide several major advantages over standalone language models. \\n6.1 Improved Accuracy and Reduced Hallucinations \\nBecause the model is given relevant source material, it is far less likely to fabricate \\ninformation. Answers are grounded in real documents rather than guesses. \\n6.2 Up-to-Date Information \\nRAG systems can retrieve from continuously updated data sources. This allows models \\nto answer questions about recent events or updated policies without retraining. \\n6.3 Domain Adaptability \\nRAG makes it easy to adapt a general-purpose LLM to a specialized domain simply by \\nchanging the underlying data source. No fine-tuning is required. \\n6.4 Cost Efficiency \\nUpdating a knowledge base is far cheaper than retraining or fine-tuning a large model. \\nThis makes RAG ideal for enterprise use. \\n6.5 Transparency and Trust \\nMany RAG syst',\n",
       "   'uired. \\n6.4 Cost Efficiency \\nUpdating a knowledge base is far cheaper than retraining or fine-tuning a large model. \\nThis makes RAG ideal for enterprise use. \\n6.5 Transparency and Trust \\nMany RAG systems can return citations or references showing where the information \\ncame from. This improves user trust and enables verification. \\n \\n7. RAG vs Fine-Tuning \\nRAG and fine-tuning are often compared, but they solve different problems. \\nAspect RAG Fine-Tuning \\nKnowledge updates Instant Requires retraining \\nCost Lower Higher \\nExplainability High (with sources) Low \\nDomain adaptation Flexible Rigid \\nLatency Slightly higher Lower \\nIn practice, many systems use both: fine-tuning for behavior and style, and RAG for \\nfactual grounding. \\n \\n8. Real-World Use Cases of RAG \\n\\n8.1 Enterprise Knowledge Assistants \\nEmployees can query internal documents, policies, and procedures using natural \\nlanguage. RAG ensures answers reflect official documentation. \\n8.2 Customer Support \\nRAG systems can retrieve prod']],\n",
       " 'uris': None,\n",
       " 'included': ['metadatas', 'documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': [[{'source': 'Test.pdf'},\n",
       "   {'source': 'Test.pdf'},\n",
       "   {'source': 'Test.pdf'},\n",
       "   {'source': 'Test.pdf'},\n",
       "   {'source': 'Test.pdf'}]],\n",
       " 'distances': [[0.9664498567581177,\n",
       "   1.098168134689331,\n",
       "   1.1430026292800903,\n",
       "   1.1740329265594482,\n",
       "   1.3357794284820557]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = vdb.query_db(\"What is the adnvantages of RAG?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcee8c3-8ced-4a0b-949d-b404faa1eb16",
   "metadata": {},
   "source": [
    "## **Mini-Project: The \"Smart Librarian\"**\n",
    "\n",
    "**Objective:** Upgrade your `VectorStore` to a production-grade **Corpus Manager**. It must identify files by their *content* (not just filename), prevent duplicate ingestion, and allow for removal of obsolete documents.\n",
    "\n",
    "**The Golden Rule:** The database state must never contain duplicate chunks for the same file content.\n",
    "\n",
    "---\n",
    "\n",
    "### Specifications\n",
    "\n",
    "#### 1. The Hashing Engine\n",
    "\n",
    "Implement a mechanism to fingerprint files.\n",
    "   * **Tool:** `hashlib.md5`.\n",
    "   * **Requirement:** Create a method `_get_file_hash(self, filepath)` that reads a file in binary mode and returns its hexadecimal digest string.\n",
    "   * **Why?** If I rename `report.pdf` to `report_final.pdf`, the hash remains identical. The system should recognize they are the same file.\n",
    "\n",
    "#### 2. The Duplicate Guard (Logic Upgrade)\n",
    "\n",
    "Modify your `ingest_pdf` method to be \"Idempotent\" (safe to run multiple times).\n",
    "   * **Step A:** Calculate the hash of the incoming PDF.\n",
    "   * **Step B:** Query the database to check if this hash already exists.\n",
    "      * *Hint:* You can use `self.collection.get(where={\"file_hash\": current_hash})`. Check if the returned list of IDs is empty or not.\n",
    "   * **Step C:**\n",
    "      * **If Exists:** Abort ingestion. Print: *\"Duplicate detected. Skipping [filename].\"*\n",
    "      * **If New:** Proceed with loading, splitting, and adding.\n",
    "\n",
    "* **Crucial:** When adding the chunks to Chroma, every chunk's metadata **must** now include `{\"file_hash\": current_hash, \"source\": filename}`.\n",
    "\n",
    "#### 3. The \"Un-Ingest\" Feature\n",
    "\n",
    "Add a `delete_file(self, filename)` method.\n",
    "   * **Input:** The filename (string).\n",
    "   * **Logic:** Remove all vector entries where the metadata `source` matches the filename.\n",
    "   * **Tool:** `self.collection.delete(where={...})`.\n",
    "   * **Output:** Print how many chunks were deleted (optional, but good for debugging).\n",
    "\n",
    "---\n",
    "\n",
    "### The Test Script (In `if __name__ == \"__main__\":`)\n",
    "\n",
    "You must write a script that proves your logic works by attempting to fool it.\n",
    "\n",
    "   1. **Clean Start:** Initialize the DB. (Optional: Clear it first if you know how, or just start with a fresh path).\n",
    "   2. **Round 1 (First Ingestion):** Ingest `test.pdf`.\n",
    "      * *Expectation:* \"Ingesting test.pdf...\"\n",
    "\n",
    "   3. **Round 2 (The Duplicate Attempt):** Attempt to ingest `test.pdf` again immediately.\n",
    "      * *Expectation:* \"Duplicate detected. Skipping...\"\n",
    "\n",
    "   4. **Round 3 (The Renamed Trap):** Copy `test.pdf` to `test_copy.pdf` (same content, different name). Attempt to ingest `test_copy.pdf`.\n",
    "      * *Expectation:* \"Duplicate detected. Skipping...\" (This proves you are checking Hash, not Filename).\n",
    "\n",
    "   5. **Round 4 (Deletion):** Call `delete_file(\"test.pdf\")`.\n",
    "      * *Expectation:* Verify via a query or count that the data is gone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8839628-a82c-469e-a3c4-12ca08a705a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate detected. Skipping Test.pdf.\n",
      "Duplicate detected. Skipping Test.pdf.\n",
      "Duplicate detected. Skipping Test - Copy.pdf.\n",
      "Deleted 0 chunks for file 'test.pdf'.\n",
      "Remaining chunks: 0\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import pypdf\n",
    "import os\n",
    "import hashlib\n",
    "import shutil\n",
    "\n",
    "\n",
    "class VectorStore():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor contains path to the chroma db\n",
    "        \"\"\"\n",
    "        self.client = chromadb.PersistentClient(path=r\"Data/\")\n",
    "        self.collection = None\n",
    "\n",
    "    def get_or_create_collection(self):\n",
    "        \"\"\"\n",
    "        Create a new collection or gets the existing collection\n",
    "        \"\"\"\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=\"my_knowledge_base\"\n",
    "        )\n",
    "        return self.collection\n",
    "\n",
    "    def get_file_hash(self, filepath):\n",
    "        \"\"\"\n",
    "        Returns an MD5 hash of the file contents\n",
    "        \"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "\n",
    "\n",
    "    def load_pdf(self, file):\n",
    "        \"\"\"\n",
    "        Loads all the text of the pdf into a string.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        reader = pypdf.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                texts.append(text)\n",
    "        return \"\\n\\n\".join(texts)\n",
    "\n",
    "    def split_text(self, text, chunk_size=1000, chunk_overlap=200):\n",
    "        \"\"\"\n",
    "        Sliding Window Splitter\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            chunks.append(text[start:start + chunk_size])\n",
    "            start += chunk_size - chunk_overlap\n",
    "        return chunks\n",
    "\n",
    "\n",
    "    def ingest_pdf(self, pdf_path):\n",
    "        \"\"\"\n",
    "        Idempotent ingestion pipeline with duplicate detection\n",
    "        \"\"\"\n",
    "        if self.collection is None:\n",
    "            self.get_or_create_collection()\n",
    "\n",
    "        filename = os.path.basename(pdf_path)\n",
    "        file_hash = self.get_file_hash(pdf_path)\n",
    "\n",
    "        existing = self.collection.get(\n",
    "            where={\"file_hash\": file_hash}\n",
    "        )\n",
    "\n",
    "        if existing[\"ids\"]:\n",
    "            print(f\"Duplicate detected. Skipping {filename}.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Ingesting {filename}...\")\n",
    "\n",
    "        all_texts = self.load_pdf(pdf_path)\n",
    "        chunks = self.split_text(all_texts)\n",
    "\n",
    "        ids = [f\"{file_hash}_chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "        metadatas = [\n",
    "            {\n",
    "                \"source\": filename,\n",
    "                \"file_hash\": file_hash\n",
    "            }\n",
    "            for _ in range(len(chunks))\n",
    "        ]\n",
    "\n",
    "        self.collection.add(\n",
    "            documents=chunks,\n",
    "            ids=ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "\n",
    "\n",
    "    def delete_file(self, filename):\n",
    "        \"\"\"\n",
    "        Deletes all chunks belonging to a given source filename\n",
    "        \"\"\"\n",
    "        if self.collection is None:\n",
    "            self.get_or_create_collection()\n",
    "\n",
    "        # Count before delete (for verification)\n",
    "        before = self.collection.get(where={\"source\": filename})\n",
    "        count = len(before[\"ids\"])\n",
    "\n",
    "        self.collection.delete(where={\"source\": filename})\n",
    "\n",
    "        print(f\"Deleted {count} chunks for file '{filename}'.\")\n",
    "\n",
    "\n",
    "    def query_db(self, query_text, n_results=5, filter_dict=None):\n",
    "        \"\"\"\n",
    "        Query the vector database\n",
    "        \"\"\"\n",
    "        if self.collection is None:\n",
    "            self.get_or_create_collection()\n",
    "\n",
    "        query = {\n",
    "            \"query_texts\": [query_text],\n",
    "            \"n_results\": n_results\n",
    "        }\n",
    "\n",
    "        if filter_dict is not None:\n",
    "            query[\"where\"] = filter_dict\n",
    "\n",
    "        return self.collection.query(**query)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vs = VectorStore()\n",
    "\n",
    "    # Optional: start clean (uncomment if needed)\n",
    "    # vs.client.delete_collection(\"my_knowledge_base\")\n",
    "\n",
    "    # Paths\n",
    "    original = r\"Data\\Test.pdf\"\n",
    "    renamed = r\"Data\\Test - Copy.pdf\"\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Round 1: First ingestion\n",
    "    # ------------------------------------------------\n",
    "    vs.ingest_pdf(original)\n",
    "    # Expect: Ingesting test.pdf...\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Round 2: Duplicate ingestion (same file)\n",
    "    # ------------------------------------------------\n",
    "    vs.ingest_pdf(original)\n",
    "    # Expect: Duplicate detected. Skipping test.pdf.\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Round 3: Renamed duplicate (same content)\n",
    "    # ------------------------------------------------\n",
    "    shutil.copyfile(original, renamed)\n",
    "    vs.ingest_pdf(renamed)\n",
    "    # Expect: Duplicate detected. Skipping test_copy.pdf.\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Round 4: Deletion\n",
    "    # ------------------------------------------------\n",
    "    vs.delete_file(\"test.pdf\")\n",
    "\n",
    "    # Optional verification\n",
    "    remaining = vs.collection.get(where={\"source\": \"test.pdf\"})\n",
    "    print(\"Remaining chunks:\", len(remaining[\"ids\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
