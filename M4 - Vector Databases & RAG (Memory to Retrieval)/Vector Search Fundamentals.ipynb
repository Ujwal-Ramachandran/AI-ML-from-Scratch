{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0df165-e7b2-43f1-84db-32129520a844",
   "metadata": {},
   "source": [
    "# **Vector Search Fundamentals**\n",
    "\n",
    "We are now transitioning from generating embeddings (which we will cover more deeply in M3) to **managing and searching** them at scale. This is the backbone of RAG and modern retrieval systems.\n",
    "\n",
    "We will focus on the fundamental geometry of vector spaces and how to traverse them efficiently using the **FAISS** library.\n",
    "\n",
    "Here is the breakdown for today's session.\n",
    "\n",
    "### Phase 1: Topic Breakdown\n",
    "\n",
    "```text\n",
    "L17: Vector Search Fundamentals\n",
    "├── Concept 1: Vector Spaces & High-Dimensional Data\n",
    "│   ├── Embeddings as coordinates\n",
    "│   ├── The Matrix (N vectors x D dimensions)\n",
    "│   ├── Simple Explanation: Finding points in a massive hyper-cube\n",
    "│   └── Task: Generate synthetic vector dataset (NumPy)\n",
    "│\n",
    "├── Concept 2: Distance Metrics\n",
    "│   ├── Euclidean Distance (L2) - Physical distance\n",
    "│   ├── Cosine Similarity - Angular distance\n",
    "│   ├── Intuition: Magnitude vs. Orientation\n",
    "│   └── Task: Manual calculation of metrics using NumPy\n",
    "│\n",
    "├── Concept 3: ANN Theory (Approximate Nearest Neighbors)\n",
    "│   ├── The Scaling Problem (O(N) Complexity)\n",
    "│   ├── IVF (Inverted File Index) - Partitioning/Clustering\n",
    "│   ├── HNSW (Hierarchical Navigable Small World) - Graph Traversal\n",
    "│   └── Task: Conceptual Check (Trade-offs)\n",
    "│\n",
    "├── Concept 4: FAISS Basics (The Tool)\n",
    "│   ├── What is FAISS? (Facebook AI Similarity Search)\n",
    "│   ├── The Index Object (IndexFlatL2)\n",
    "│   ├── The Workflow: Index -> Add -> Search\n",
    "│   └── Task: Implement \"Brute Force\" search in FAISS\n",
    "│\n",
    "└── Mini-Project: The Search Benchmark\n",
    "    ├── Setup Large Dataset\n",
    "    ├── Implement IndexFlatL2 (Exact)\n",
    "    ├── Implement IndexIVFFlat (Approximate)\n",
    "    ├── Train the Index (Clustering)\n",
    "    └── Compare speed (latency) and recall\n",
    "\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d5f8c4-4f60-40cf-bb35-bce6fc31dbc1",
   "metadata": {},
   "source": [
    "## **Concept 1: Vector Spaces & High-Dimensional Data**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In traditional databases, we search for exact matches (e.g., `SELECT * FROM users WHERE name = \"Alice\"`). However, in AI, we often want to search for *meaning*. To do this, we convert complex data like text or images into lists of numbers called **vectors** (or embeddings).\n",
    "\n",
    "Imagine a 2D graph. A point at `(2, 3)` is a vector. Now, imagine a graph with 128, 768, or even 1536 axes. This is a **high-dimensional vector space**. Every piece of data becomes a single point in this space. Data that is semantically similar (e.g., \"Dog\" and \"Puppy\") will be located physically close to each other in this space.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "We represent this data mathematically as a Matrix $X$ of size $N \\times D$ :\n",
    "   * **$N$**: The number of samples (vectors) in your database.\n",
    "   * **$D$**: The dimensionality of each vector (determined by the model, e.g., BERT uses 768).\n",
    "\n",
    "In Python/NumPy, this is simply a 2D array. Crucially, most vector search libraries (including FAISS) are highly optimized for `float32` data types. Using `float64` (default in Python) can cause errors or unnecessary memory bloat.\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "Imagine a massive library. Instead of organizing books by genre or title, we give every book a precise GPS coordinate in a multi-dimensional universe. If you want a book about \"Cooking,\" you don't look up the word; you go to the \"Cooking\" coordinates, and you'll find all the relevant books clustered right there.\n",
    "\n",
    "### Trade-offs\n",
    "   * **Pros:** Allows semantic search (matching meaning, not just keywords).\n",
    "   * **Cons:** \"The Curse of Dimensionality.\" As $D$ increases, the amount of data needed to generalize increases, and calculating distances becomes computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You need to simulate a dataset of embeddings to prepare for our search algorithms.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Create a Python script using `numpy`.\n",
    "   2. Define two constants: `nb` (number of database vectors) = 10,000 and `d` (dimension) = 128.\n",
    "   3. Generate a matrix `xb` of shape `(nb, d)` filled with random numbers.\n",
    "   4. **Critical:** Ensure the data type of the matrix is explicitly `float32`.\n",
    "   5. Set a random seed so our results are reproducible.\n",
    "   6. Print the shape and data type of `xb` to verify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d08cb8c-feb3-43ce-afee-fffd9df647fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 128)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "nb = 10000\n",
    "d = 128\n",
    "\n",
    "xb = np.random.rand(nb, d).astype(np.float32)\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf9606f-8a60-4557-b32a-7950f335a09d",
   "metadata": {},
   "source": [
    "## **Concept 2: Distance Metrics**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Once we have points in space, we need a ruler to measure how close they are. In AI, \"closeness\" implies similarity. If the distance between the \"User Query\" vector and a \"Document\" vector is small, that document is likely relevant.\n",
    "\n",
    "### Mechanics\n",
    "\n",
    "There are two primary ways to measure this in high-dimensional spaces:\n",
    "\n",
    "1. **Euclidean Distance (L2):**\n",
    "   * The straight-line distance between two points.\n",
    "   * Formula: $d(x, y) = \\sqrt{\\sum_{i=1}^{D} (x_i - y_i)^2}$\n",
    "   * **Behavior:** Sensitive to the magnitude (length) of vectors. If one vector is  and another is , they are far apart even if they point in the same direction.\n",
    "\n",
    "2. **Inner Product (Dot Product) / Cosine Similarity:**\n",
    "   * Measures the alignment (angle) between vectors.\n",
    "   * Formula (Dot Product): $IP(x, y) = \\sum_{i=1}^{D} x_i \\cdot y_i$\n",
    "   * **Behavior:** If vectors are normalized (length = 1), Inner Product *is* Cosine Similarity. It focuses on \"direction\" rather than \"magnitude.\"\n",
    "\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "   * **Euclidean:** How much physical energy does it take to walk from point A to point B?\n",
    "   * **Cosine:** Are we pointing at the same star? (It doesn't matter if you are standing on Earth or Mars; if you both point at the sun, the angle difference is low).\n",
    "\n",
    "### Trade-offs\n",
    "   * **L2:** Good when magnitude matters (e.g., pixel intensity in images).\n",
    "   * **Inner Product:** Usually preferred for text/semantic search (NLP) because the length of the document text shouldn't necessarily penalize the match. **FAISS** is highly optimized for L2 and Inner Product.\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "You will implement these metrics manually using NumPy to understand the linear algebra behind the scenes.\n",
    "\n",
    "**Specifications:**\n",
    "   1. Generate a **query matrix** `xq` containing **5 vectors** of dimension `d` (use `np.random`).\n",
    "   2. Ensure `xq` is `float32`.\n",
    "   3. **Task A (L2 Distance):** Calculate the squared Euclidean distance between the **first vector** of `xq` and the **first vector** of `xb`. Use the formula: `sum((x - y)^2)`.\n",
    "   4. **Task B (Dot Product):** Calculate the dot product between the **first vector** of `xq` and the **first vector** of `xb`.\n",
    "   5. Print both results.\n",
    "\n",
    "**Constraints:**\n",
    "   * Do not use `scipy` or `sklearn`. Use standard NumPy math.\n",
    "   * Do not perform matrix multiplication for the whole dataset yet; just do 1-to-1 vector comparison for this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980594e1-cd8f-4543-bdbb-86db97865686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance = 22.061784744262695\n",
      "Dot Product = 29.73244857788086\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "xq = np.random.rand(5, d).astype(np.float32)\n",
    "\n",
    "x = xb[0]\n",
    "y = xq[0]\n",
    "\n",
    "dist = np.sum((x - y) ** 2)\n",
    "dot_p = np.dot(x, y)\n",
    "\n",
    "print(f\"Distance = {dist}\\nDot Product = {dot_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b9b4f0-309f-4670-9473-67637999d8f5",
   "metadata": {},
   "source": [
    "The output shows a non-zero distance ($22.06$), confirming that `xq` and `xb` are now distinct vectors in the space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Concept 3: ANN Theory (Approximate Nearest Neighbors)**\n",
    "\n",
    "### Intuition\n",
    "\n",
    "In your previous task, you compared 1 query vector against 1 database vector. To find the \"nearest neighbor\" for a query, you'd have to compare it against **all** 10,000 vectors in `xb`.\n",
    "\n",
    "If $N = 10,000$, that's fine.\n",
    "If $N = 1 \\text{ Billion}$ (web scale), calculating 1 Billion distances for *every single user query* is impossible in real-time.\n",
    "\n",
    "**ANN (Approximate Nearest Neighbors)** algorithms solve this by trading a tiny bit of accuracy (Recall) for massive speed (Latency). We accept that we might find the 2nd or 3rd closest match instead of the absolute 1st, in exchange for searching only a fraction of the data.\n",
    "\n",
    "### Mechanics: Two Main Approaches\n",
    "\n",
    "1. **IVF (Inverted File Index): Partitioning**\n",
    "* We divide the vector space into clusters (cells).\n",
    "* We compute the \"centroid\" (center point) of each cluster.\n",
    "* When a query comes in, we measure the distance to the *centroids*.\n",
    "* We identify the closest centroid and **only** search the vectors inside that specific cell.\n",
    "* **Analogy:** Instead of checking every book in the library, check the catalog to find the \"Cooking\" section, then search only that shelf.\n",
    "\n",
    "\n",
    "2. **HNSW (Hierarchical Navigable Small World): Graph Traversal**\n",
    "* Vectors are nodes in a graph connected to their neighbors.\n",
    "* It builds a multi-layer graph (like a highway system).\n",
    "* Top layers have long connections (fast travel across the map). Bottom layers have dense connections (local precision).\n",
    "* **Analogy:** Playing \"Six Degrees of Kevin Bacon\" to find a path from A to B.\n",
    "\n",
    "\n",
    "\n",
    "### Simpler Explanation\n",
    "\n",
    "* **Brute Force (Flat):** Check every single haystalk to find the needle. (100% accurate, Slow).\n",
    "* **ANN (IVF/HNSW):** Use a metal detector to find the general area, then look there. (99% accurate, Super Fast).\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "* **Brute Force:** Recall = 100%. Latency = High. Memory = Low.\n",
    "* **IVF:** Recall = Tunable (based on how many cells you probe). Latency = Low. Requires \"Training\" step.\n",
    "* **HNSW:** Recall = High. Latency = Very Low. Memory = High (needs to store graph edges).\n",
    "\n",
    "---\n",
    "\n",
    "### Your Task\n",
    "\n",
    "This is a conceptual check to ensure you understand the architecture choices before we code.\n",
    "\n",
    "**Scenario:**\n",
    "\n",
    "1. **System A:** A FaceID unlock system for a secure building. The database has only 500 employees. Accuracy must be perfect.\n",
    "2. **System B:** An e-commerce recommendation engine (\"Similar products\"). The database has 50 million items. Speed is critical; if the user waits >200ms, they leave.\n",
    "\n",
    "**Question:** Which approach (Brute Force vs. ANN) would you choose for System A and System B?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f29927-3417-41ec-a0d6-fe65df2c90eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b0008-47a5-4f95-b239-5cfb6e2d3663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14593ade-692e-41ee-864b-7fcd681b3b64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dbbac-e693-4be6-a477-f93e2c90ac74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c87e7e-9cce-4ce0-a7c5-9a467a7d8f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ee032-505f-4a9b-b99b-38c587cda8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1a1b4-bedd-4af8-a8e7-92f1aef784db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab496319-9b13-4faf-985d-57a7475c82b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084afc6-485b-436a-9047-df9d1b6614d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcf0752-5686-4f22-be10-7847c8f0a88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaa472f-e4f9-4c25-92b1-f8e6514d98a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5483f-aacc-4030-9c6d-5bfb739892f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b3ca81-03c6-4226-a1c1-c400067dde20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fe1c6-ed90-4535-b7e9-213584692142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5c2a3d-e558-4cba-ae97-b056dc5734ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
